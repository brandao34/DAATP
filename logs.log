2024-11-06 12:15:36,661:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 12:15:36,661:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 12:15:36,661:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 12:15:36,661:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 12:15:40,201:INFO:PyCaret ClassificationExperiment
2024-11-06 12:15:40,201:INFO:Logging name: clf-default-name
2024-11-06 12:15:40,202:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-06 12:15:40,202:INFO:version 3.3.2
2024-11-06 12:15:40,202:INFO:Initializing setup()
2024-11-06 12:15:40,202:INFO:self.USI: bdc4
2024-11-06 12:15:40,202:INFO:self._variable_keys: {'target_param', '_available_plots', 'exp_id', 'y', 'gpu_n_jobs_param', '_ml_usecase', 'data', 'memory', 'fix_imbalance', 'USI', 'fold_generator', 'exp_name_log', 'seed', 'idx', 'fold_groups_param', 'X_test', 'y_test', 'log_plots_param', 'logging_param', 'y_train', 'X', 'html_param', 'pipeline', 'is_multiclass', 'n_jobs_param', 'gpu_param', 'X_train', 'fold_shuffle_param'}
2024-11-06 12:15:40,202:INFO:Checking environment
2024-11-06 12:15:40,202:INFO:python_version: 3.11.10
2024-11-06 12:15:40,202:INFO:python_build: ('main', 'Oct  3 2024 07:29:13')
2024-11-06 12:15:40,202:INFO:machine: x86_64
2024-11-06 12:15:40,205:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 12:15:40,206:INFO:Memory: svmem(total=33037213696, available=25285898240, percent=23.5, used=7119929344, free=14051102720, active=7983906816, inactive=7839006720, buffers=672542720, cached=11193638912, shared=157995008, slab=1747529728)
2024-11-06 12:15:40,208:INFO:Physical Core: 8
2024-11-06 12:15:40,208:INFO:Logical Core: 16
2024-11-06 12:15:40,208:INFO:Checking libraries
2024-11-06 12:15:40,208:INFO:System:
2024-11-06 12:15:40,208:INFO:    python: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0]
2024-11-06 12:15:40,208:INFO:executable: /home/cid34senhas/miniconda3/envs/myenv/bin/python3
2024-11-06 12:15:40,208:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 12:15:40,208:INFO:PyCaret required dependencies:
2024-11-06 12:15:40,229:INFO:                 pip: 24.2
2024-11-06 12:15:40,229:INFO:          setuptools: 75.1.0
2024-11-06 12:15:40,229:INFO:             pycaret: 3.3.2
2024-11-06 12:15:40,229:INFO:             IPython: 8.29.0
2024-11-06 12:15:40,229:INFO:          ipywidgets: 8.1.5
2024-11-06 12:15:40,229:INFO:                tqdm: 4.66.6
2024-11-06 12:15:40,229:INFO:               numpy: 1.26.4
2024-11-06 12:15:40,229:INFO:              pandas: 2.1.4
2024-11-06 12:15:40,229:INFO:              jinja2: 3.1.4
2024-11-06 12:15:40,229:INFO:               scipy: 1.11.4
2024-11-06 12:15:40,229:INFO:              joblib: 1.3.2
2024-11-06 12:15:40,229:INFO:             sklearn: 1.4.2
2024-11-06 12:15:40,229:INFO:                pyod: 2.0.2
2024-11-06 12:15:40,229:INFO:            imblearn: 0.12.4
2024-11-06 12:15:40,229:INFO:   category_encoders: 2.6.4
2024-11-06 12:15:40,229:INFO:            lightgbm: 4.5.0
2024-11-06 12:15:40,229:INFO:               numba: 0.60.0
2024-11-06 12:15:40,229:INFO:            requests: 2.32.3
2024-11-06 12:15:40,229:INFO:          matplotlib: 3.7.5
2024-11-06 12:15:40,229:INFO:          scikitplot: 0.3.7
2024-11-06 12:15:40,229:INFO:         yellowbrick: 1.5
2024-11-06 12:15:40,230:INFO:              plotly: 5.24.1
2024-11-06 12:15:40,230:INFO:    plotly-resampler: Not installed
2024-11-06 12:15:40,230:INFO:             kaleido: 0.2.1
2024-11-06 12:15:40,230:INFO:           schemdraw: 0.15
2024-11-06 12:15:40,230:INFO:         statsmodels: 0.14.4
2024-11-06 12:15:40,230:INFO:              sktime: 0.26.0
2024-11-06 12:15:40,230:INFO:               tbats: 1.1.3
2024-11-06 12:15:40,230:INFO:            pmdarima: 2.0.4
2024-11-06 12:15:40,230:INFO:              psutil: 6.1.0
2024-11-06 12:15:40,230:INFO:          markupsafe: 3.0.2
2024-11-06 12:15:40,230:INFO:             pickle5: Not installed
2024-11-06 12:15:40,230:INFO:         cloudpickle: 3.1.0
2024-11-06 12:15:40,230:INFO:         deprecation: 2.1.0
2024-11-06 12:15:40,230:INFO:              xxhash: 3.5.0
2024-11-06 12:15:40,230:INFO:           wurlitzer: 3.1.1
2024-11-06 12:15:40,230:INFO:PyCaret optional dependencies:
2024-11-06 12:15:40,247:INFO:                shap: Not installed
2024-11-06 12:15:40,247:INFO:           interpret: Not installed
2024-11-06 12:15:40,247:INFO:                umap: Not installed
2024-11-06 12:15:40,248:INFO:     ydata_profiling: Not installed
2024-11-06 12:15:40,248:INFO:  explainerdashboard: Not installed
2024-11-06 12:15:40,248:INFO:             autoviz: Not installed
2024-11-06 12:15:40,248:INFO:           fairlearn: Not installed
2024-11-06 12:15:40,248:INFO:          deepchecks: Not installed
2024-11-06 12:15:40,248:INFO:             xgboost: 2.1.2
2024-11-06 12:15:40,248:INFO:            catboost: Not installed
2024-11-06 12:15:40,248:INFO:              kmodes: Not installed
2024-11-06 12:15:40,248:INFO:             mlxtend: Not installed
2024-11-06 12:15:40,248:INFO:       statsforecast: Not installed
2024-11-06 12:15:40,248:INFO:        tune_sklearn: Not installed
2024-11-06 12:15:40,248:INFO:                 ray: Not installed
2024-11-06 12:15:40,248:INFO:            hyperopt: Not installed
2024-11-06 12:15:40,248:INFO:              optuna: Not installed
2024-11-06 12:15:40,248:INFO:               skopt: Not installed
2024-11-06 12:15:40,248:INFO:              mlflow: Not installed
2024-11-06 12:15:40,248:INFO:              gradio: Not installed
2024-11-06 12:15:40,248:INFO:             fastapi: Not installed
2024-11-06 12:15:40,248:INFO:             uvicorn: Not installed
2024-11-06 12:15:40,248:INFO:              m2cgen: Not installed
2024-11-06 12:15:40,248:INFO:           evidently: Not installed
2024-11-06 12:15:40,248:INFO:               fugue: Not installed
2024-11-06 12:15:40,248:INFO:           streamlit: Not installed
2024-11-06 12:15:40,248:INFO:             prophet: Not installed
2024-11-06 12:15:40,248:INFO:None
2024-11-06 12:15:40,248:INFO:Set up data.
2024-11-06 12:16:13,353:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 12:16:13,353:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 12:16:13,353:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 12:16:13,353:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 12:16:16,861:INFO:PyCaret ClassificationExperiment
2024-11-06 12:16:16,861:INFO:Logging name: clf-default-name
2024-11-06 12:16:16,861:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-06 12:16:16,861:INFO:version 3.3.2
2024-11-06 12:16:16,862:INFO:Initializing setup()
2024-11-06 12:16:16,862:INFO:self.USI: ed2b
2024-11-06 12:16:16,862:INFO:self._variable_keys: {'n_jobs_param', 'gpu_n_jobs_param', 'X', 'y', 'log_plots_param', 'target_param', '_ml_usecase', 'fold_shuffle_param', 'USI', '_available_plots', 'fold_groups_param', 'data', 'y_train', 'idx', 'logging_param', 'exp_id', 'seed', 'is_multiclass', 'memory', 'gpu_param', 'X_train', 'fold_generator', 'y_test', 'X_test', 'html_param', 'pipeline', 'exp_name_log', 'fix_imbalance'}
2024-11-06 12:16:16,862:INFO:Checking environment
2024-11-06 12:16:16,862:INFO:python_version: 3.11.10
2024-11-06 12:16:16,862:INFO:python_build: ('main', 'Oct  3 2024 07:29:13')
2024-11-06 12:16:16,862:INFO:machine: x86_64
2024-11-06 12:16:16,864:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 12:16:16,864:INFO:Memory: svmem(total=33037213696, available=25363750912, percent=23.2, used=7050158080, free=14128750592, active=7929753600, inactive=7839232000, buffers=672714752, cached=11185590272, shared=149925888, slab=1746677760)
2024-11-06 12:16:16,865:INFO:Physical Core: 8
2024-11-06 12:16:16,866:INFO:Logical Core: 16
2024-11-06 12:16:16,866:INFO:Checking libraries
2024-11-06 12:16:16,866:INFO:System:
2024-11-06 12:16:16,866:INFO:    python: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0]
2024-11-06 12:16:16,866:INFO:executable: /home/cid34senhas/miniconda3/envs/myenv/bin/python3
2024-11-06 12:16:16,866:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 12:16:16,866:INFO:PyCaret required dependencies:
2024-11-06 12:16:16,887:INFO:                 pip: 24.2
2024-11-06 12:16:16,887:INFO:          setuptools: 75.1.0
2024-11-06 12:16:16,887:INFO:             pycaret: 3.3.2
2024-11-06 12:16:16,887:INFO:             IPython: 8.29.0
2024-11-06 12:16:16,887:INFO:          ipywidgets: 8.1.5
2024-11-06 12:16:16,887:INFO:                tqdm: 4.66.6
2024-11-06 12:16:16,887:INFO:               numpy: 1.26.4
2024-11-06 12:16:16,887:INFO:              pandas: 2.1.4
2024-11-06 12:16:16,887:INFO:              jinja2: 3.1.4
2024-11-06 12:16:16,887:INFO:               scipy: 1.11.4
2024-11-06 12:16:16,887:INFO:              joblib: 1.3.2
2024-11-06 12:16:16,887:INFO:             sklearn: 1.4.2
2024-11-06 12:16:16,887:INFO:                pyod: 2.0.2
2024-11-06 12:16:16,887:INFO:            imblearn: 0.12.4
2024-11-06 12:16:16,887:INFO:   category_encoders: 2.6.4
2024-11-06 12:16:16,887:INFO:            lightgbm: 4.5.0
2024-11-06 12:16:16,887:INFO:               numba: 0.60.0
2024-11-06 12:16:16,887:INFO:            requests: 2.32.3
2024-11-06 12:16:16,887:INFO:          matplotlib: 3.7.5
2024-11-06 12:16:16,887:INFO:          scikitplot: 0.3.7
2024-11-06 12:16:16,887:INFO:         yellowbrick: 1.5
2024-11-06 12:16:16,887:INFO:              plotly: 5.24.1
2024-11-06 12:16:16,887:INFO:    plotly-resampler: Not installed
2024-11-06 12:16:16,887:INFO:             kaleido: 0.2.1
2024-11-06 12:16:16,887:INFO:           schemdraw: 0.15
2024-11-06 12:16:16,887:INFO:         statsmodels: 0.14.4
2024-11-06 12:16:16,887:INFO:              sktime: 0.26.0
2024-11-06 12:16:16,887:INFO:               tbats: 1.1.3
2024-11-06 12:16:16,887:INFO:            pmdarima: 2.0.4
2024-11-06 12:16:16,887:INFO:              psutil: 6.1.0
2024-11-06 12:16:16,887:INFO:          markupsafe: 3.0.2
2024-11-06 12:16:16,887:INFO:             pickle5: Not installed
2024-11-06 12:16:16,887:INFO:         cloudpickle: 3.1.0
2024-11-06 12:16:16,887:INFO:         deprecation: 2.1.0
2024-11-06 12:16:16,887:INFO:              xxhash: 3.5.0
2024-11-06 12:16:16,887:INFO:           wurlitzer: 3.1.1
2024-11-06 12:16:16,888:INFO:PyCaret optional dependencies:
2024-11-06 12:16:16,907:INFO:                shap: Not installed
2024-11-06 12:16:16,907:INFO:           interpret: Not installed
2024-11-06 12:16:16,907:INFO:                umap: Not installed
2024-11-06 12:16:16,907:INFO:     ydata_profiling: Not installed
2024-11-06 12:16:16,907:INFO:  explainerdashboard: Not installed
2024-11-06 12:16:16,907:INFO:             autoviz: Not installed
2024-11-06 12:16:16,907:INFO:           fairlearn: Not installed
2024-11-06 12:16:16,907:INFO:          deepchecks: Not installed
2024-11-06 12:16:16,907:INFO:             xgboost: 2.1.2
2024-11-06 12:16:16,907:INFO:            catboost: Not installed
2024-11-06 12:16:16,907:INFO:              kmodes: Not installed
2024-11-06 12:16:16,907:INFO:             mlxtend: Not installed
2024-11-06 12:16:16,907:INFO:       statsforecast: Not installed
2024-11-06 12:16:16,907:INFO:        tune_sklearn: Not installed
2024-11-06 12:16:16,907:INFO:                 ray: Not installed
2024-11-06 12:16:16,907:INFO:            hyperopt: Not installed
2024-11-06 12:16:16,907:INFO:              optuna: Not installed
2024-11-06 12:16:16,907:INFO:               skopt: Not installed
2024-11-06 12:16:16,908:INFO:              mlflow: Not installed
2024-11-06 12:16:16,908:INFO:              gradio: Not installed
2024-11-06 12:16:16,908:INFO:             fastapi: Not installed
2024-11-06 12:16:16,908:INFO:             uvicorn: Not installed
2024-11-06 12:16:16,908:INFO:              m2cgen: Not installed
2024-11-06 12:16:16,908:INFO:           evidently: Not installed
2024-11-06 12:16:16,908:INFO:               fugue: Not installed
2024-11-06 12:16:16,908:INFO:           streamlit: Not installed
2024-11-06 12:16:16,908:INFO:             prophet: Not installed
2024-11-06 12:16:16,908:INFO:None
2024-11-06 12:16:16,908:INFO:Set up data.
2024-11-06 12:16:17,027:INFO:Set up folding strategy.
2024-11-06 12:16:17,027:INFO:Set up train/test split.
2024-11-06 12:16:17,165:INFO:Set up index.
2024-11-06 12:16:17,166:INFO:Assigning column types.
2024-11-06 12:16:17,253:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-06 12:16:17,311:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 12:16:17,314:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 12:16:17,353:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 12:16:17,356:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 12:16:17,414:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 12:16:17,415:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 12:16:17,451:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 12:16:17,454:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 12:16:17,454:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-06 12:16:17,512:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 12:16:17,548:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 12:16:17,552:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 12:16:17,610:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 12:16:17,645:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 12:16:17,648:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 12:16:17,649:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-06 12:16:17,742:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 12:16:17,745:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 12:16:17,838:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 12:16:17,842:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 12:16:17,843:INFO:Preparing preprocessing pipeline...
2024-11-06 12:16:17,857:INFO:Set up simple imputation.
2024-11-06 12:16:17,933:INFO:Set up column name cleaning.
2024-11-06 12:16:18,202:INFO:Finished creating preprocessing pipeline.
2024-11-06 12:16:18,210:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['diagnostics_Mask-original_VoxelNum',
                                             'original_shape_Maximum2DDiameterSlice',
                                             'original_shape_MeshVolume',
                                             'original_shape_MinorAxisLength',
                                             'original_shape_Sphericity',
                                             'original_shape_SurfaceArea',
                                             'original_shape_SurfaceVolumeRatio',
                                             '...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-06 12:16:18,210:INFO:Creating final display dataframe.
2024-11-06 12:16:19,263:INFO:Setup _display_container:                     Description             Value
0                    Session id              6347
1                        Target        Transition
2                   Target type        Multiclass
3           Original data shape        (295, 800)
4        Transformed data shape        (295, 800)
5   Transformed train set shape        (206, 800)
6    Transformed test set shape         (89, 800)
7              Numeric features               799
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              ed2b
2024-11-06 12:16:19,322:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 12:16:19,324:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 12:16:19,381:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 12:16:19,383:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 12:16:19,384:INFO:setup() successfully completed in 2.52s...............
2024-11-06 12:16:19,384:INFO:Initializing compare_models()
2024-11-06 12:16:19,384:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x72bf6af7d350>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x72bf6af7d350>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-06 12:16:19,384:INFO:Checking exceptions
2024-11-06 12:16:19,477:INFO:Preparing display monitor
2024-11-06 12:16:19,479:INFO:Initializing Logistic Regression
2024-11-06 12:16:19,479:INFO:Total runtime is 1.3232231140136718e-06 minutes
2024-11-06 12:16:19,479:INFO:SubProcess create_model() called ==================================
2024-11-06 12:16:19,479:INFO:Initializing create_model()
2024-11-06 12:16:19,479:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x72bf6af7d350>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x72bf6b4528d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 12:16:19,479:INFO:Checking exceptions
2024-11-06 12:16:19,479:INFO:Importing libraries
2024-11-06 12:16:19,479:INFO:Copying training dataset
2024-11-06 12:16:19,536:INFO:Defining folds
2024-11-06 12:16:19,536:INFO:Declaring metric variables
2024-11-06 12:16:19,536:INFO:Importing untrained model
2024-11-06 12:16:19,536:INFO:Logistic Regression Imported successfully
2024-11-06 12:16:19,536:INFO:Starting cross validation
2024-11-06 12:16:19,538:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 12:16:23,400:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 12:16:23,439:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:23,467:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 12:16:23,483:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 12:16:23,509:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 12:16:23,510:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:23,520:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:23,545:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:23,616:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 12:16:23,644:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:23,787:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 12:16:23,812:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:23,877:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 12:16:23,897:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:23,954:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 12:16:23,967:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 12:16:23,976:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:23,988:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:24,075:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 12:16:24,093:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:24,110:INFO:Calculating mean and std
2024-11-06 12:16:24,111:INFO:Creating metrics dataframe
2024-11-06 12:16:24,114:INFO:Uploading results into container
2024-11-06 12:16:24,115:INFO:Uploading model into container now
2024-11-06 12:16:24,116:INFO:_master_model_container: 1
2024-11-06 12:16:24,116:INFO:_display_container: 2
2024-11-06 12:16:24,117:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=6347, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-06 12:16:24,117:INFO:create_model() successfully completed......................................
2024-11-06 12:16:24,206:INFO:SubProcess create_model() end ==================================
2024-11-06 12:16:24,206:INFO:Creating metrics dataframe
2024-11-06 12:16:24,208:INFO:Initializing K Neighbors Classifier
2024-11-06 12:16:24,208:INFO:Total runtime is 0.07881431182225546 minutes
2024-11-06 12:16:24,208:INFO:SubProcess create_model() called ==================================
2024-11-06 12:16:24,208:INFO:Initializing create_model()
2024-11-06 12:16:24,208:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x72bf6af7d350>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x72bf6b4528d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 12:16:24,208:INFO:Checking exceptions
2024-11-06 12:16:24,208:INFO:Importing libraries
2024-11-06 12:16:24,208:INFO:Copying training dataset
2024-11-06 12:16:24,266:INFO:Defining folds
2024-11-06 12:16:24,266:INFO:Declaring metric variables
2024-11-06 12:16:24,266:INFO:Importing untrained model
2024-11-06 12:16:24,266:INFO:K Neighbors Classifier Imported successfully
2024-11-06 12:16:24,266:INFO:Starting cross validation
2024-11-06 12:16:24,268:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 12:16:25,670:INFO:Calculating mean and std
2024-11-06 12:16:25,671:INFO:Creating metrics dataframe
2024-11-06 12:16:25,673:INFO:Uploading results into container
2024-11-06 12:16:25,674:INFO:Uploading model into container now
2024-11-06 12:16:25,674:INFO:_master_model_container: 2
2024-11-06 12:16:25,674:INFO:_display_container: 2
2024-11-06 12:16:25,674:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-06 12:16:25,675:INFO:create_model() successfully completed......................................
2024-11-06 12:16:25,747:INFO:SubProcess create_model() end ==================================
2024-11-06 12:16:25,747:INFO:Creating metrics dataframe
2024-11-06 12:16:25,749:INFO:Initializing Naive Bayes
2024-11-06 12:16:25,749:INFO:Total runtime is 0.1045032779375712 minutes
2024-11-06 12:16:25,749:INFO:SubProcess create_model() called ==================================
2024-11-06 12:16:25,749:INFO:Initializing create_model()
2024-11-06 12:16:25,749:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x72bf6af7d350>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x72bf6b4528d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 12:16:25,749:INFO:Checking exceptions
2024-11-06 12:16:25,749:INFO:Importing libraries
2024-11-06 12:16:25,749:INFO:Copying training dataset
2024-11-06 12:16:25,805:INFO:Defining folds
2024-11-06 12:16:25,805:INFO:Declaring metric variables
2024-11-06 12:16:25,805:INFO:Importing untrained model
2024-11-06 12:16:25,805:INFO:Naive Bayes Imported successfully
2024-11-06 12:16:25,805:INFO:Starting cross validation
2024-11-06 12:16:25,807:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 12:16:25,933:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:26,109:INFO:Calculating mean and std
2024-11-06 12:16:26,110:INFO:Creating metrics dataframe
2024-11-06 12:16:26,117:INFO:Uploading results into container
2024-11-06 12:16:26,118:INFO:Uploading model into container now
2024-11-06 12:16:26,119:INFO:_master_model_container: 3
2024-11-06 12:16:26,119:INFO:_display_container: 2
2024-11-06 12:16:26,119:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-06 12:16:26,119:INFO:create_model() successfully completed......................................
2024-11-06 12:16:26,196:INFO:SubProcess create_model() end ==================================
2024-11-06 12:16:26,196:INFO:Creating metrics dataframe
2024-11-06 12:16:26,197:INFO:Initializing Decision Tree Classifier
2024-11-06 12:16:26,197:INFO:Total runtime is 0.11197496255238851 minutes
2024-11-06 12:16:26,198:INFO:SubProcess create_model() called ==================================
2024-11-06 12:16:26,198:INFO:Initializing create_model()
2024-11-06 12:16:26,198:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x72bf6af7d350>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x72bf6b4528d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 12:16:26,198:INFO:Checking exceptions
2024-11-06 12:16:26,198:INFO:Importing libraries
2024-11-06 12:16:26,198:INFO:Copying training dataset
2024-11-06 12:16:26,255:INFO:Defining folds
2024-11-06 12:16:26,255:INFO:Declaring metric variables
2024-11-06 12:16:26,255:INFO:Importing untrained model
2024-11-06 12:16:26,255:INFO:Decision Tree Classifier Imported successfully
2024-11-06 12:16:26,256:INFO:Starting cross validation
2024-11-06 12:16:26,257:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 12:16:26,654:INFO:Calculating mean and std
2024-11-06 12:16:26,655:INFO:Creating metrics dataframe
2024-11-06 12:16:26,656:INFO:Uploading results into container
2024-11-06 12:16:26,657:INFO:Uploading model into container now
2024-11-06 12:16:26,657:INFO:_master_model_container: 4
2024-11-06 12:16:26,657:INFO:_display_container: 2
2024-11-06 12:16:26,657:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=6347, splitter='best')
2024-11-06 12:16:26,657:INFO:create_model() successfully completed......................................
2024-11-06 12:16:26,715:INFO:SubProcess create_model() end ==================================
2024-11-06 12:16:26,715:INFO:Creating metrics dataframe
2024-11-06 12:16:26,716:INFO:Initializing SVM - Linear Kernel
2024-11-06 12:16:26,716:INFO:Total runtime is 0.12062302430470784 minutes
2024-11-06 12:16:26,716:INFO:SubProcess create_model() called ==================================
2024-11-06 12:16:26,717:INFO:Initializing create_model()
2024-11-06 12:16:26,717:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x72bf6af7d350>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x72bf6b4528d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 12:16:26,717:INFO:Checking exceptions
2024-11-06 12:16:26,717:INFO:Importing libraries
2024-11-06 12:16:26,717:INFO:Copying training dataset
2024-11-06 12:16:26,772:INFO:Defining folds
2024-11-06 12:16:26,772:INFO:Declaring metric variables
2024-11-06 12:16:26,772:INFO:Importing untrained model
2024-11-06 12:16:26,772:INFO:SVM - Linear Kernel Imported successfully
2024-11-06 12:16:26,772:INFO:Starting cross validation
2024-11-06 12:16:26,774:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 12:16:26,908:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:26,912:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:27,001:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:27,007:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:27,011:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:27,016:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:27,066:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:27,070:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:27,091:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:27,095:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:27,109:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:27,110:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:27,113:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:27,113:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:27,120:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:27,123:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:27,124:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:27,127:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:27,128:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:27,131:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:27,139:INFO:Calculating mean and std
2024-11-06 12:16:27,140:INFO:Creating metrics dataframe
2024-11-06 12:16:27,141:INFO:Uploading results into container
2024-11-06 12:16:27,141:INFO:Uploading model into container now
2024-11-06 12:16:27,142:INFO:_master_model_container: 5
2024-11-06 12:16:27,142:INFO:_display_container: 2
2024-11-06 12:16:27,142:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=6347, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-06 12:16:27,142:INFO:create_model() successfully completed......................................
2024-11-06 12:16:27,219:INFO:SubProcess create_model() end ==================================
2024-11-06 12:16:27,219:INFO:Creating metrics dataframe
2024-11-06 12:16:27,221:INFO:Initializing Ridge Classifier
2024-11-06 12:16:27,221:INFO:Total runtime is 0.12903964519500732 minutes
2024-11-06 12:16:27,221:INFO:SubProcess create_model() called ==================================
2024-11-06 12:16:27,222:INFO:Initializing create_model()
2024-11-06 12:16:27,222:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x72bf6af7d350>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x72bf6b4528d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 12:16:27,222:INFO:Checking exceptions
2024-11-06 12:16:27,222:INFO:Importing libraries
2024-11-06 12:16:27,222:INFO:Copying training dataset
2024-11-06 12:16:27,287:INFO:Defining folds
2024-11-06 12:16:27,287:INFO:Declaring metric variables
2024-11-06 12:16:27,287:INFO:Importing untrained model
2024-11-06 12:16:27,287:INFO:Ridge Classifier Imported successfully
2024-11-06 12:16:27,287:INFO:Starting cross validation
2024-11-06 12:16:27,289:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 12:16:27,379:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 12:16:27,406:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:27,410:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:27,422:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 12:16:27,448:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:27,510:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 12:16:27,515:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 12:16:27,537:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:27,540:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:27,541:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:27,547:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 12:16:27,550:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 12:16:27,561:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 12:16:27,566:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 12:16:27,571:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:27,573:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:27,576:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:27,584:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:27,590:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:27,599:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 12:16:27,615:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 12:16:27,621:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:27,624:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:27,637:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:27,654:INFO:Calculating mean and std
2024-11-06 12:16:27,655:INFO:Creating metrics dataframe
2024-11-06 12:16:27,658:INFO:Uploading results into container
2024-11-06 12:16:27,659:INFO:Uploading model into container now
2024-11-06 12:16:27,660:INFO:_master_model_container: 6
2024-11-06 12:16:27,660:INFO:_display_container: 2
2024-11-06 12:16:27,660:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=6347, solver='auto',
                tol=0.0001)
2024-11-06 12:16:27,660:INFO:create_model() successfully completed......................................
2024-11-06 12:16:27,737:INFO:SubProcess create_model() end ==================================
2024-11-06 12:16:27,737:INFO:Creating metrics dataframe
2024-11-06 12:16:27,739:INFO:Initializing Random Forest Classifier
2024-11-06 12:16:27,739:INFO:Total runtime is 0.13766388495763143 minutes
2024-11-06 12:16:27,739:INFO:SubProcess create_model() called ==================================
2024-11-06 12:16:27,739:INFO:Initializing create_model()
2024-11-06 12:16:27,739:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x72bf6af7d350>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x72bf6b4528d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 12:16:27,739:INFO:Checking exceptions
2024-11-06 12:16:27,739:INFO:Importing libraries
2024-11-06 12:16:27,739:INFO:Copying training dataset
2024-11-06 12:16:27,801:INFO:Defining folds
2024-11-06 12:16:27,801:INFO:Declaring metric variables
2024-11-06 12:16:27,801:INFO:Importing untrained model
2024-11-06 12:16:27,801:INFO:Random Forest Classifier Imported successfully
2024-11-06 12:16:27,801:INFO:Starting cross validation
2024-11-06 12:16:27,803:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 12:16:28,531:INFO:Calculating mean and std
2024-11-06 12:16:28,532:INFO:Creating metrics dataframe
2024-11-06 12:16:28,533:INFO:Uploading results into container
2024-11-06 12:16:28,534:INFO:Uploading model into container now
2024-11-06 12:16:28,534:INFO:_master_model_container: 7
2024-11-06 12:16:28,534:INFO:_display_container: 2
2024-11-06 12:16:28,534:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6347, verbose=0,
                       warm_start=False)
2024-11-06 12:16:28,534:INFO:create_model() successfully completed......................................
2024-11-06 12:16:28,593:INFO:SubProcess create_model() end ==================================
2024-11-06 12:16:28,593:INFO:Creating metrics dataframe
2024-11-06 12:16:28,594:INFO:Initializing Quadratic Discriminant Analysis
2024-11-06 12:16:28,595:INFO:Total runtime is 0.151926589012146 minutes
2024-11-06 12:16:28,595:INFO:SubProcess create_model() called ==================================
2024-11-06 12:16:28,595:INFO:Initializing create_model()
2024-11-06 12:16:28,595:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x72bf6af7d350>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x72bf6b4528d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 12:16:28,595:INFO:Checking exceptions
2024-11-06 12:16:28,595:INFO:Importing libraries
2024-11-06 12:16:28,595:INFO:Copying training dataset
2024-11-06 12:16:28,651:INFO:Defining folds
2024-11-06 12:16:28,651:INFO:Declaring metric variables
2024-11-06 12:16:28,651:INFO:Importing untrained model
2024-11-06 12:16:28,651:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-06 12:16:28,651:INFO:Starting cross validation
2024-11-06 12:16:28,653:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 12:16:28,740:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 12:16:28,771:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 12:16:28,774:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 12:16:28,795:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:28,802:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:28,811:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:28,816:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:28,820:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 12:16:28,843:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 12:16:28,863:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 12:16:28,864:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:28,878:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 12:16:28,883:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 12:16:28,889:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:28,904:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:28,906:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:28,910:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:28,911:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:28,918:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 12:16:28,931:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 12:16:28,942:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:28,955:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:28,971:INFO:Calculating mean and std
2024-11-06 12:16:28,972:INFO:Creating metrics dataframe
2024-11-06 12:16:28,973:INFO:Uploading results into container
2024-11-06 12:16:28,973:INFO:Uploading model into container now
2024-11-06 12:16:28,973:INFO:_master_model_container: 8
2024-11-06 12:16:28,973:INFO:_display_container: 2
2024-11-06 12:16:28,973:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-06 12:16:28,973:INFO:create_model() successfully completed......................................
2024-11-06 12:16:29,050:INFO:SubProcess create_model() end ==================================
2024-11-06 12:16:29,050:INFO:Creating metrics dataframe
2024-11-06 12:16:29,051:INFO:Initializing Ada Boost Classifier
2024-11-06 12:16:29,051:INFO:Total runtime is 0.1595401167869568 minutes
2024-11-06 12:16:29,051:INFO:SubProcess create_model() called ==================================
2024-11-06 12:16:29,052:INFO:Initializing create_model()
2024-11-06 12:16:29,052:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x72bf6af7d350>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x72bf6b4528d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 12:16:29,052:INFO:Checking exceptions
2024-11-06 12:16:29,052:INFO:Importing libraries
2024-11-06 12:16:29,052:INFO:Copying training dataset
2024-11-06 12:16:29,107:INFO:Defining folds
2024-11-06 12:16:29,108:INFO:Declaring metric variables
2024-11-06 12:16:29,108:INFO:Importing untrained model
2024-11-06 12:16:29,108:INFO:Ada Boost Classifier Imported successfully
2024-11-06 12:16:29,108:INFO:Starting cross validation
2024-11-06 12:16:29,109:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 12:16:29,191:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 12:16:29,230:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 12:16:29,247:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 12:16:29,275:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 12:16:29,298:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 12:16:29,313:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 12:16:29,347:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 12:16:29,365:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 12:16:29,403:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 12:16:29,436:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 12:16:30,061:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:30,150:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:30,200:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:30,230:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:30,271:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:30,277:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:30,322:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:30,324:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:30,327:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:30,346:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:30,394:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:30,396:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:30,409:INFO:Calculating mean and std
2024-11-06 12:16:30,410:INFO:Creating metrics dataframe
2024-11-06 12:16:30,413:INFO:Uploading results into container
2024-11-06 12:16:30,414:INFO:Uploading model into container now
2024-11-06 12:16:30,414:INFO:_master_model_container: 9
2024-11-06 12:16:30,414:INFO:_display_container: 2
2024-11-06 12:16:30,415:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=6347)
2024-11-06 12:16:30,415:INFO:create_model() successfully completed......................................
2024-11-06 12:16:30,490:INFO:SubProcess create_model() end ==================================
2024-11-06 12:16:30,490:INFO:Creating metrics dataframe
2024-11-06 12:16:30,491:INFO:Initializing Gradient Boosting Classifier
2024-11-06 12:16:30,491:INFO:Total runtime is 0.18354169925053915 minutes
2024-11-06 12:16:30,492:INFO:SubProcess create_model() called ==================================
2024-11-06 12:16:30,492:INFO:Initializing create_model()
2024-11-06 12:16:30,492:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x72bf6af7d350>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x72bf6b4528d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 12:16:30,492:INFO:Checking exceptions
2024-11-06 12:16:30,492:INFO:Importing libraries
2024-11-06 12:16:30,492:INFO:Copying training dataset
2024-11-06 12:16:30,548:INFO:Defining folds
2024-11-06 12:16:30,548:INFO:Declaring metric variables
2024-11-06 12:16:30,548:INFO:Importing untrained model
2024-11-06 12:16:30,548:INFO:Gradient Boosting Classifier Imported successfully
2024-11-06 12:16:30,548:INFO:Starting cross validation
2024-11-06 12:16:30,550:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 12:16:46,312:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:46,315:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:46,471:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:46,502:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:46,651:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:46,713:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:46,884:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:47,616:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:47,618:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:47,775:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:48,084:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:48,207:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:48,220:INFO:Calculating mean and std
2024-11-06 12:16:48,221:INFO:Creating metrics dataframe
2024-11-06 12:16:48,224:INFO:Uploading results into container
2024-11-06 12:16:48,225:INFO:Uploading model into container now
2024-11-06 12:16:48,225:INFO:_master_model_container: 10
2024-11-06 12:16:48,225:INFO:_display_container: 2
2024-11-06 12:16:48,226:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6347, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-06 12:16:48,226:INFO:create_model() successfully completed......................................
2024-11-06 12:16:48,302:INFO:SubProcess create_model() end ==================================
2024-11-06 12:16:48,303:INFO:Creating metrics dataframe
2024-11-06 12:16:48,304:INFO:Initializing Linear Discriminant Analysis
2024-11-06 12:16:48,304:INFO:Total runtime is 0.4804194688796997 minutes
2024-11-06 12:16:48,304:INFO:SubProcess create_model() called ==================================
2024-11-06 12:16:48,304:INFO:Initializing create_model()
2024-11-06 12:16:48,304:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x72bf6af7d350>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x72bf6b4528d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 12:16:48,304:INFO:Checking exceptions
2024-11-06 12:16:48,304:INFO:Importing libraries
2024-11-06 12:16:48,305:INFO:Copying training dataset
2024-11-06 12:16:48,361:INFO:Defining folds
2024-11-06 12:16:48,361:INFO:Declaring metric variables
2024-11-06 12:16:48,361:INFO:Importing untrained model
2024-11-06 12:16:48,361:INFO:Linear Discriminant Analysis Imported successfully
2024-11-06 12:16:48,361:INFO:Starting cross validation
2024-11-06 12:16:48,363:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 12:16:48,537:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:48,567:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:48,630:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:48,647:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:48,667:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:48,679:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:48,695:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:48,712:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:48,722:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:48,727:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 12:16:48,733:INFO:Calculating mean and std
2024-11-06 12:16:48,734:INFO:Creating metrics dataframe
2024-11-06 12:16:48,736:INFO:Uploading results into container
2024-11-06 12:16:48,737:INFO:Uploading model into container now
2024-11-06 12:16:48,737:INFO:_master_model_container: 11
2024-11-06 12:16:48,737:INFO:_display_container: 2
2024-11-06 12:16:48,738:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-06 12:16:48,738:INFO:create_model() successfully completed......................................
2024-11-06 12:16:48,813:INFO:SubProcess create_model() end ==================================
2024-11-06 12:16:48,813:INFO:Creating metrics dataframe
2024-11-06 12:16:48,815:INFO:Initializing Extra Trees Classifier
2024-11-06 12:16:48,815:INFO:Total runtime is 0.4889270305633545 minutes
2024-11-06 12:16:48,815:INFO:SubProcess create_model() called ==================================
2024-11-06 12:16:48,815:INFO:Initializing create_model()
2024-11-06 12:16:48,815:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x72bf6af7d350>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x72bf6b4528d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 12:16:48,815:INFO:Checking exceptions
2024-11-06 12:16:48,815:INFO:Importing libraries
2024-11-06 12:16:48,815:INFO:Copying training dataset
2024-11-06 12:16:48,871:INFO:Defining folds
2024-11-06 12:16:48,871:INFO:Declaring metric variables
2024-11-06 12:16:48,871:INFO:Importing untrained model
2024-11-06 12:16:48,872:INFO:Extra Trees Classifier Imported successfully
2024-11-06 12:16:48,872:INFO:Starting cross validation
2024-11-06 12:16:48,873:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 12:16:49,382:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:16:49,447:INFO:Calculating mean and std
2024-11-06 12:16:49,448:INFO:Creating metrics dataframe
2024-11-06 12:16:49,449:INFO:Uploading results into container
2024-11-06 12:16:49,450:INFO:Uploading model into container now
2024-11-06 12:16:49,450:INFO:_master_model_container: 12
2024-11-06 12:16:49,450:INFO:_display_container: 2
2024-11-06 12:16:49,450:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=6347, verbose=0,
                     warm_start=False)
2024-11-06 12:16:49,450:INFO:create_model() successfully completed......................................
2024-11-06 12:16:49,517:INFO:SubProcess create_model() end ==================================
2024-11-06 12:16:49,517:INFO:Creating metrics dataframe
2024-11-06 12:16:49,519:INFO:Initializing Extreme Gradient Boosting
2024-11-06 12:16:49,519:INFO:Total runtime is 0.5006688276926676 minutes
2024-11-06 12:16:49,519:INFO:SubProcess create_model() called ==================================
2024-11-06 12:16:49,519:INFO:Initializing create_model()
2024-11-06 12:16:49,519:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x72bf6af7d350>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x72bf6b4528d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 12:16:49,519:INFO:Checking exceptions
2024-11-06 12:16:49,519:INFO:Importing libraries
2024-11-06 12:16:49,519:INFO:Copying training dataset
2024-11-06 12:16:49,575:INFO:Defining folds
2024-11-06 12:16:49,575:INFO:Declaring metric variables
2024-11-06 12:16:49,575:INFO:Importing untrained model
2024-11-06 12:16:49,576:INFO:Extreme Gradient Boosting Imported successfully
2024-11-06 12:16:49,576:INFO:Starting cross validation
2024-11-06 12:16:49,577:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 12:16:59,675:INFO:Calculating mean and std
2024-11-06 12:16:59,676:INFO:Creating metrics dataframe
2024-11-06 12:16:59,678:INFO:Uploading results into container
2024-11-06 12:16:59,678:INFO:Uploading model into container now
2024-11-06 12:16:59,679:INFO:_master_model_container: 13
2024-11-06 12:16:59,679:INFO:_display_container: 2
2024-11-06 12:16:59,680:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-11-06 12:16:59,680:INFO:create_model() successfully completed......................................
2024-11-06 12:16:59,755:INFO:SubProcess create_model() end ==================================
2024-11-06 12:16:59,755:INFO:Creating metrics dataframe
2024-11-06 12:16:59,756:INFO:Initializing Light Gradient Boosting Machine
2024-11-06 12:16:59,756:INFO:Total runtime is 0.6712900479634603 minutes
2024-11-06 12:16:59,756:INFO:SubProcess create_model() called ==================================
2024-11-06 12:16:59,757:INFO:Initializing create_model()
2024-11-06 12:16:59,757:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x72bf6af7d350>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x72bf6b4528d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 12:16:59,757:INFO:Checking exceptions
2024-11-06 12:16:59,757:INFO:Importing libraries
2024-11-06 12:16:59,757:INFO:Copying training dataset
2024-11-06 12:16:59,813:INFO:Defining folds
2024-11-06 12:16:59,813:INFO:Declaring metric variables
2024-11-06 12:16:59,813:INFO:Importing untrained model
2024-11-06 12:16:59,813:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-06 12:16:59,813:INFO:Starting cross validation
2024-11-06 12:16:59,815:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 12:17:13,294:INFO:Calculating mean and std
2024-11-06 12:17:13,295:INFO:Creating metrics dataframe
2024-11-06 12:17:13,296:INFO:Uploading results into container
2024-11-06 12:17:13,297:INFO:Uploading model into container now
2024-11-06 12:17:13,297:INFO:_master_model_container: 14
2024-11-06 12:17:13,297:INFO:_display_container: 2
2024-11-06 12:17:13,298:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6347, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-06 12:17:13,298:INFO:create_model() successfully completed......................................
2024-11-06 12:17:13,419:INFO:SubProcess create_model() end ==================================
2024-11-06 12:17:13,419:INFO:Creating metrics dataframe
2024-11-06 12:17:13,421:INFO:Initializing Dummy Classifier
2024-11-06 12:17:13,422:INFO:Total runtime is 0.8990422407786052 minutes
2024-11-06 12:17:13,422:INFO:SubProcess create_model() called ==================================
2024-11-06 12:17:13,422:INFO:Initializing create_model()
2024-11-06 12:17:13,422:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x72bf6af7d350>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x72bf6b4528d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 12:17:13,422:INFO:Checking exceptions
2024-11-06 12:17:13,422:INFO:Importing libraries
2024-11-06 12:17:13,422:INFO:Copying training dataset
2024-11-06 12:17:13,482:INFO:Defining folds
2024-11-06 12:17:13,482:INFO:Declaring metric variables
2024-11-06 12:17:13,482:INFO:Importing untrained model
2024-11-06 12:17:13,482:INFO:Dummy Classifier Imported successfully
2024-11-06 12:17:13,482:INFO:Starting cross validation
2024-11-06 12:17:13,484:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 12:17:13,620:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:17:13,632:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:17:13,667:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:17:13,684:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:17:13,702:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:17:13,710:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:17:13,773:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:17:13,782:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:17:13,786:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:17:13,802:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 12:17:13,808:INFO:Calculating mean and std
2024-11-06 12:17:13,809:INFO:Creating metrics dataframe
2024-11-06 12:17:13,810:INFO:Uploading results into container
2024-11-06 12:17:13,811:INFO:Uploading model into container now
2024-11-06 12:17:13,811:INFO:_master_model_container: 15
2024-11-06 12:17:13,811:INFO:_display_container: 2
2024-11-06 12:17:13,811:INFO:DummyClassifier(constant=None, random_state=6347, strategy='prior')
2024-11-06 12:17:13,811:INFO:create_model() successfully completed......................................
2024-11-06 12:17:13,886:INFO:SubProcess create_model() end ==================================
2024-11-06 12:17:13,886:INFO:Creating metrics dataframe
2024-11-06 12:17:13,889:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-06 12:17:13,890:INFO:Initializing create_model()
2024-11-06 12:17:13,890:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x72bf6af7d350>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6347, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 12:17:13,890:INFO:Checking exceptions
2024-11-06 12:17:13,891:INFO:Importing libraries
2024-11-06 12:17:13,891:INFO:Copying training dataset
2024-11-06 12:17:13,954:INFO:Defining folds
2024-11-06 12:17:13,955:INFO:Declaring metric variables
2024-11-06 12:17:13,955:INFO:Importing untrained model
2024-11-06 12:17:13,955:INFO:Declaring custom model
2024-11-06 12:17:13,955:INFO:Random Forest Classifier Imported successfully
2024-11-06 12:17:13,957:INFO:Cross validation set to False
2024-11-06 12:17:13,957:INFO:Fitting Model
2024-11-06 12:17:14,144:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6347, verbose=0,
                       warm_start=False)
2024-11-06 12:17:14,144:INFO:create_model() successfully completed......................................
2024-11-06 12:17:14,225:INFO:_master_model_container: 15
2024-11-06 12:17:14,226:INFO:_display_container: 2
2024-11-06 12:17:14,226:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6347, verbose=0,
                       warm_start=False)
2024-11-06 12:17:14,226:INFO:compare_models() successfully completed......................................
