2024-11-06 12:26:53,101:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 12:26:53,102:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 12:26:53,102:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 12:26:53,102:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 14:26:49,606:INFO:PyCaret ClassificationExperiment
2024-11-06 14:26:49,606:INFO:Logging name: clf-default-name
2024-11-06 14:26:49,606:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-06 14:26:49,607:INFO:version 3.3.2
2024-11-06 14:26:49,607:INFO:Initializing setup()
2024-11-06 14:26:49,607:INFO:self.USI: 8522
2024-11-06 14:26:49,607:INFO:self._variable_keys: {'gpu_param', 'gpu_n_jobs_param', 'html_param', 'memory', 'X_train', 'y_test', 'fix_imbalance', '_ml_usecase', 'fold_groups_param', 'exp_id', 'n_jobs_param', 'X_test', 'USI', 'is_multiclass', 'pipeline', 'fold_generator', 'fold_shuffle_param', 'y', 'seed', 'exp_name_log', 'logging_param', '_available_plots', 'idx', 'log_plots_param', 'X', 'data', 'y_train', 'target_param'}
2024-11-06 14:26:49,607:INFO:Checking environment
2024-11-06 14:26:49,607:INFO:python_version: 3.11.10
2024-11-06 14:26:49,607:INFO:python_build: ('main', 'Oct  3 2024 07:29:13')
2024-11-06 14:26:49,607:INFO:machine: x86_64
2024-11-06 14:26:49,607:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 14:26:49,608:INFO:Memory: svmem(total=33037213696, available=23512838144, percent=28.8, used=8829792256, free=11759038464, active=9722789888, inactive=8203862016, buffers=814522368, cached=11633860608, shared=221196288, slab=1926746112)
2024-11-06 14:26:49,611:INFO:Physical Core: 8
2024-11-06 14:26:49,611:INFO:Logical Core: 16
2024-11-06 14:26:49,611:INFO:Checking libraries
2024-11-06 14:26:49,611:INFO:System:
2024-11-06 14:26:49,612:INFO:    python: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0]
2024-11-06 14:26:49,612:INFO:executable: /home/cid34senhas/miniconda3/envs/myenv/bin/python
2024-11-06 14:26:49,612:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 14:26:49,612:INFO:PyCaret required dependencies:
2024-11-06 14:26:49,644:INFO:                 pip: 24.2
2024-11-06 14:26:49,644:INFO:          setuptools: 75.1.0
2024-11-06 14:26:49,644:INFO:             pycaret: 3.3.2
2024-11-06 14:26:49,644:INFO:             IPython: 8.29.0
2024-11-06 14:26:49,644:INFO:          ipywidgets: 8.1.5
2024-11-06 14:26:49,644:INFO:                tqdm: 4.66.6
2024-11-06 14:26:49,644:INFO:               numpy: 1.26.4
2024-11-06 14:26:49,644:INFO:              pandas: 2.1.4
2024-11-06 14:26:49,644:INFO:              jinja2: 3.1.4
2024-11-06 14:26:49,644:INFO:               scipy: 1.11.4
2024-11-06 14:26:49,644:INFO:              joblib: 1.3.2
2024-11-06 14:26:49,644:INFO:             sklearn: 1.4.2
2024-11-06 14:26:49,644:INFO:                pyod: 2.0.2
2024-11-06 14:26:49,644:INFO:            imblearn: 0.12.4
2024-11-06 14:26:49,644:INFO:   category_encoders: 2.6.4
2024-11-06 14:26:49,644:INFO:            lightgbm: 4.5.0
2024-11-06 14:26:49,644:INFO:               numba: 0.60.0
2024-11-06 14:26:49,644:INFO:            requests: 2.32.3
2024-11-06 14:26:49,645:INFO:          matplotlib: 3.7.5
2024-11-06 14:26:49,645:INFO:          scikitplot: 0.3.7
2024-11-06 14:26:49,645:INFO:         yellowbrick: 1.5
2024-11-06 14:26:49,645:INFO:              plotly: 5.24.1
2024-11-06 14:26:49,645:INFO:    plotly-resampler: Not installed
2024-11-06 14:26:49,645:INFO:             kaleido: 0.2.1
2024-11-06 14:26:49,645:INFO:           schemdraw: 0.15
2024-11-06 14:26:49,645:INFO:         statsmodels: 0.14.4
2024-11-06 14:26:49,645:INFO:              sktime: 0.26.0
2024-11-06 14:26:49,645:INFO:               tbats: 1.1.3
2024-11-06 14:26:49,645:INFO:            pmdarima: 2.0.4
2024-11-06 14:26:49,645:INFO:              psutil: 6.1.0
2024-11-06 14:26:49,645:INFO:          markupsafe: 3.0.2
2024-11-06 14:26:49,645:INFO:             pickle5: Not installed
2024-11-06 14:26:49,645:INFO:         cloudpickle: 3.1.0
2024-11-06 14:26:49,645:INFO:         deprecation: 2.1.0
2024-11-06 14:26:49,645:INFO:              xxhash: 3.5.0
2024-11-06 14:26:49,645:INFO:           wurlitzer: 3.1.1
2024-11-06 14:26:49,645:INFO:PyCaret optional dependencies:
2024-11-06 14:26:49,666:INFO:                shap: Not installed
2024-11-06 14:26:49,666:INFO:           interpret: Not installed
2024-11-06 14:26:49,666:INFO:                umap: Not installed
2024-11-06 14:26:49,666:INFO:     ydata_profiling: Not installed
2024-11-06 14:26:49,666:INFO:  explainerdashboard: Not installed
2024-11-06 14:26:49,666:INFO:             autoviz: Not installed
2024-11-06 14:26:49,666:INFO:           fairlearn: Not installed
2024-11-06 14:26:49,666:INFO:          deepchecks: Not installed
2024-11-06 14:26:49,666:INFO:             xgboost: 2.1.2
2024-11-06 14:26:49,666:INFO:            catboost: Not installed
2024-11-06 14:26:49,666:INFO:              kmodes: Not installed
2024-11-06 14:26:49,666:INFO:             mlxtend: Not installed
2024-11-06 14:26:49,666:INFO:       statsforecast: Not installed
2024-11-06 14:26:49,666:INFO:        tune_sklearn: Not installed
2024-11-06 14:26:49,666:INFO:                 ray: Not installed
2024-11-06 14:26:49,666:INFO:            hyperopt: Not installed
2024-11-06 14:26:49,666:INFO:              optuna: Not installed
2024-11-06 14:26:49,666:INFO:               skopt: Not installed
2024-11-06 14:26:49,666:INFO:              mlflow: Not installed
2024-11-06 14:26:49,666:INFO:              gradio: Not installed
2024-11-06 14:26:49,666:INFO:             fastapi: Not installed
2024-11-06 14:26:49,667:INFO:             uvicorn: Not installed
2024-11-06 14:26:49,667:INFO:              m2cgen: Not installed
2024-11-06 14:26:49,667:INFO:           evidently: Not installed
2024-11-06 14:26:49,667:INFO:               fugue: Not installed
2024-11-06 14:26:49,667:INFO:           streamlit: Not installed
2024-11-06 14:26:49,667:INFO:             prophet: Not installed
2024-11-06 14:26:49,667:INFO:None
2024-11-06 14:26:49,667:INFO:Set up data.
2024-11-06 14:26:49,987:INFO:Set up folding strategy.
2024-11-06 14:26:49,987:INFO:Set up train/test split.
2024-11-06 14:26:50,116:INFO:Set up index.
2024-11-06 14:26:50,119:INFO:Assigning column types.
2024-11-06 14:26:50,399:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-06 14:26:50,435:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 14:26:50,437:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 14:26:50,461:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 14:26:50,463:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 14:26:50,499:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 14:26:50,499:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 14:26:50,521:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 14:26:50,523:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 14:26:50,523:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-06 14:26:50,559:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 14:26:50,582:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 14:26:50,584:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 14:26:50,620:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 14:26:50,642:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 14:26:50,644:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 14:26:50,645:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-06 14:26:50,702:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 14:26:50,704:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 14:26:50,760:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 14:26:50,762:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 14:26:50,763:INFO:Preparing preprocessing pipeline...
2024-11-06 14:26:50,784:INFO:Set up simple imputation.
2024-11-06 14:26:50,922:INFO:Set up column name cleaning.
2024-11-06 14:26:51,538:INFO:Finished creating preprocessing pipeline.
2024-11-06 14:26:51,548:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['diagnostics_Image-original_Mean',
                                             'diagnostics_Image-original_Maximum',
                                             'diagnostics_Mask-original_VoxelNum',
                                             'original_shape_Elongation',
                                             'original_shape_Flatness',
                                             'original_shape_LeastAxisLength',
                                             'original_shape_MajorAxisLength',
                                             'ori...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-06 14:26:51,548:INFO:Creating final display dataframe.
2024-11-06 14:26:55,094:INFO:Setup _display_container:                     Description             Value
0                    Session id              3753
1                        Target        Transition
2                   Target type        Multiclass
3           Original data shape       (295, 2014)
4        Transformed data shape       (295, 2014)
5   Transformed train set shape       (206, 2014)
6    Transformed test set shape        (89, 2014)
7              Numeric features              2013
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              8522
2024-11-06 14:26:55,156:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 14:26:55,158:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 14:26:55,214:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 14:26:55,217:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 14:26:55,217:INFO:setup() successfully completed in 5.61s...............
2024-11-06 14:27:15,517:INFO:PyCaret ClassificationExperiment
2024-11-06 14:27:15,517:INFO:Logging name: clf-default-name
2024-11-06 14:27:15,517:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-06 14:27:15,517:INFO:version 3.3.2
2024-11-06 14:27:15,517:INFO:Initializing setup()
2024-11-06 14:27:15,517:INFO:self.USI: 2a81
2024-11-06 14:27:15,517:INFO:self._variable_keys: {'gpu_param', 'gpu_n_jobs_param', 'html_param', 'memory', 'X_train', 'y_test', 'fix_imbalance', '_ml_usecase', 'fold_groups_param', 'exp_id', 'n_jobs_param', 'X_test', 'USI', 'is_multiclass', 'pipeline', 'fold_generator', 'fold_shuffle_param', 'y', 'seed', 'exp_name_log', 'logging_param', '_available_plots', 'idx', 'log_plots_param', 'X', 'data', 'y_train', 'target_param'}
2024-11-06 14:27:15,517:INFO:Checking environment
2024-11-06 14:27:15,517:INFO:python_version: 3.11.10
2024-11-06 14:27:15,517:INFO:python_build: ('main', 'Oct  3 2024 07:29:13')
2024-11-06 14:27:15,517:INFO:machine: x86_64
2024-11-06 14:27:15,517:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 14:27:15,517:INFO:Memory: svmem(total=33037213696, available=23451033600, percent=29.0, used=8887365632, free=11690549248, active=9793458176, inactive=8210427904, buffers=814870528, cached=11644428288, shared=225394688, slab=1926819840)
2024-11-06 14:27:15,518:INFO:Physical Core: 8
2024-11-06 14:27:15,518:INFO:Logical Core: 16
2024-11-06 14:27:15,518:INFO:Checking libraries
2024-11-06 14:27:15,518:INFO:System:
2024-11-06 14:27:15,518:INFO:    python: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0]
2024-11-06 14:27:15,518:INFO:executable: /home/cid34senhas/miniconda3/envs/myenv/bin/python
2024-11-06 14:27:15,518:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 14:27:15,518:INFO:PyCaret required dependencies:
2024-11-06 14:27:15,518:INFO:                 pip: 24.2
2024-11-06 14:27:15,518:INFO:          setuptools: 75.1.0
2024-11-06 14:27:15,518:INFO:             pycaret: 3.3.2
2024-11-06 14:27:15,518:INFO:             IPython: 8.29.0
2024-11-06 14:27:15,518:INFO:          ipywidgets: 8.1.5
2024-11-06 14:27:15,518:INFO:                tqdm: 4.66.6
2024-11-06 14:27:15,518:INFO:               numpy: 1.26.4
2024-11-06 14:27:15,518:INFO:              pandas: 2.1.4
2024-11-06 14:27:15,518:INFO:              jinja2: 3.1.4
2024-11-06 14:27:15,518:INFO:               scipy: 1.11.4
2024-11-06 14:27:15,518:INFO:              joblib: 1.3.2
2024-11-06 14:27:15,518:INFO:             sklearn: 1.4.2
2024-11-06 14:27:15,518:INFO:                pyod: 2.0.2
2024-11-06 14:27:15,518:INFO:            imblearn: 0.12.4
2024-11-06 14:27:15,518:INFO:   category_encoders: 2.6.4
2024-11-06 14:27:15,518:INFO:            lightgbm: 4.5.0
2024-11-06 14:27:15,518:INFO:               numba: 0.60.0
2024-11-06 14:27:15,518:INFO:            requests: 2.32.3
2024-11-06 14:27:15,518:INFO:          matplotlib: 3.7.5
2024-11-06 14:27:15,518:INFO:          scikitplot: 0.3.7
2024-11-06 14:27:15,519:INFO:         yellowbrick: 1.5
2024-11-06 14:27:15,519:INFO:              plotly: 5.24.1
2024-11-06 14:27:15,519:INFO:    plotly-resampler: Not installed
2024-11-06 14:27:15,519:INFO:             kaleido: 0.2.1
2024-11-06 14:27:15,519:INFO:           schemdraw: 0.15
2024-11-06 14:27:15,519:INFO:         statsmodels: 0.14.4
2024-11-06 14:27:15,519:INFO:              sktime: 0.26.0
2024-11-06 14:27:15,519:INFO:               tbats: 1.1.3
2024-11-06 14:27:15,519:INFO:            pmdarima: 2.0.4
2024-11-06 14:27:15,519:INFO:              psutil: 6.1.0
2024-11-06 14:27:15,519:INFO:          markupsafe: 3.0.2
2024-11-06 14:27:15,519:INFO:             pickle5: Not installed
2024-11-06 14:27:15,519:INFO:         cloudpickle: 3.1.0
2024-11-06 14:27:15,519:INFO:         deprecation: 2.1.0
2024-11-06 14:27:15,519:INFO:              xxhash: 3.5.0
2024-11-06 14:27:15,519:INFO:           wurlitzer: 3.1.1
2024-11-06 14:27:15,519:INFO:PyCaret optional dependencies:
2024-11-06 14:27:15,519:INFO:                shap: Not installed
2024-11-06 14:27:15,519:INFO:           interpret: Not installed
2024-11-06 14:27:15,519:INFO:                umap: Not installed
2024-11-06 14:27:15,519:INFO:     ydata_profiling: Not installed
2024-11-06 14:27:15,519:INFO:  explainerdashboard: Not installed
2024-11-06 14:27:15,519:INFO:             autoviz: Not installed
2024-11-06 14:27:15,519:INFO:           fairlearn: Not installed
2024-11-06 14:27:15,519:INFO:          deepchecks: Not installed
2024-11-06 14:27:15,519:INFO:             xgboost: 2.1.2
2024-11-06 14:27:15,519:INFO:            catboost: Not installed
2024-11-06 14:27:15,519:INFO:              kmodes: Not installed
2024-11-06 14:27:15,519:INFO:             mlxtend: Not installed
2024-11-06 14:27:15,519:INFO:       statsforecast: Not installed
2024-11-06 14:27:15,519:INFO:        tune_sklearn: Not installed
2024-11-06 14:27:15,519:INFO:                 ray: Not installed
2024-11-06 14:27:15,519:INFO:            hyperopt: Not installed
2024-11-06 14:27:15,519:INFO:              optuna: Not installed
2024-11-06 14:27:15,519:INFO:               skopt: Not installed
2024-11-06 14:27:15,519:INFO:              mlflow: Not installed
2024-11-06 14:27:15,519:INFO:              gradio: Not installed
2024-11-06 14:27:15,519:INFO:             fastapi: Not installed
2024-11-06 14:27:15,519:INFO:             uvicorn: Not installed
2024-11-06 14:27:15,519:INFO:              m2cgen: Not installed
2024-11-06 14:27:15,519:INFO:           evidently: Not installed
2024-11-06 14:27:15,519:INFO:               fugue: Not installed
2024-11-06 14:27:15,519:INFO:           streamlit: Not installed
2024-11-06 14:27:15,519:INFO:             prophet: Not installed
2024-11-06 14:27:15,519:INFO:None
2024-11-06 14:27:15,519:INFO:Set up data.
2024-11-06 14:27:15,736:INFO:Set up folding strategy.
2024-11-06 14:27:15,736:INFO:Set up train/test split.
2024-11-06 14:27:15,985:INFO:Set up index.
2024-11-06 14:27:15,989:INFO:Assigning column types.
2024-11-06 14:27:16,267:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-06 14:27:16,303:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 14:27:16,304:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 14:27:16,325:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 14:27:16,328:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 14:27:16,363:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 14:27:16,363:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 14:27:16,385:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 14:27:16,387:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 14:27:16,388:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-06 14:27:16,423:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 14:27:16,444:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 14:27:16,446:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 14:27:16,481:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 14:27:16,503:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 14:27:16,505:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 14:27:16,505:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-06 14:27:16,561:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 14:27:16,563:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 14:27:16,619:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 14:27:16,621:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 14:27:16,621:INFO:Preparing preprocessing pipeline...
2024-11-06 14:27:16,645:INFO:Set up simple imputation.
2024-11-06 14:27:16,666:INFO:Set up column name cleaning.
2024-11-06 14:27:17,403:INFO:Finished creating preprocessing pipeline.
2024-11-06 14:27:17,413:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['diagnostics_Image-original_Mean',
                                             'diagnostics_Image-original_Maximum',
                                             'diagnostics_Mask-original_VoxelNum',
                                             'original_shape_Elongation',
                                             'original_shape_Flatness',
                                             'original_shape_LeastAxisLength',
                                             'original_shape_MajorAxisLength',
                                             'ori...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-06 14:27:17,413:INFO:Creating final display dataframe.
2024-11-06 14:27:20,774:INFO:Setup _display_container:                     Description             Value
0                    Session id              8281
1                        Target        Transition
2                   Target type        Multiclass
3           Original data shape       (295, 2014)
4        Transformed data shape       (295, 2014)
5   Transformed train set shape       (206, 2014)
6    Transformed test set shape        (89, 2014)
7              Numeric features              2013
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              2a81
2024-11-06 14:27:20,835:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 14:27:20,837:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 14:27:20,894:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 14:27:20,896:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 14:27:20,897:INFO:setup() successfully completed in 5.38s...............
2024-11-06 14:27:22,713:INFO:Initializing compare_models()
2024-11-06 14:27:22,713:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e9cee7b1d90>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7e9cee7b1d90>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-06 14:27:22,713:INFO:Checking exceptions
2024-11-06 14:27:22,933:INFO:Preparing display monitor
2024-11-06 14:27:22,947:INFO:Initializing Logistic Regression
2024-11-06 14:27:22,948:INFO:Total runtime is 2.5073687235514325e-06 minutes
2024-11-06 14:27:22,949:INFO:SubProcess create_model() called ==================================
2024-11-06 14:27:22,950:INFO:Initializing create_model()
2024-11-06 14:27:22,950:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e9cee7b1d90>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e9d042e2590>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 14:27:22,950:INFO:Checking exceptions
2024-11-06 14:27:22,950:INFO:Importing libraries
2024-11-06 14:27:22,950:INFO:Copying training dataset
2024-11-06 14:27:23,219:INFO:Defining folds
2024-11-06 14:27:23,219:INFO:Declaring metric variables
2024-11-06 14:27:23,221:INFO:Importing untrained model
2024-11-06 14:27:23,223:INFO:Logistic Regression Imported successfully
2024-11-06 14:27:23,227:INFO:Starting cross validation
2024-11-06 14:27:23,231:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 14:27:31,183:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 14:27:31,255:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:31,331:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 14:27:31,418:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:31,516:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 14:27:31,582:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 14:27:31,591:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 14:27:31,592:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:31,634:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:31,642:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:32,066:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 14:27:32,078:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 14:27:32,098:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 14:27:32,117:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:32,124:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 14:27:32,125:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:32,141:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:32,172:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:32,353:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 14:27:32,393:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:32,403:INFO:Calculating mean and std
2024-11-06 14:27:32,406:INFO:Creating metrics dataframe
2024-11-06 14:27:32,411:INFO:Uploading results into container
2024-11-06 14:27:32,412:INFO:Uploading model into container now
2024-11-06 14:27:32,413:INFO:_master_model_container: 1
2024-11-06 14:27:32,413:INFO:_display_container: 2
2024-11-06 14:27:32,414:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=8281, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-06 14:27:32,414:INFO:create_model() successfully completed......................................
2024-11-06 14:27:32,578:INFO:SubProcess create_model() end ==================================
2024-11-06 14:27:32,578:INFO:Creating metrics dataframe
2024-11-06 14:27:32,583:INFO:Initializing K Neighbors Classifier
2024-11-06 14:27:32,584:INFO:Total runtime is 0.16060408353805544 minutes
2024-11-06 14:27:32,586:INFO:SubProcess create_model() called ==================================
2024-11-06 14:27:32,587:INFO:Initializing create_model()
2024-11-06 14:27:32,587:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e9cee7b1d90>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e9d042e2590>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 14:27:32,587:INFO:Checking exceptions
2024-11-06 14:27:32,587:INFO:Importing libraries
2024-11-06 14:27:32,587:INFO:Copying training dataset
2024-11-06 14:27:32,746:INFO:Defining folds
2024-11-06 14:27:32,746:INFO:Declaring metric variables
2024-11-06 14:27:32,748:INFO:Importing untrained model
2024-11-06 14:27:32,750:INFO:K Neighbors Classifier Imported successfully
2024-11-06 14:27:32,755:INFO:Starting cross validation
2024-11-06 14:27:32,759:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 14:27:33,476:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:27:34,618:INFO:Calculating mean and std
2024-11-06 14:27:34,620:INFO:Creating metrics dataframe
2024-11-06 14:27:34,623:INFO:Uploading results into container
2024-11-06 14:27:34,623:INFO:Uploading model into container now
2024-11-06 14:27:34,624:INFO:_master_model_container: 2
2024-11-06 14:27:34,624:INFO:_display_container: 2
2024-11-06 14:27:34,624:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-06 14:27:34,624:INFO:create_model() successfully completed......................................
2024-11-06 14:27:34,753:INFO:SubProcess create_model() end ==================================
2024-11-06 14:27:34,753:INFO:Creating metrics dataframe
2024-11-06 14:27:34,758:INFO:Initializing Naive Bayes
2024-11-06 14:27:34,758:INFO:Total runtime is 0.196848205725352 minutes
2024-11-06 14:27:34,761:INFO:SubProcess create_model() called ==================================
2024-11-06 14:27:34,761:INFO:Initializing create_model()
2024-11-06 14:27:34,761:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e9cee7b1d90>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e9d042e2590>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 14:27:34,761:INFO:Checking exceptions
2024-11-06 14:27:34,762:INFO:Importing libraries
2024-11-06 14:27:34,762:INFO:Copying training dataset
2024-11-06 14:27:34,924:INFO:Defining folds
2024-11-06 14:27:34,924:INFO:Declaring metric variables
2024-11-06 14:27:34,927:INFO:Importing untrained model
2024-11-06 14:27:34,929:INFO:Naive Bayes Imported successfully
2024-11-06 14:27:34,934:INFO:Starting cross validation
2024-11-06 14:27:34,939:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 14:27:35,645:INFO:Calculating mean and std
2024-11-06 14:27:35,647:INFO:Creating metrics dataframe
2024-11-06 14:27:35,649:INFO:Uploading results into container
2024-11-06 14:27:35,650:INFO:Uploading model into container now
2024-11-06 14:27:35,650:INFO:_master_model_container: 3
2024-11-06 14:27:35,650:INFO:_display_container: 2
2024-11-06 14:27:35,650:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-06 14:27:35,650:INFO:create_model() successfully completed......................................
2024-11-06 14:27:35,786:INFO:SubProcess create_model() end ==================================
2024-11-06 14:27:35,786:INFO:Creating metrics dataframe
2024-11-06 14:27:35,791:INFO:Initializing Decision Tree Classifier
2024-11-06 14:27:35,791:INFO:Total runtime is 0.21405527989069623 minutes
2024-11-06 14:27:35,794:INFO:SubProcess create_model() called ==================================
2024-11-06 14:27:35,794:INFO:Initializing create_model()
2024-11-06 14:27:35,794:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e9cee7b1d90>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e9d042e2590>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 14:27:35,794:INFO:Checking exceptions
2024-11-06 14:27:35,794:INFO:Importing libraries
2024-11-06 14:27:35,794:INFO:Copying training dataset
2024-11-06 14:27:35,956:INFO:Defining folds
2024-11-06 14:27:35,956:INFO:Declaring metric variables
2024-11-06 14:27:35,958:INFO:Importing untrained model
2024-11-06 14:27:35,961:INFO:Decision Tree Classifier Imported successfully
2024-11-06 14:27:35,967:INFO:Starting cross validation
2024-11-06 14:27:35,971:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 14:27:36,886:INFO:Calculating mean and std
2024-11-06 14:27:36,889:INFO:Creating metrics dataframe
2024-11-06 14:27:36,890:INFO:Uploading results into container
2024-11-06 14:27:36,891:INFO:Uploading model into container now
2024-11-06 14:27:36,891:INFO:_master_model_container: 4
2024-11-06 14:27:36,891:INFO:_display_container: 2
2024-11-06 14:27:36,892:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=8281, splitter='best')
2024-11-06 14:27:36,892:INFO:create_model() successfully completed......................................
2024-11-06 14:27:37,020:INFO:SubProcess create_model() end ==================================
2024-11-06 14:27:37,020:INFO:Creating metrics dataframe
2024-11-06 14:27:37,025:INFO:Initializing SVM - Linear Kernel
2024-11-06 14:27:37,025:INFO:Total runtime is 0.2346236824989319 minutes
2024-11-06 14:27:37,027:INFO:SubProcess create_model() called ==================================
2024-11-06 14:27:37,027:INFO:Initializing create_model()
2024-11-06 14:27:37,028:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e9cee7b1d90>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e9d042e2590>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 14:27:37,028:INFO:Checking exceptions
2024-11-06 14:27:37,028:INFO:Importing libraries
2024-11-06 14:27:37,028:INFO:Copying training dataset
2024-11-06 14:27:37,188:INFO:Defining folds
2024-11-06 14:27:37,189:INFO:Declaring metric variables
2024-11-06 14:27:37,191:INFO:Importing untrained model
2024-11-06 14:27:37,195:INFO:SVM - Linear Kernel Imported successfully
2024-11-06 14:27:37,200:INFO:Starting cross validation
2024-11-06 14:27:37,204:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 14:27:37,694:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:37,699:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:27:37,720:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:37,725:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:27:37,763:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:37,768:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:27:37,791:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:37,795:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:27:37,824:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:37,828:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:27:37,833:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:37,838:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:27:37,883:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:37,886:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:27:37,923:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:37,926:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:27:37,962:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:37,965:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:27:37,984:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:37,987:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:27:37,995:INFO:Calculating mean and std
2024-11-06 14:27:37,997:INFO:Creating metrics dataframe
2024-11-06 14:27:38,000:INFO:Uploading results into container
2024-11-06 14:27:38,000:INFO:Uploading model into container now
2024-11-06 14:27:38,001:INFO:_master_model_container: 5
2024-11-06 14:27:38,001:INFO:_display_container: 2
2024-11-06 14:27:38,002:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=8281, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-06 14:27:38,002:INFO:create_model() successfully completed......................................
2024-11-06 14:27:38,129:INFO:SubProcess create_model() end ==================================
2024-11-06 14:27:38,129:INFO:Creating metrics dataframe
2024-11-06 14:27:38,134:INFO:Initializing Ridge Classifier
2024-11-06 14:27:38,134:INFO:Total runtime is 0.2531087160110474 minutes
2024-11-06 14:27:38,136:INFO:SubProcess create_model() called ==================================
2024-11-06 14:27:38,136:INFO:Initializing create_model()
2024-11-06 14:27:38,136:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e9cee7b1d90>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e9d042e2590>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 14:27:38,136:INFO:Checking exceptions
2024-11-06 14:27:38,136:INFO:Importing libraries
2024-11-06 14:27:38,136:INFO:Copying training dataset
2024-11-06 14:27:38,301:INFO:Defining folds
2024-11-06 14:27:38,301:INFO:Declaring metric variables
2024-11-06 14:27:38,304:INFO:Importing untrained model
2024-11-06 14:27:38,307:INFO:Ridge Classifier Imported successfully
2024-11-06 14:27:38,312:INFO:Starting cross validation
2024-11-06 14:27:38,315:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 14:27:38,547:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 14:27:38,588:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 14:27:38,656:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:38,683:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 14:27:38,723:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 14:27:38,728:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 14:27:38,746:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:38,801:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 14:27:38,817:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:38,840:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 14:27:38,849:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:38,854:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 14:27:38,854:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:38,883:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 14:27:38,919:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:38,938:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 14:27:38,956:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:38,966:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:38,989:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:39,033:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:39,047:INFO:Calculating mean and std
2024-11-06 14:27:39,049:INFO:Creating metrics dataframe
2024-11-06 14:27:39,052:INFO:Uploading results into container
2024-11-06 14:27:39,052:INFO:Uploading model into container now
2024-11-06 14:27:39,053:INFO:_master_model_container: 6
2024-11-06 14:27:39,053:INFO:_display_container: 2
2024-11-06 14:27:39,053:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=8281, solver='auto',
                tol=0.0001)
2024-11-06 14:27:39,053:INFO:create_model() successfully completed......................................
2024-11-06 14:27:39,191:INFO:SubProcess create_model() end ==================================
2024-11-06 14:27:39,191:INFO:Creating metrics dataframe
2024-11-06 14:27:39,196:INFO:Initializing Random Forest Classifier
2024-11-06 14:27:39,196:INFO:Total runtime is 0.2708187301953634 minutes
2024-11-06 14:27:39,199:INFO:SubProcess create_model() called ==================================
2024-11-06 14:27:39,199:INFO:Initializing create_model()
2024-11-06 14:27:39,199:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e9cee7b1d90>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e9d042e2590>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 14:27:39,199:INFO:Checking exceptions
2024-11-06 14:27:39,199:INFO:Importing libraries
2024-11-06 14:27:39,199:INFO:Copying training dataset
2024-11-06 14:27:39,359:INFO:Defining folds
2024-11-06 14:27:39,359:INFO:Declaring metric variables
2024-11-06 14:27:39,361:INFO:Importing untrained model
2024-11-06 14:27:39,364:INFO:Random Forest Classifier Imported successfully
2024-11-06 14:27:39,369:INFO:Starting cross validation
2024-11-06 14:27:39,372:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 14:27:40,466:INFO:Calculating mean and std
2024-11-06 14:27:40,468:INFO:Creating metrics dataframe
2024-11-06 14:27:40,471:INFO:Uploading results into container
2024-11-06 14:27:40,471:INFO:Uploading model into container now
2024-11-06 14:27:40,472:INFO:_master_model_container: 7
2024-11-06 14:27:40,472:INFO:_display_container: 2
2024-11-06 14:27:40,472:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=8281, verbose=0,
                       warm_start=False)
2024-11-06 14:27:40,473:INFO:create_model() successfully completed......................................
2024-11-06 14:27:40,612:INFO:SubProcess create_model() end ==================================
2024-11-06 14:27:40,613:INFO:Creating metrics dataframe
2024-11-06 14:27:40,618:INFO:Initializing Quadratic Discriminant Analysis
2024-11-06 14:27:40,618:INFO:Total runtime is 0.29450670083363856 minutes
2024-11-06 14:27:40,620:INFO:SubProcess create_model() called ==================================
2024-11-06 14:27:40,620:INFO:Initializing create_model()
2024-11-06 14:27:40,620:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e9cee7b1d90>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e9d042e2590>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 14:27:40,620:INFO:Checking exceptions
2024-11-06 14:27:40,620:INFO:Importing libraries
2024-11-06 14:27:40,620:INFO:Copying training dataset
2024-11-06 14:27:40,785:INFO:Defining folds
2024-11-06 14:27:40,785:INFO:Declaring metric variables
2024-11-06 14:27:40,788:INFO:Importing untrained model
2024-11-06 14:27:40,791:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-06 14:27:40,796:INFO:Starting cross validation
2024-11-06 14:27:40,800:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 14:27:41,009:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 14:27:41,075:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 14:27:41,105:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 14:27:41,132:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:41,136:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:27:41,163:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 14:27:41,221:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 14:27:41,239:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 14:27:41,268:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:41,293:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 14:27:41,304:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:41,307:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:41,320:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 14:27:41,345:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 14:27:41,368:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:41,392:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 14:27:41,393:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:41,423:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:41,444:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:41,469:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:41,501:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:41,517:INFO:Calculating mean and std
2024-11-06 14:27:41,518:INFO:Creating metrics dataframe
2024-11-06 14:27:41,519:INFO:Uploading results into container
2024-11-06 14:27:41,520:INFO:Uploading model into container now
2024-11-06 14:27:41,520:INFO:_master_model_container: 8
2024-11-06 14:27:41,520:INFO:_display_container: 2
2024-11-06 14:27:41,520:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-06 14:27:41,520:INFO:create_model() successfully completed......................................
2024-11-06 14:27:41,642:INFO:SubProcess create_model() end ==================================
2024-11-06 14:27:41,643:INFO:Creating metrics dataframe
2024-11-06 14:27:41,648:INFO:Initializing Ada Boost Classifier
2024-11-06 14:27:41,648:INFO:Total runtime is 0.31168312629063927 minutes
2024-11-06 14:27:41,651:INFO:SubProcess create_model() called ==================================
2024-11-06 14:27:41,651:INFO:Initializing create_model()
2024-11-06 14:27:41,651:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e9cee7b1d90>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e9d042e2590>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 14:27:41,651:INFO:Checking exceptions
2024-11-06 14:27:41,651:INFO:Importing libraries
2024-11-06 14:27:41,651:INFO:Copying training dataset
2024-11-06 14:27:41,802:INFO:Defining folds
2024-11-06 14:27:41,803:INFO:Declaring metric variables
2024-11-06 14:27:41,805:INFO:Importing untrained model
2024-11-06 14:27:41,807:INFO:Ada Boost Classifier Imported successfully
2024-11-06 14:27:41,811:INFO:Starting cross validation
2024-11-06 14:27:41,815:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 14:27:42,031:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 14:27:42,150:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 14:27:42,152:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 14:27:42,179:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 14:27:42,270:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 14:27:42,312:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 14:27:42,341:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 14:27:42,357:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 14:27:42,380:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 14:27:42,386:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 14:27:44,094:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:44,144:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:44,328:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:44,346:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:44,395:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:44,448:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:44,475:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:44,494:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:44,511:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:44,541:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:27:44,553:INFO:Calculating mean and std
2024-11-06 14:27:44,554:INFO:Creating metrics dataframe
2024-11-06 14:27:44,558:INFO:Uploading results into container
2024-11-06 14:27:44,559:INFO:Uploading model into container now
2024-11-06 14:27:44,559:INFO:_master_model_container: 9
2024-11-06 14:27:44,559:INFO:_display_container: 2
2024-11-06 14:27:44,560:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=8281)
2024-11-06 14:27:44,560:INFO:create_model() successfully completed......................................
2024-11-06 14:27:44,685:INFO:SubProcess create_model() end ==================================
2024-11-06 14:27:44,685:INFO:Creating metrics dataframe
2024-11-06 14:27:44,691:INFO:Initializing Gradient Boosting Classifier
2024-11-06 14:27:44,691:INFO:Total runtime is 0.36239596605300906 minutes
2024-11-06 14:27:44,693:INFO:SubProcess create_model() called ==================================
2024-11-06 14:27:44,694:INFO:Initializing create_model()
2024-11-06 14:27:44,694:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e9cee7b1d90>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e9d042e2590>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 14:27:44,694:INFO:Checking exceptions
2024-11-06 14:27:44,694:INFO:Importing libraries
2024-11-06 14:27:44,694:INFO:Copying training dataset
2024-11-06 14:27:44,856:INFO:Defining folds
2024-11-06 14:27:44,856:INFO:Declaring metric variables
2024-11-06 14:27:44,859:INFO:Importing untrained model
2024-11-06 14:27:44,861:INFO:Gradient Boosting Classifier Imported successfully
2024-11-06 14:27:44,865:INFO:Starting cross validation
2024-11-06 14:27:44,870:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 14:28:21,807:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:21,821:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:22,022:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:22,049:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:22,187:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:22,799:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:24,789:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:25,281:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:25,343:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:25,677:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:25,695:INFO:Calculating mean and std
2024-11-06 14:28:25,697:INFO:Creating metrics dataframe
2024-11-06 14:28:25,701:INFO:Uploading results into container
2024-11-06 14:28:25,702:INFO:Uploading model into container now
2024-11-06 14:28:25,703:INFO:_master_model_container: 10
2024-11-06 14:28:25,703:INFO:_display_container: 2
2024-11-06 14:28:25,703:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8281, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-06 14:28:25,704:INFO:create_model() successfully completed......................................
2024-11-06 14:28:25,842:INFO:SubProcess create_model() end ==================================
2024-11-06 14:28:25,843:INFO:Creating metrics dataframe
2024-11-06 14:28:25,848:INFO:Initializing Linear Discriminant Analysis
2024-11-06 14:28:25,848:INFO:Total runtime is 1.048346730073293 minutes
2024-11-06 14:28:25,851:INFO:SubProcess create_model() called ==================================
2024-11-06 14:28:25,851:INFO:Initializing create_model()
2024-11-06 14:28:25,851:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e9cee7b1d90>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e9d042e2590>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 14:28:25,851:INFO:Checking exceptions
2024-11-06 14:28:25,851:INFO:Importing libraries
2024-11-06 14:28:25,851:INFO:Copying training dataset
2024-11-06 14:28:26,010:INFO:Defining folds
2024-11-06 14:28:26,010:INFO:Declaring metric variables
2024-11-06 14:28:26,013:INFO:Importing untrained model
2024-11-06 14:28:26,016:INFO:Linear Discriminant Analysis Imported successfully
2024-11-06 14:28:26,021:INFO:Starting cross validation
2024-11-06 14:28:26,025:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 14:28:26,405:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:26,626:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:26,784:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:26,805:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:26,824:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:26,863:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:26,872:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:26,880:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:26,889:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:26,931:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 14:28:26,942:INFO:Calculating mean and std
2024-11-06 14:28:26,943:INFO:Creating metrics dataframe
2024-11-06 14:28:26,944:INFO:Uploading results into container
2024-11-06 14:28:26,945:INFO:Uploading model into container now
2024-11-06 14:28:26,945:INFO:_master_model_container: 11
2024-11-06 14:28:26,945:INFO:_display_container: 2
2024-11-06 14:28:26,945:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-06 14:28:26,945:INFO:create_model() successfully completed......................................
2024-11-06 14:28:27,086:INFO:SubProcess create_model() end ==================================
2024-11-06 14:28:27,086:INFO:Creating metrics dataframe
2024-11-06 14:28:27,093:INFO:Initializing Extra Trees Classifier
2024-11-06 14:28:27,093:INFO:Total runtime is 1.0691009918848673 minutes
2024-11-06 14:28:27,097:INFO:SubProcess create_model() called ==================================
2024-11-06 14:28:27,098:INFO:Initializing create_model()
2024-11-06 14:28:27,098:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e9cee7b1d90>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e9d042e2590>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 14:28:27,098:INFO:Checking exceptions
2024-11-06 14:28:27,098:INFO:Importing libraries
2024-11-06 14:28:27,098:INFO:Copying training dataset
2024-11-06 14:28:27,264:INFO:Defining folds
2024-11-06 14:28:27,265:INFO:Declaring metric variables
2024-11-06 14:28:27,267:INFO:Importing untrained model
2024-11-06 14:28:27,270:INFO:Extra Trees Classifier Imported successfully
2024-11-06 14:28:27,275:INFO:Starting cross validation
2024-11-06 14:28:27,279:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 14:28:27,892:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:28:28,263:INFO:Calculating mean and std
2024-11-06 14:28:28,264:INFO:Creating metrics dataframe
2024-11-06 14:28:28,265:INFO:Uploading results into container
2024-11-06 14:28:28,265:INFO:Uploading model into container now
2024-11-06 14:28:28,265:INFO:_master_model_container: 12
2024-11-06 14:28:28,266:INFO:_display_container: 2
2024-11-06 14:28:28,266:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=8281, verbose=0,
                     warm_start=False)
2024-11-06 14:28:28,266:INFO:create_model() successfully completed......................................
2024-11-06 14:28:28,401:INFO:SubProcess create_model() end ==================================
2024-11-06 14:28:28,401:INFO:Creating metrics dataframe
2024-11-06 14:28:28,407:INFO:Initializing Extreme Gradient Boosting
2024-11-06 14:28:28,407:INFO:Total runtime is 1.0909918387730915 minutes
2024-11-06 14:28:28,409:INFO:SubProcess create_model() called ==================================
2024-11-06 14:28:28,410:INFO:Initializing create_model()
2024-11-06 14:28:28,410:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e9cee7b1d90>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e9d042e2590>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 14:28:28,410:INFO:Checking exceptions
2024-11-06 14:28:28,410:INFO:Importing libraries
2024-11-06 14:28:28,410:INFO:Copying training dataset
2024-11-06 14:28:28,571:INFO:Defining folds
2024-11-06 14:28:28,571:INFO:Declaring metric variables
2024-11-06 14:28:28,574:INFO:Importing untrained model
2024-11-06 14:28:28,576:INFO:Extreme Gradient Boosting Imported successfully
2024-11-06 14:28:28,581:INFO:Starting cross validation
2024-11-06 14:28:28,584:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 14:28:52,522:INFO:Calculating mean and std
2024-11-06 14:28:52,524:INFO:Creating metrics dataframe
2024-11-06 14:28:52,525:INFO:Uploading results into container
2024-11-06 14:28:52,526:INFO:Uploading model into container now
2024-11-06 14:28:52,526:INFO:_master_model_container: 13
2024-11-06 14:28:52,527:INFO:_display_container: 2
2024-11-06 14:28:52,527:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-11-06 14:28:52,527:INFO:create_model() successfully completed......................................
2024-11-06 14:28:52,672:INFO:SubProcess create_model() end ==================================
2024-11-06 14:28:52,672:INFO:Creating metrics dataframe
2024-11-06 14:28:52,679:INFO:Initializing Light Gradient Boosting Machine
2024-11-06 14:28:52,679:INFO:Total runtime is 1.4955331405003864 minutes
2024-11-06 14:28:52,683:INFO:SubProcess create_model() called ==================================
2024-11-06 14:28:52,683:INFO:Initializing create_model()
2024-11-06 14:28:52,683:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e9cee7b1d90>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e9d042e2590>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 14:28:52,683:INFO:Checking exceptions
2024-11-06 14:28:52,683:INFO:Importing libraries
2024-11-06 14:28:52,683:INFO:Copying training dataset
2024-11-06 14:28:52,858:INFO:Defining folds
2024-11-06 14:28:52,858:INFO:Declaring metric variables
2024-11-06 14:28:52,860:INFO:Importing untrained model
2024-11-06 14:28:52,864:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-06 14:28:52,869:INFO:Starting cross validation
2024-11-06 14:28:52,873:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 14:29:21,929:INFO:Calculating mean and std
2024-11-06 14:29:21,930:INFO:Creating metrics dataframe
2024-11-06 14:29:21,931:INFO:Uploading results into container
2024-11-06 14:29:21,932:INFO:Uploading model into container now
2024-11-06 14:29:21,932:INFO:_master_model_container: 14
2024-11-06 14:29:21,932:INFO:_display_container: 2
2024-11-06 14:29:21,932:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8281, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-06 14:29:21,932:INFO:create_model() successfully completed......................................
2024-11-06 14:29:22,092:INFO:SubProcess create_model() end ==================================
2024-11-06 14:29:22,092:INFO:Creating metrics dataframe
2024-11-06 14:29:22,098:INFO:Initializing Dummy Classifier
2024-11-06 14:29:22,099:INFO:Total runtime is 1.9858534018198648 minutes
2024-11-06 14:29:22,101:INFO:SubProcess create_model() called ==================================
2024-11-06 14:29:22,101:INFO:Initializing create_model()
2024-11-06 14:29:22,101:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e9cee7b1d90>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e9d042e2590>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 14:29:22,101:INFO:Checking exceptions
2024-11-06 14:29:22,101:INFO:Importing libraries
2024-11-06 14:29:22,101:INFO:Copying training dataset
2024-11-06 14:29:22,266:INFO:Defining folds
2024-11-06 14:29:22,266:INFO:Declaring metric variables
2024-11-06 14:29:22,269:INFO:Importing untrained model
2024-11-06 14:29:22,272:INFO:Dummy Classifier Imported successfully
2024-11-06 14:29:22,277:INFO:Starting cross validation
2024-11-06 14:29:22,281:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 14:29:22,645:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:29:22,659:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:29:22,791:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:29:22,808:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:29:22,853:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:29:22,871:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:29:22,875:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:29:22,952:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:29:22,957:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:29:23,007:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 14:29:23,017:INFO:Calculating mean and std
2024-11-06 14:29:23,019:INFO:Creating metrics dataframe
2024-11-06 14:29:23,021:INFO:Uploading results into container
2024-11-06 14:29:23,022:INFO:Uploading model into container now
2024-11-06 14:29:23,023:INFO:_master_model_container: 15
2024-11-06 14:29:23,023:INFO:_display_container: 2
2024-11-06 14:29:23,024:INFO:DummyClassifier(constant=None, random_state=8281, strategy='prior')
2024-11-06 14:29:23,024:INFO:create_model() successfully completed......................................
2024-11-06 14:29:23,168:INFO:SubProcess create_model() end ==================================
2024-11-06 14:29:23,168:INFO:Creating metrics dataframe
2024-11-06 14:29:23,176:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-06 14:29:23,183:INFO:Initializing create_model()
2024-11-06 14:29:23,183:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e9cee7b1d90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=8281, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 14:29:23,183:INFO:Checking exceptions
2024-11-06 14:29:23,185:INFO:Importing libraries
2024-11-06 14:29:23,185:INFO:Copying training dataset
2024-11-06 14:29:23,351:INFO:Defining folds
2024-11-06 14:29:23,351:INFO:Declaring metric variables
2024-11-06 14:29:23,351:INFO:Importing untrained model
2024-11-06 14:29:23,351:INFO:Declaring custom model
2024-11-06 14:29:23,351:INFO:Random Forest Classifier Imported successfully
2024-11-06 14:29:23,355:INFO:Cross validation set to False
2024-11-06 14:29:23,355:INFO:Fitting Model
2024-11-06 14:29:23,737:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=8281, verbose=0,
                       warm_start=False)
2024-11-06 14:29:23,737:INFO:create_model() successfully completed......................................
2024-11-06 14:29:23,890:INFO:_master_model_container: 15
2024-11-06 14:29:23,891:INFO:_display_container: 2
2024-11-06 14:29:23,891:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=8281, verbose=0,
                       warm_start=False)
2024-11-06 14:29:23,891:INFO:compare_models() successfully completed......................................
2024-11-06 15:21:02,421:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 15:21:02,421:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 15:21:02,421:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 15:21:02,422:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 15:21:10,730:INFO:PyCaret ClassificationExperiment
2024-11-06 15:21:10,730:INFO:Logging name: clf-default-name
2024-11-06 15:21:10,730:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-06 15:21:10,730:INFO:version 3.3.2
2024-11-06 15:21:10,730:INFO:Initializing setup()
2024-11-06 15:21:10,730:INFO:self.USI: 6a69
2024-11-06 15:21:10,730:INFO:self._variable_keys: {'logging_param', 'idx', 'pipeline', 'seed', 'y_train', 'y_test', 'target_param', 'USI', 'fold_groups_param', 'is_multiclass', 'memory', 'exp_id', 'fold_shuffle_param', '_available_plots', 'y', 'X', 'gpu_n_jobs_param', 'X_test', 'log_plots_param', 'n_jobs_param', 'X_train', 'fold_generator', 'gpu_param', 'data', 'html_param', 'fix_imbalance', 'exp_name_log', '_ml_usecase'}
2024-11-06 15:21:10,730:INFO:Checking environment
2024-11-06 15:21:10,730:INFO:python_version: 3.11.10
2024-11-06 15:21:10,730:INFO:python_build: ('main', 'Oct  3 2024 07:29:13')
2024-11-06 15:21:10,730:INFO:machine: x86_64
2024-11-06 15:21:10,730:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 15:21:10,731:INFO:Memory: svmem(total=33037213696, available=23317573632, percent=29.4, used=8996335616, free=10757292032, active=9964642304, inactive=8844320768, buffers=840122368, cached=12443463680, shared=249884672, slab=1989218304)
2024-11-06 15:21:10,731:INFO:Physical Core: 8
2024-11-06 15:21:10,732:INFO:Logical Core: 16
2024-11-06 15:21:10,732:INFO:Checking libraries
2024-11-06 15:21:10,732:INFO:System:
2024-11-06 15:21:10,732:INFO:    python: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0]
2024-11-06 15:21:10,732:INFO:executable: /home/cid34senhas/miniconda3/envs/myenv/bin/python
2024-11-06 15:21:10,732:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 15:21:10,732:INFO:PyCaret required dependencies:
2024-11-06 15:21:10,748:INFO:                 pip: 24.2
2024-11-06 15:21:10,748:INFO:          setuptools: 75.1.0
2024-11-06 15:21:10,748:INFO:             pycaret: 3.3.2
2024-11-06 15:21:10,748:INFO:             IPython: 8.29.0
2024-11-06 15:21:10,748:INFO:          ipywidgets: 8.1.5
2024-11-06 15:21:10,748:INFO:                tqdm: 4.66.6
2024-11-06 15:21:10,748:INFO:               numpy: 1.26.4
2024-11-06 15:21:10,748:INFO:              pandas: 2.1.4
2024-11-06 15:21:10,748:INFO:              jinja2: 3.1.4
2024-11-06 15:21:10,748:INFO:               scipy: 1.11.4
2024-11-06 15:21:10,748:INFO:              joblib: 1.3.2
2024-11-06 15:21:10,748:INFO:             sklearn: 1.4.2
2024-11-06 15:21:10,748:INFO:                pyod: 2.0.2
2024-11-06 15:21:10,748:INFO:            imblearn: 0.12.4
2024-11-06 15:21:10,748:INFO:   category_encoders: 2.6.4
2024-11-06 15:21:10,748:INFO:            lightgbm: 4.5.0
2024-11-06 15:21:10,748:INFO:               numba: 0.60.0
2024-11-06 15:21:10,748:INFO:            requests: 2.32.3
2024-11-06 15:21:10,748:INFO:          matplotlib: 3.7.5
2024-11-06 15:21:10,748:INFO:          scikitplot: 0.3.7
2024-11-06 15:21:10,748:INFO:         yellowbrick: 1.5
2024-11-06 15:21:10,748:INFO:              plotly: 5.24.1
2024-11-06 15:21:10,748:INFO:    plotly-resampler: Not installed
2024-11-06 15:21:10,748:INFO:             kaleido: 0.2.1
2024-11-06 15:21:10,748:INFO:           schemdraw: 0.15
2024-11-06 15:21:10,748:INFO:         statsmodels: 0.14.4
2024-11-06 15:21:10,748:INFO:              sktime: 0.26.0
2024-11-06 15:21:10,748:INFO:               tbats: 1.1.3
2024-11-06 15:21:10,748:INFO:            pmdarima: 2.0.4
2024-11-06 15:21:10,748:INFO:              psutil: 6.1.0
2024-11-06 15:21:10,748:INFO:          markupsafe: 3.0.2
2024-11-06 15:21:10,748:INFO:             pickle5: Not installed
2024-11-06 15:21:10,748:INFO:         cloudpickle: 3.1.0
2024-11-06 15:21:10,748:INFO:         deprecation: 2.1.0
2024-11-06 15:21:10,748:INFO:              xxhash: 3.5.0
2024-11-06 15:21:10,748:INFO:           wurlitzer: 3.1.1
2024-11-06 15:21:10,748:INFO:PyCaret optional dependencies:
2024-11-06 15:21:10,768:INFO:                shap: Not installed
2024-11-06 15:21:10,768:INFO:           interpret: Not installed
2024-11-06 15:21:10,769:INFO:                umap: Not installed
2024-11-06 15:21:10,769:INFO:     ydata_profiling: Not installed
2024-11-06 15:21:10,769:INFO:  explainerdashboard: Not installed
2024-11-06 15:21:10,769:INFO:             autoviz: Not installed
2024-11-06 15:21:10,769:INFO:           fairlearn: Not installed
2024-11-06 15:21:10,769:INFO:          deepchecks: Not installed
2024-11-06 15:21:10,769:INFO:             xgboost: 2.1.2
2024-11-06 15:21:10,769:INFO:            catboost: Not installed
2024-11-06 15:21:10,769:INFO:              kmodes: Not installed
2024-11-06 15:21:10,769:INFO:             mlxtend: Not installed
2024-11-06 15:21:10,769:INFO:       statsforecast: Not installed
2024-11-06 15:21:10,769:INFO:        tune_sklearn: Not installed
2024-11-06 15:21:10,769:INFO:                 ray: Not installed
2024-11-06 15:21:10,769:INFO:            hyperopt: Not installed
2024-11-06 15:21:10,769:INFO:              optuna: Not installed
2024-11-06 15:21:10,769:INFO:               skopt: Not installed
2024-11-06 15:21:10,769:INFO:              mlflow: Not installed
2024-11-06 15:21:10,769:INFO:              gradio: Not installed
2024-11-06 15:21:10,769:INFO:             fastapi: Not installed
2024-11-06 15:21:10,769:INFO:             uvicorn: Not installed
2024-11-06 15:21:10,769:INFO:              m2cgen: Not installed
2024-11-06 15:21:10,769:INFO:           evidently: Not installed
2024-11-06 15:21:10,769:INFO:               fugue: Not installed
2024-11-06 15:21:10,769:INFO:           streamlit: Not installed
2024-11-06 15:21:10,769:INFO:             prophet: Not installed
2024-11-06 15:21:10,769:INFO:None
2024-11-06 15:21:10,769:INFO:Set up data.
2024-11-06 15:21:11,033:INFO:Set up folding strategy.
2024-11-06 15:21:11,033:INFO:Set up train/test split.
2024-11-06 15:21:11,214:INFO:Set up index.
2024-11-06 15:21:11,217:INFO:Assigning column types.
2024-11-06 15:21:11,420:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-06 15:21:11,455:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 15:21:11,457:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 15:21:11,480:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:21:11,482:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:21:11,517:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 15:21:11,518:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 15:21:11,539:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:21:11,541:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:21:11,541:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-06 15:21:11,576:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 15:21:11,598:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:21:11,600:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:21:11,635:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 15:21:11,656:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:21:11,658:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:21:11,658:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-06 15:21:11,714:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:21:11,716:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:21:11,773:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:21:11,775:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:21:11,776:INFO:Preparing preprocessing pipeline...
2024-11-06 15:21:11,796:INFO:Set up simple imputation.
2024-11-06 15:21:11,878:INFO:Set up column name cleaning.
2024-11-06 15:21:12,545:INFO:Finished creating preprocessing pipeline.
2024-11-06 15:21:12,554:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['diagnostics_Image-original_Mean',
                                             'diagnostics_Image-original_Maximum',
                                             'diagnostics_Mask-original_VoxelNum',
                                             'original_shape_Elongation',
                                             'original_shape_Flatness',
                                             'original_shape_LeastAxisLength',
                                             'original_shape_MajorAxisLength',
                                             'ori...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-06 15:21:12,554:INFO:Creating final display dataframe.
2024-11-06 15:21:16,035:INFO:Setup _display_container:                     Description             Value
0                    Session id               417
1                        Target        Transition
2                   Target type        Multiclass
3           Original data shape       (295, 2014)
4        Transformed data shape       (295, 2014)
5   Transformed train set shape       (206, 2014)
6    Transformed test set shape        (89, 2014)
7              Numeric features              2013
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              6a69
2024-11-06 15:21:16,100:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:21:16,102:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:21:16,163:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:21:16,166:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:21:16,167:INFO:setup() successfully completed in 5.44s...............
2024-11-06 15:21:16,175:INFO:Initializing compare_models()
2024-11-06 15:21:16,175:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc252b8b50>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x79cc252b8b50>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-06 15:21:16,175:INFO:Checking exceptions
2024-11-06 15:21:16,355:INFO:Preparing display monitor
2024-11-06 15:21:16,375:INFO:Initializing Logistic Regression
2024-11-06 15:21:16,376:INFO:Total runtime is 3.743171691894531e-06 minutes
2024-11-06 15:21:16,378:INFO:SubProcess create_model() called ==================================
2024-11-06 15:21:16,379:INFO:Initializing create_model()
2024-11-06 15:21:16,379:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc252b8b50>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc2fbd4f10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 15:21:16,379:INFO:Checking exceptions
2024-11-06 15:21:16,379:INFO:Importing libraries
2024-11-06 15:21:16,379:INFO:Copying training dataset
2024-11-06 15:21:16,690:INFO:Defining folds
2024-11-06 15:21:16,690:INFO:Declaring metric variables
2024-11-06 15:21:16,695:INFO:Importing untrained model
2024-11-06 15:21:16,700:INFO:Logistic Regression Imported successfully
2024-11-06 15:21:16,704:INFO:Starting cross validation
2024-11-06 15:21:16,707:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 15:21:24,350:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:24,424:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:24,548:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:24,612:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:24,830:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:24,915:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:25,122:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:25,179:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:25,283:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:25,296:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:25,332:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:25,344:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:25,744:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:25,793:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:25,875:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:25,880:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:25,915:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:25,921:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:25,979:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:26,020:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:26,033:INFO:Calculating mean and std
2024-11-06 15:21:26,036:INFO:Creating metrics dataframe
2024-11-06 15:21:26,041:INFO:Uploading results into container
2024-11-06 15:21:26,042:INFO:Uploading model into container now
2024-11-06 15:21:26,043:INFO:_master_model_container: 1
2024-11-06 15:21:26,043:INFO:_display_container: 2
2024-11-06 15:21:26,044:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=417, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-06 15:21:26,044:INFO:create_model() successfully completed......................................
2024-11-06 15:21:26,164:INFO:SubProcess create_model() end ==================================
2024-11-06 15:21:26,165:INFO:Creating metrics dataframe
2024-11-06 15:21:26,169:INFO:Initializing K Neighbors Classifier
2024-11-06 15:21:26,169:INFO:Total runtime is 0.16323543787002562 minutes
2024-11-06 15:21:26,172:INFO:SubProcess create_model() called ==================================
2024-11-06 15:21:26,172:INFO:Initializing create_model()
2024-11-06 15:21:26,172:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc252b8b50>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc2fbd4f10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 15:21:26,172:INFO:Checking exceptions
2024-11-06 15:21:26,172:INFO:Importing libraries
2024-11-06 15:21:26,172:INFO:Copying training dataset
2024-11-06 15:21:26,395:INFO:Defining folds
2024-11-06 15:21:26,396:INFO:Declaring metric variables
2024-11-06 15:21:26,399:INFO:Importing untrained model
2024-11-06 15:21:26,401:INFO:K Neighbors Classifier Imported successfully
2024-11-06 15:21:26,405:INFO:Starting cross validation
2024-11-06 15:21:26,409:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 15:21:27,310:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:21:28,248:INFO:Calculating mean and std
2024-11-06 15:21:28,250:INFO:Creating metrics dataframe
2024-11-06 15:21:28,252:INFO:Uploading results into container
2024-11-06 15:21:28,253:INFO:Uploading model into container now
2024-11-06 15:21:28,254:INFO:_master_model_container: 2
2024-11-06 15:21:28,254:INFO:_display_container: 2
2024-11-06 15:21:28,255:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-06 15:21:28,255:INFO:create_model() successfully completed......................................
2024-11-06 15:21:28,341:INFO:SubProcess create_model() end ==================================
2024-11-06 15:21:28,341:INFO:Creating metrics dataframe
2024-11-06 15:21:28,346:INFO:Initializing Naive Bayes
2024-11-06 15:21:28,346:INFO:Total runtime is 0.19950620730717974 minutes
2024-11-06 15:21:28,348:INFO:SubProcess create_model() called ==================================
2024-11-06 15:21:28,348:INFO:Initializing create_model()
2024-11-06 15:21:28,348:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc252b8b50>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc2fbd4f10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 15:21:28,348:INFO:Checking exceptions
2024-11-06 15:21:28,349:INFO:Importing libraries
2024-11-06 15:21:28,349:INFO:Copying training dataset
2024-11-06 15:21:28,568:INFO:Defining folds
2024-11-06 15:21:28,568:INFO:Declaring metric variables
2024-11-06 15:21:28,570:INFO:Importing untrained model
2024-11-06 15:21:28,574:INFO:Naive Bayes Imported successfully
2024-11-06 15:21:28,580:INFO:Starting cross validation
2024-11-06 15:21:28,584:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 15:21:29,346:INFO:Calculating mean and std
2024-11-06 15:21:29,348:INFO:Creating metrics dataframe
2024-11-06 15:21:29,349:INFO:Uploading results into container
2024-11-06 15:21:29,349:INFO:Uploading model into container now
2024-11-06 15:21:29,350:INFO:_master_model_container: 3
2024-11-06 15:21:29,350:INFO:_display_container: 2
2024-11-06 15:21:29,350:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-06 15:21:29,350:INFO:create_model() successfully completed......................................
2024-11-06 15:21:29,427:INFO:SubProcess create_model() end ==================================
2024-11-06 15:21:29,427:INFO:Creating metrics dataframe
2024-11-06 15:21:29,432:INFO:Initializing Decision Tree Classifier
2024-11-06 15:21:29,432:INFO:Total runtime is 0.21761593818664549 minutes
2024-11-06 15:21:29,435:INFO:SubProcess create_model() called ==================================
2024-11-06 15:21:29,435:INFO:Initializing create_model()
2024-11-06 15:21:29,435:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc252b8b50>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc2fbd4f10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 15:21:29,435:INFO:Checking exceptions
2024-11-06 15:21:29,435:INFO:Importing libraries
2024-11-06 15:21:29,435:INFO:Copying training dataset
2024-11-06 15:21:29,660:INFO:Defining folds
2024-11-06 15:21:29,661:INFO:Declaring metric variables
2024-11-06 15:21:29,663:INFO:Importing untrained model
2024-11-06 15:21:29,666:INFO:Decision Tree Classifier Imported successfully
2024-11-06 15:21:29,671:INFO:Starting cross validation
2024-11-06 15:21:29,674:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 15:21:30,570:INFO:Calculating mean and std
2024-11-06 15:21:30,571:INFO:Creating metrics dataframe
2024-11-06 15:21:30,573:INFO:Uploading results into container
2024-11-06 15:21:30,574:INFO:Uploading model into container now
2024-11-06 15:21:30,574:INFO:_master_model_container: 4
2024-11-06 15:21:30,574:INFO:_display_container: 2
2024-11-06 15:21:30,574:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=417, splitter='best')
2024-11-06 15:21:30,574:INFO:create_model() successfully completed......................................
2024-11-06 15:21:30,654:INFO:SubProcess create_model() end ==================================
2024-11-06 15:21:30,654:INFO:Creating metrics dataframe
2024-11-06 15:21:30,659:INFO:Initializing SVM - Linear Kernel
2024-11-06 15:21:30,660:INFO:Total runtime is 0.23807105620702107 minutes
2024-11-06 15:21:30,662:INFO:SubProcess create_model() called ==================================
2024-11-06 15:21:30,662:INFO:Initializing create_model()
2024-11-06 15:21:30,662:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc252b8b50>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc2fbd4f10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 15:21:30,662:INFO:Checking exceptions
2024-11-06 15:21:30,662:INFO:Importing libraries
2024-11-06 15:21:30,662:INFO:Copying training dataset
2024-11-06 15:21:30,896:INFO:Defining folds
2024-11-06 15:21:30,896:INFO:Declaring metric variables
2024-11-06 15:21:30,898:INFO:Importing untrained model
2024-11-06 15:21:30,901:INFO:SVM - Linear Kernel Imported successfully
2024-11-06 15:21:30,906:INFO:Starting cross validation
2024-11-06 15:21:30,910:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 15:21:31,306:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:31,312:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:21:31,355:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:31,359:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:21:31,433:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:31,437:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:21:31,439:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:31,445:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:21:31,504:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:31,508:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:21:31,540:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:31,544:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:21:31,574:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:31,578:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:21:31,618:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:31,621:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:21:31,622:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:31,625:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:21:31,661:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:31,663:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:21:31,674:INFO:Calculating mean and std
2024-11-06 15:21:31,675:INFO:Creating metrics dataframe
2024-11-06 15:21:31,676:INFO:Uploading results into container
2024-11-06 15:21:31,677:INFO:Uploading model into container now
2024-11-06 15:21:31,678:INFO:_master_model_container: 5
2024-11-06 15:21:31,678:INFO:_display_container: 2
2024-11-06 15:21:31,678:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=417, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-06 15:21:31,678:INFO:create_model() successfully completed......................................
2024-11-06 15:21:50,964:INFO:Initializing compare_models()
2024-11-06 15:21:50,964:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc252b8b50>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x79cc252b8b50>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-06 15:21:50,964:INFO:Checking exceptions
2024-11-06 15:21:51,028:INFO:Preparing display monitor
2024-11-06 15:21:51,042:INFO:Initializing Logistic Regression
2024-11-06 15:21:51,043:INFO:Total runtime is 3.612041473388672e-06 minutes
2024-11-06 15:21:51,045:INFO:SubProcess create_model() called ==================================
2024-11-06 15:21:51,045:INFO:Initializing create_model()
2024-11-06 15:21:51,045:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc252b8b50>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc1fffa910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 15:21:51,045:INFO:Checking exceptions
2024-11-06 15:21:51,045:INFO:Importing libraries
2024-11-06 15:21:51,045:INFO:Copying training dataset
2024-11-06 15:21:51,340:INFO:Defining folds
2024-11-06 15:21:51,340:INFO:Declaring metric variables
2024-11-06 15:21:51,343:INFO:Importing untrained model
2024-11-06 15:21:51,345:INFO:Logistic Regression Imported successfully
2024-11-06 15:21:51,349:INFO:Starting cross validation
2024-11-06 15:21:51,352:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 15:21:57,459:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:57,522:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:57,536:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:57,602:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:57,799:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:57,858:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:57,942:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:58,000:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:58,129:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:58,190:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:58,341:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:58,393:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:58,430:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:58,476:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:58,589:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:58,627:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:58,759:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:58,761:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 15:21:58,801:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:58,805:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:21:58,817:INFO:Calculating mean and std
2024-11-06 15:21:58,818:INFO:Creating metrics dataframe
2024-11-06 15:21:58,820:INFO:Uploading results into container
2024-11-06 15:21:58,820:INFO:Uploading model into container now
2024-11-06 15:21:58,820:INFO:_master_model_container: 6
2024-11-06 15:21:58,820:INFO:_display_container: 2
2024-11-06 15:21:58,821:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=417, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-06 15:21:58,821:INFO:create_model() successfully completed......................................
2024-11-06 15:21:58,959:INFO:SubProcess create_model() end ==================================
2024-11-06 15:21:58,959:INFO:Creating metrics dataframe
2024-11-06 15:21:58,964:INFO:Initializing K Neighbors Classifier
2024-11-06 15:21:58,964:INFO:Total runtime is 0.13203437328338624 minutes
2024-11-06 15:21:58,967:INFO:SubProcess create_model() called ==================================
2024-11-06 15:21:58,968:INFO:Initializing create_model()
2024-11-06 15:21:58,968:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc252b8b50>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc1fffa910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 15:21:58,968:INFO:Checking exceptions
2024-11-06 15:21:58,968:INFO:Importing libraries
2024-11-06 15:21:58,968:INFO:Copying training dataset
2024-11-06 15:21:59,118:INFO:Defining folds
2024-11-06 15:21:59,118:INFO:Declaring metric variables
2024-11-06 15:21:59,121:INFO:Importing untrained model
2024-11-06 15:21:59,124:INFO:K Neighbors Classifier Imported successfully
2024-11-06 15:21:59,129:INFO:Starting cross validation
2024-11-06 15:21:59,132:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 15:21:59,867:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:21:59,877:INFO:Calculating mean and std
2024-11-06 15:21:59,878:INFO:Creating metrics dataframe
2024-11-06 15:21:59,879:INFO:Uploading results into container
2024-11-06 15:21:59,879:INFO:Uploading model into container now
2024-11-06 15:21:59,880:INFO:_master_model_container: 7
2024-11-06 15:21:59,880:INFO:_display_container: 2
2024-11-06 15:21:59,880:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-06 15:21:59,880:INFO:create_model() successfully completed......................................
2024-11-06 15:22:00,012:INFO:SubProcess create_model() end ==================================
2024-11-06 15:22:00,012:INFO:Creating metrics dataframe
2024-11-06 15:22:00,017:INFO:Initializing Naive Bayes
2024-11-06 15:22:00,018:INFO:Total runtime is 0.1495887597401937 minutes
2024-11-06 15:22:00,020:INFO:SubProcess create_model() called ==================================
2024-11-06 15:22:00,021:INFO:Initializing create_model()
2024-11-06 15:22:00,021:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc252b8b50>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc1fffa910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 15:22:00,021:INFO:Checking exceptions
2024-11-06 15:22:00,021:INFO:Importing libraries
2024-11-06 15:22:00,021:INFO:Copying training dataset
2024-11-06 15:22:00,174:INFO:Defining folds
2024-11-06 15:22:00,174:INFO:Declaring metric variables
2024-11-06 15:22:00,177:INFO:Importing untrained model
2024-11-06 15:22:00,181:INFO:Naive Bayes Imported successfully
2024-11-06 15:22:00,186:INFO:Starting cross validation
2024-11-06 15:22:00,190:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 15:22:00,857:INFO:Calculating mean and std
2024-11-06 15:22:00,858:INFO:Creating metrics dataframe
2024-11-06 15:22:00,860:INFO:Uploading results into container
2024-11-06 15:22:00,860:INFO:Uploading model into container now
2024-11-06 15:22:00,860:INFO:_master_model_container: 8
2024-11-06 15:22:00,860:INFO:_display_container: 2
2024-11-06 15:22:00,860:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-06 15:22:00,860:INFO:create_model() successfully completed......................................
2024-11-06 15:22:00,991:INFO:SubProcess create_model() end ==================================
2024-11-06 15:22:00,991:INFO:Creating metrics dataframe
2024-11-06 15:22:00,998:INFO:Initializing Decision Tree Classifier
2024-11-06 15:22:00,998:INFO:Total runtime is 0.16592506965001425 minutes
2024-11-06 15:22:01,000:INFO:SubProcess create_model() called ==================================
2024-11-06 15:22:01,001:INFO:Initializing create_model()
2024-11-06 15:22:01,001:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc252b8b50>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc1fffa910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 15:22:01,001:INFO:Checking exceptions
2024-11-06 15:22:01,001:INFO:Importing libraries
2024-11-06 15:22:01,001:INFO:Copying training dataset
2024-11-06 15:22:01,166:INFO:Defining folds
2024-11-06 15:22:01,166:INFO:Declaring metric variables
2024-11-06 15:22:01,168:INFO:Importing untrained model
2024-11-06 15:22:01,171:INFO:Decision Tree Classifier Imported successfully
2024-11-06 15:22:01,176:INFO:Starting cross validation
2024-11-06 15:22:01,179:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 15:22:02,080:INFO:Calculating mean and std
2024-11-06 15:22:02,081:INFO:Creating metrics dataframe
2024-11-06 15:22:02,083:INFO:Uploading results into container
2024-11-06 15:22:02,084:INFO:Uploading model into container now
2024-11-06 15:22:02,085:INFO:_master_model_container: 9
2024-11-06 15:22:02,085:INFO:_display_container: 2
2024-11-06 15:22:02,085:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=417, splitter='best')
2024-11-06 15:22:02,085:INFO:create_model() successfully completed......................................
2024-11-06 15:22:02,233:INFO:SubProcess create_model() end ==================================
2024-11-06 15:22:02,233:INFO:Creating metrics dataframe
2024-11-06 15:22:02,239:INFO:Initializing SVM - Linear Kernel
2024-11-06 15:22:02,239:INFO:Total runtime is 0.18660640319188437 minutes
2024-11-06 15:22:02,242:INFO:SubProcess create_model() called ==================================
2024-11-06 15:22:02,242:INFO:Initializing create_model()
2024-11-06 15:22:02,243:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc252b8b50>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc1fffa910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 15:22:02,243:INFO:Checking exceptions
2024-11-06 15:22:02,243:INFO:Importing libraries
2024-11-06 15:22:02,243:INFO:Copying training dataset
2024-11-06 15:22:02,402:INFO:Defining folds
2024-11-06 15:22:02,402:INFO:Declaring metric variables
2024-11-06 15:22:02,404:INFO:Importing untrained model
2024-11-06 15:22:02,408:INFO:SVM - Linear Kernel Imported successfully
2024-11-06 15:22:02,414:INFO:Starting cross validation
2024-11-06 15:22:02,417:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 15:22:02,844:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:02,848:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:22:02,938:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:02,943:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:22:02,994:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:02,998:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:22:03,047:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:03,052:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:22:03,088:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:03,092:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:22:03,138:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:03,141:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:22:03,152:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:03,155:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:22:03,174:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:03,177:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:22:03,223:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:03,226:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:22:03,275:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:03,278:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:22:03,290:INFO:Calculating mean and std
2024-11-06 15:22:03,291:INFO:Creating metrics dataframe
2024-11-06 15:22:03,293:INFO:Uploading results into container
2024-11-06 15:22:03,294:INFO:Uploading model into container now
2024-11-06 15:22:03,294:INFO:_master_model_container: 10
2024-11-06 15:22:03,294:INFO:_display_container: 2
2024-11-06 15:22:03,295:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=417, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-06 15:22:03,295:INFO:create_model() successfully completed......................................
2024-11-06 15:22:03,433:INFO:SubProcess create_model() end ==================================
2024-11-06 15:22:03,433:INFO:Creating metrics dataframe
2024-11-06 15:22:03,438:INFO:Initializing Ridge Classifier
2024-11-06 15:22:03,438:INFO:Total runtime is 0.20659884214401247 minutes
2024-11-06 15:22:03,441:INFO:SubProcess create_model() called ==================================
2024-11-06 15:22:03,441:INFO:Initializing create_model()
2024-11-06 15:22:03,441:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc252b8b50>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc1fffa910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 15:22:03,441:INFO:Checking exceptions
2024-11-06 15:22:03,441:INFO:Importing libraries
2024-11-06 15:22:03,441:INFO:Copying training dataset
2024-11-06 15:22:03,593:INFO:Defining folds
2024-11-06 15:22:03,593:INFO:Declaring metric variables
2024-11-06 15:22:03,596:INFO:Importing untrained model
2024-11-06 15:22:03,599:INFO:Ridge Classifier Imported successfully
2024-11-06 15:22:03,604:INFO:Starting cross validation
2024-11-06 15:22:03,608:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 15:22:03,949:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 15:22:03,955:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 15:22:04,007:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:04,008:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 15:22:04,009:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 15:22:04,029:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:04,057:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:04,075:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:04,105:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 15:22:04,106:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 15:22:04,122:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 15:22:04,154:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:04,161:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 15:22:04,172:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:04,206:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:04,211:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 15:22:04,213:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:04,225:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 15:22:04,269:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:04,303:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:04,311:INFO:Calculating mean and std
2024-11-06 15:22:04,312:INFO:Creating metrics dataframe
2024-11-06 15:22:04,313:INFO:Uploading results into container
2024-11-06 15:22:04,314:INFO:Uploading model into container now
2024-11-06 15:22:04,314:INFO:_master_model_container: 11
2024-11-06 15:22:04,314:INFO:_display_container: 2
2024-11-06 15:22:04,315:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=417, solver='auto',
                tol=0.0001)
2024-11-06 15:22:04,315:INFO:create_model() successfully completed......................................
2024-11-06 15:22:04,452:INFO:SubProcess create_model() end ==================================
2024-11-06 15:22:04,452:INFO:Creating metrics dataframe
2024-11-06 15:22:04,457:INFO:Initializing Random Forest Classifier
2024-11-06 15:22:04,457:INFO:Total runtime is 0.2235804120699565 minutes
2024-11-06 15:22:04,460:INFO:SubProcess create_model() called ==================================
2024-11-06 15:22:04,460:INFO:Initializing create_model()
2024-11-06 15:22:04,460:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc252b8b50>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc1fffa910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 15:22:04,460:INFO:Checking exceptions
2024-11-06 15:22:04,460:INFO:Importing libraries
2024-11-06 15:22:04,460:INFO:Copying training dataset
2024-11-06 15:22:04,605:INFO:Defining folds
2024-11-06 15:22:04,605:INFO:Declaring metric variables
2024-11-06 15:22:04,607:INFO:Importing untrained model
2024-11-06 15:22:04,610:INFO:Random Forest Classifier Imported successfully
2024-11-06 15:22:04,615:INFO:Starting cross validation
2024-11-06 15:22:04,618:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 15:22:05,519:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:22:05,699:INFO:Calculating mean and std
2024-11-06 15:22:05,700:INFO:Creating metrics dataframe
2024-11-06 15:22:05,703:INFO:Uploading results into container
2024-11-06 15:22:05,704:INFO:Uploading model into container now
2024-11-06 15:22:05,705:INFO:_master_model_container: 12
2024-11-06 15:22:05,705:INFO:_display_container: 2
2024-11-06 15:22:05,706:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=417, verbose=0,
                       warm_start=False)
2024-11-06 15:22:05,706:INFO:create_model() successfully completed......................................
2024-11-06 15:22:05,850:INFO:SubProcess create_model() end ==================================
2024-11-06 15:22:05,850:INFO:Creating metrics dataframe
2024-11-06 15:22:05,856:INFO:Initializing Quadratic Discriminant Analysis
2024-11-06 15:22:05,856:INFO:Total runtime is 0.24689813852310183 minutes
2024-11-06 15:22:05,858:INFO:SubProcess create_model() called ==================================
2024-11-06 15:22:05,859:INFO:Initializing create_model()
2024-11-06 15:22:05,859:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc252b8b50>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc1fffa910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 15:22:05,859:INFO:Checking exceptions
2024-11-06 15:22:05,859:INFO:Importing libraries
2024-11-06 15:22:05,859:INFO:Copying training dataset
2024-11-06 15:22:06,019:INFO:Defining folds
2024-11-06 15:22:06,020:INFO:Declaring metric variables
2024-11-06 15:22:06,023:INFO:Importing untrained model
2024-11-06 15:22:06,026:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-06 15:22:06,032:INFO:Starting cross validation
2024-11-06 15:22:06,036:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 15:22:06,337:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 15:22:06,371:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 15:22:06,405:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:06,409:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:22:06,422:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 15:22:06,447:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:06,452:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:22:06,513:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 15:22:06,563:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 15:22:06,564:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:06,567:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:22:06,591:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 15:22:06,606:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:06,612:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:22:06,637:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:06,696:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 15:22:06,742:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 15:22:06,756:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:06,760:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:06,766:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 15:22:06,801:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:06,804:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:22:06,825:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:06,827:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 15:22:06,888:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:06,901:INFO:Calculating mean and std
2024-11-06 15:22:06,903:INFO:Creating metrics dataframe
2024-11-06 15:22:06,907:INFO:Uploading results into container
2024-11-06 15:22:06,909:INFO:Uploading model into container now
2024-11-06 15:22:06,909:INFO:_master_model_container: 13
2024-11-06 15:22:06,909:INFO:_display_container: 2
2024-11-06 15:22:06,910:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-06 15:22:06,910:INFO:create_model() successfully completed......................................
2024-11-06 15:22:07,054:INFO:SubProcess create_model() end ==================================
2024-11-06 15:22:07,054:INFO:Creating metrics dataframe
2024-11-06 15:22:07,059:INFO:Initializing Ada Boost Classifier
2024-11-06 15:22:07,059:INFO:Total runtime is 0.26694558858871464 minutes
2024-11-06 15:22:07,062:INFO:SubProcess create_model() called ==================================
2024-11-06 15:22:07,063:INFO:Initializing create_model()
2024-11-06 15:22:07,063:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc252b8b50>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc1fffa910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 15:22:07,063:INFO:Checking exceptions
2024-11-06 15:22:07,063:INFO:Importing libraries
2024-11-06 15:22:07,063:INFO:Copying training dataset
2024-11-06 15:22:07,213:INFO:Defining folds
2024-11-06 15:22:07,213:INFO:Declaring metric variables
2024-11-06 15:22:07,215:INFO:Importing untrained model
2024-11-06 15:22:07,218:INFO:Ada Boost Classifier Imported successfully
2024-11-06 15:22:07,223:INFO:Starting cross validation
2024-11-06 15:22:07,227:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 15:22:07,574:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 15:22:07,580:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 15:22:07,590:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 15:22:07,712:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 15:22:07,748:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 15:22:07,761:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 15:22:07,785:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 15:22:07,794:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 15:22:07,850:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 15:22:07,894:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 15:22:09,716:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:09,833:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:09,840:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:09,934:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:09,945:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:09,960:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:09,985:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:09,988:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 15:22:09,997:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:10,067:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:10,083:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 15:22:10,101:INFO:Calculating mean and std
2024-11-06 15:22:10,102:INFO:Creating metrics dataframe
2024-11-06 15:22:10,105:INFO:Uploading results into container
2024-11-06 15:22:10,106:INFO:Uploading model into container now
2024-11-06 15:22:10,106:INFO:_master_model_container: 14
2024-11-06 15:22:10,106:INFO:_display_container: 2
2024-11-06 15:22:10,107:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=417)
2024-11-06 15:22:10,107:INFO:create_model() successfully completed......................................
2024-11-06 15:22:10,252:INFO:SubProcess create_model() end ==================================
2024-11-06 15:22:10,252:INFO:Creating metrics dataframe
2024-11-06 15:22:10,258:INFO:Initializing Gradient Boosting Classifier
2024-11-06 15:22:10,258:INFO:Total runtime is 0.32025858561197923 minutes
2024-11-06 15:22:10,260:INFO:SubProcess create_model() called ==================================
2024-11-06 15:22:10,260:INFO:Initializing create_model()
2024-11-06 15:22:10,260:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc252b8b50>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc1fffa910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 15:22:10,260:INFO:Checking exceptions
2024-11-06 15:22:10,260:INFO:Importing libraries
2024-11-06 15:22:10,260:INFO:Copying training dataset
2024-11-06 15:22:10,417:INFO:Defining folds
2024-11-06 15:22:10,417:INFO:Declaring metric variables
2024-11-06 15:22:10,420:INFO:Importing untrained model
2024-11-06 15:22:10,423:INFO:Gradient Boosting Classifier Imported successfully
2024-11-06 15:22:10,428:INFO:Starting cross validation
2024-11-06 15:22:10,432:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 15:47:02,954:INFO:PyCaret ClassificationExperiment
2024-11-06 15:47:02,955:INFO:Logging name: clf-default-name
2024-11-06 15:47:02,955:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-06 15:47:02,955:INFO:version 3.3.2
2024-11-06 15:47:02,955:INFO:Initializing setup()
2024-11-06 15:47:02,955:INFO:self.USI: 4790
2024-11-06 15:47:02,955:INFO:self._variable_keys: {'logging_param', 'idx', 'pipeline', 'seed', 'y_train', 'y_test', 'target_param', 'USI', 'fold_groups_param', 'is_multiclass', 'memory', 'exp_id', 'fold_shuffle_param', '_available_plots', 'y', 'X', 'gpu_n_jobs_param', 'X_test', 'log_plots_param', 'n_jobs_param', 'X_train', 'fold_generator', 'gpu_param', 'data', 'html_param', 'fix_imbalance', 'exp_name_log', '_ml_usecase'}
2024-11-06 15:47:02,955:INFO:Checking environment
2024-11-06 15:47:02,955:INFO:python_version: 3.11.10
2024-11-06 15:47:02,955:INFO:python_build: ('main', 'Oct  3 2024 07:29:13')
2024-11-06 15:47:02,955:INFO:machine: x86_64
2024-11-06 15:47:02,955:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 15:47:02,955:INFO:Memory: svmem(total=33037213696, available=24106049536, percent=27.0, used=8173170688, free=11512307712, active=9122250752, inactive=8875270144, buffers=850960384, cached=12500774912, shared=284577792, slab=2000551936)
2024-11-06 15:47:02,957:INFO:Physical Core: 8
2024-11-06 15:47:02,957:INFO:Logical Core: 16
2024-11-06 15:47:02,957:INFO:Checking libraries
2024-11-06 15:47:02,957:INFO:System:
2024-11-06 15:47:02,957:INFO:    python: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0]
2024-11-06 15:47:02,957:INFO:executable: /home/cid34senhas/miniconda3/envs/myenv/bin/python
2024-11-06 15:47:02,957:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 15:47:02,957:INFO:PyCaret required dependencies:
2024-11-06 15:47:02,957:INFO:                 pip: 24.2
2024-11-06 15:47:02,957:INFO:          setuptools: 75.1.0
2024-11-06 15:47:02,957:INFO:             pycaret: 3.3.2
2024-11-06 15:47:02,957:INFO:             IPython: 8.29.0
2024-11-06 15:47:02,957:INFO:          ipywidgets: 8.1.5
2024-11-06 15:47:02,957:INFO:                tqdm: 4.66.6
2024-11-06 15:47:02,957:INFO:               numpy: 1.26.4
2024-11-06 15:47:02,957:INFO:              pandas: 2.1.4
2024-11-06 15:47:02,957:INFO:              jinja2: 3.1.4
2024-11-06 15:47:02,958:INFO:               scipy: 1.11.4
2024-11-06 15:47:02,958:INFO:              joblib: 1.3.2
2024-11-06 15:47:02,958:INFO:             sklearn: 1.4.2
2024-11-06 15:47:02,958:INFO:                pyod: 2.0.2
2024-11-06 15:47:02,958:INFO:            imblearn: 0.12.4
2024-11-06 15:47:02,958:INFO:   category_encoders: 2.6.4
2024-11-06 15:47:02,958:INFO:            lightgbm: 4.5.0
2024-11-06 15:47:02,958:INFO:               numba: 0.60.0
2024-11-06 15:47:02,958:INFO:            requests: 2.32.3
2024-11-06 15:47:02,958:INFO:          matplotlib: 3.7.5
2024-11-06 15:47:02,958:INFO:          scikitplot: 0.3.7
2024-11-06 15:47:02,958:INFO:         yellowbrick: 1.5
2024-11-06 15:47:02,958:INFO:              plotly: 5.24.1
2024-11-06 15:47:02,958:INFO:    plotly-resampler: Not installed
2024-11-06 15:47:02,958:INFO:             kaleido: 0.2.1
2024-11-06 15:47:02,958:INFO:           schemdraw: 0.15
2024-11-06 15:47:02,958:INFO:         statsmodels: 0.14.4
2024-11-06 15:47:02,958:INFO:              sktime: 0.26.0
2024-11-06 15:47:02,958:INFO:               tbats: 1.1.3
2024-11-06 15:47:02,959:INFO:            pmdarima: 2.0.4
2024-11-06 15:47:02,959:INFO:              psutil: 6.1.0
2024-11-06 15:47:02,959:INFO:          markupsafe: 3.0.2
2024-11-06 15:47:02,959:INFO:             pickle5: Not installed
2024-11-06 15:47:02,959:INFO:         cloudpickle: 3.1.0
2024-11-06 15:47:02,959:INFO:         deprecation: 2.1.0
2024-11-06 15:47:02,959:INFO:              xxhash: 3.5.0
2024-11-06 15:47:02,959:INFO:           wurlitzer: 3.1.1
2024-11-06 15:47:02,959:INFO:PyCaret optional dependencies:
2024-11-06 15:47:02,959:INFO:                shap: Not installed
2024-11-06 15:47:02,959:INFO:           interpret: Not installed
2024-11-06 15:47:02,959:INFO:                umap: Not installed
2024-11-06 15:47:02,959:INFO:     ydata_profiling: Not installed
2024-11-06 15:47:02,959:INFO:  explainerdashboard: Not installed
2024-11-06 15:47:02,959:INFO:             autoviz: Not installed
2024-11-06 15:47:02,959:INFO:           fairlearn: Not installed
2024-11-06 15:47:02,959:INFO:          deepchecks: Not installed
2024-11-06 15:47:02,959:INFO:             xgboost: 2.1.2
2024-11-06 15:47:02,959:INFO:            catboost: Not installed
2024-11-06 15:47:02,960:INFO:              kmodes: Not installed
2024-11-06 15:47:02,960:INFO:             mlxtend: Not installed
2024-11-06 15:47:02,960:INFO:       statsforecast: Not installed
2024-11-06 15:47:02,960:INFO:        tune_sklearn: Not installed
2024-11-06 15:47:02,960:INFO:                 ray: Not installed
2024-11-06 15:47:02,960:INFO:            hyperopt: Not installed
2024-11-06 15:47:02,960:INFO:              optuna: Not installed
2024-11-06 15:47:02,960:INFO:               skopt: Not installed
2024-11-06 15:47:02,960:INFO:              mlflow: Not installed
2024-11-06 15:47:02,960:INFO:              gradio: Not installed
2024-11-06 15:47:02,960:INFO:             fastapi: Not installed
2024-11-06 15:47:02,960:INFO:             uvicorn: Not installed
2024-11-06 15:47:02,960:INFO:              m2cgen: Not installed
2024-11-06 15:47:02,960:INFO:           evidently: Not installed
2024-11-06 15:47:02,960:INFO:               fugue: Not installed
2024-11-06 15:47:02,960:INFO:           streamlit: Not installed
2024-11-06 15:47:02,960:INFO:             prophet: Not installed
2024-11-06 15:47:02,960:INFO:None
2024-11-06 15:47:02,960:INFO:Set up data.
2024-11-06 15:47:03,156:INFO:Set up folding strategy.
2024-11-06 15:47:03,157:INFO:Set up train/test split.
2024-11-06 15:47:03,492:INFO:Set up index.
2024-11-06 15:47:03,496:INFO:Assigning column types.
2024-11-06 15:47:03,866:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-06 15:47:03,901:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 15:47:03,902:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 15:47:03,924:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:47:03,926:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:47:03,961:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 15:47:03,962:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 15:47:03,984:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:47:03,986:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:47:03,987:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-06 15:47:04,022:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 15:47:04,043:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:47:04,045:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:47:04,080:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 15:47:04,102:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:47:04,104:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:47:04,104:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-06 15:47:04,160:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:47:04,162:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:47:04,219:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:47:04,221:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:47:04,222:INFO:Preparing preprocessing pipeline...
2024-11-06 15:47:04,246:INFO:Set up simple imputation.
2024-11-06 15:47:04,270:INFO:Set up column name cleaning.
2024-11-06 15:47:05,103:INFO:Finished creating preprocessing pipeline.
2024-11-06 15:47:05,114:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['diagnostics_Image-original_Mean',
                                             'diagnostics_Image-original_Maximum',
                                             'diagnostics_Mask-original_VoxelNum',
                                             'original_shape_Elongation',
                                             'original_shape_Flatness',
                                             'original_shape_LeastAxisLength',
                                             'original_shape_MajorAxisLength',
                                             'ori...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-06 15:47:05,114:INFO:Creating final display dataframe.
2024-11-06 15:47:06,402:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pipeline.py:289: UserWarning: Persisting input arguments took 0.58s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-11-06 15:47:07,820:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pipeline.py:289: UserWarning: Persisting input arguments took 0.57s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-11-06 15:47:08,705:INFO:Setup _display_container:                     Description             Value
0                    Session id              6646
1                        Target        Transition
2                   Target type        Multiclass
3           Original data shape       (295, 2014)
4        Transformed data shape       (295, 2014)
5   Transformed train set shape       (206, 2014)
6    Transformed test set shape        (89, 2014)
7              Numeric features              2013
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              4790
2024-11-06 15:47:08,770:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:47:08,772:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:47:08,829:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:47:08,831:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:47:08,832:INFO:setup() successfully completed in 5.88s...............
2024-11-06 15:58:12,730:INFO:PyCaret ClassificationExperiment
2024-11-06 15:58:12,731:INFO:Logging name: clf-default-name
2024-11-06 15:58:12,731:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-06 15:58:12,731:INFO:version 3.3.2
2024-11-06 15:58:12,731:INFO:Initializing setup()
2024-11-06 15:58:12,731:INFO:self.USI: 54af
2024-11-06 15:58:12,731:INFO:self._variable_keys: {'logging_param', 'idx', 'pipeline', 'seed', 'y_train', 'y_test', 'target_param', 'USI', 'fold_groups_param', 'is_multiclass', 'memory', 'exp_id', 'fold_shuffle_param', '_available_plots', 'y', 'X', 'gpu_n_jobs_param', 'X_test', 'log_plots_param', 'n_jobs_param', 'X_train', 'fold_generator', 'gpu_param', 'data', 'html_param', 'fix_imbalance', 'exp_name_log', '_ml_usecase'}
2024-11-06 15:58:12,731:INFO:Checking environment
2024-11-06 15:58:12,731:INFO:python_version: 3.11.10
2024-11-06 15:58:12,731:INFO:python_build: ('main', 'Oct  3 2024 07:29:13')
2024-11-06 15:58:12,731:INFO:machine: x86_64
2024-11-06 15:58:12,731:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 15:58:12,732:INFO:Memory: svmem(total=33037213696, available=24003690496, percent=27.3, used=8238845952, free=11398107136, active=9253789696, inactive=8886636544, buffers=855031808, cached=12545228800, shared=321298432, slab=2004078592)
2024-11-06 15:58:12,733:INFO:Physical Core: 8
2024-11-06 15:58:12,733:INFO:Logical Core: 16
2024-11-06 15:58:12,733:INFO:Checking libraries
2024-11-06 15:58:12,733:INFO:System:
2024-11-06 15:58:12,733:INFO:    python: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0]
2024-11-06 15:58:12,733:INFO:executable: /home/cid34senhas/miniconda3/envs/myenv/bin/python
2024-11-06 15:58:12,734:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 15:58:12,734:INFO:PyCaret required dependencies:
2024-11-06 15:58:12,734:INFO:                 pip: 24.2
2024-11-06 15:58:12,734:INFO:          setuptools: 75.1.0
2024-11-06 15:58:12,734:INFO:             pycaret: 3.3.2
2024-11-06 15:58:12,734:INFO:             IPython: 8.29.0
2024-11-06 15:58:12,734:INFO:          ipywidgets: 8.1.5
2024-11-06 15:58:12,734:INFO:                tqdm: 4.66.6
2024-11-06 15:58:12,734:INFO:               numpy: 1.26.4
2024-11-06 15:58:12,734:INFO:              pandas: 2.1.4
2024-11-06 15:58:12,734:INFO:              jinja2: 3.1.4
2024-11-06 15:58:12,734:INFO:               scipy: 1.11.4
2024-11-06 15:58:12,734:INFO:              joblib: 1.3.2
2024-11-06 15:58:12,734:INFO:             sklearn: 1.4.2
2024-11-06 15:58:12,734:INFO:                pyod: 2.0.2
2024-11-06 15:58:12,734:INFO:            imblearn: 0.12.4
2024-11-06 15:58:12,734:INFO:   category_encoders: 2.6.4
2024-11-06 15:58:12,734:INFO:            lightgbm: 4.5.0
2024-11-06 15:58:12,734:INFO:               numba: 0.60.0
2024-11-06 15:58:12,734:INFO:            requests: 2.32.3
2024-11-06 15:58:12,735:INFO:          matplotlib: 3.7.5
2024-11-06 15:58:12,735:INFO:          scikitplot: 0.3.7
2024-11-06 15:58:12,735:INFO:         yellowbrick: 1.5
2024-11-06 15:58:12,735:INFO:              plotly: 5.24.1
2024-11-06 15:58:12,735:INFO:    plotly-resampler: Not installed
2024-11-06 15:58:12,735:INFO:             kaleido: 0.2.1
2024-11-06 15:58:12,735:INFO:           schemdraw: 0.15
2024-11-06 15:58:12,735:INFO:         statsmodels: 0.14.4
2024-11-06 15:58:12,735:INFO:              sktime: 0.26.0
2024-11-06 15:58:12,735:INFO:               tbats: 1.1.3
2024-11-06 15:58:12,735:INFO:            pmdarima: 2.0.4
2024-11-06 15:58:12,735:INFO:              psutil: 6.1.0
2024-11-06 15:58:12,735:INFO:          markupsafe: 3.0.2
2024-11-06 15:58:12,735:INFO:             pickle5: Not installed
2024-11-06 15:58:12,735:INFO:         cloudpickle: 3.1.0
2024-11-06 15:58:12,735:INFO:         deprecation: 2.1.0
2024-11-06 15:58:12,735:INFO:              xxhash: 3.5.0
2024-11-06 15:58:12,736:INFO:           wurlitzer: 3.1.1
2024-11-06 15:58:12,736:INFO:PyCaret optional dependencies:
2024-11-06 15:58:12,736:INFO:                shap: Not installed
2024-11-06 15:58:12,736:INFO:           interpret: Not installed
2024-11-06 15:58:12,736:INFO:                umap: Not installed
2024-11-06 15:58:12,736:INFO:     ydata_profiling: Not installed
2024-11-06 15:58:12,736:INFO:  explainerdashboard: Not installed
2024-11-06 15:58:12,736:INFO:             autoviz: Not installed
2024-11-06 15:58:12,736:INFO:           fairlearn: Not installed
2024-11-06 15:58:12,736:INFO:          deepchecks: Not installed
2024-11-06 15:58:12,736:INFO:             xgboost: 2.1.2
2024-11-06 15:58:12,736:INFO:            catboost: Not installed
2024-11-06 15:58:12,736:INFO:              kmodes: Not installed
2024-11-06 15:58:12,736:INFO:             mlxtend: Not installed
2024-11-06 15:58:12,736:INFO:       statsforecast: Not installed
2024-11-06 15:58:12,737:INFO:        tune_sklearn: Not installed
2024-11-06 15:58:12,737:INFO:                 ray: Not installed
2024-11-06 15:58:12,737:INFO:            hyperopt: Not installed
2024-11-06 15:58:12,737:INFO:              optuna: Not installed
2024-11-06 15:58:12,737:INFO:               skopt: Not installed
2024-11-06 15:58:12,737:INFO:              mlflow: Not installed
2024-11-06 15:58:12,737:INFO:              gradio: Not installed
2024-11-06 15:58:12,737:INFO:             fastapi: Not installed
2024-11-06 15:58:12,737:INFO:             uvicorn: Not installed
2024-11-06 15:58:12,737:INFO:              m2cgen: Not installed
2024-11-06 15:58:12,737:INFO:           evidently: Not installed
2024-11-06 15:58:12,737:INFO:               fugue: Not installed
2024-11-06 15:58:12,737:INFO:           streamlit: Not installed
2024-11-06 15:58:12,737:INFO:             prophet: Not installed
2024-11-06 15:58:12,737:INFO:None
2024-11-06 15:58:12,737:INFO:Set up data.
2024-11-06 15:58:12,949:INFO:Set up folding strategy.
2024-11-06 15:58:12,950:INFO:Set up train/test split.
2024-11-06 15:58:13,086:INFO:Set up index.
2024-11-06 15:58:13,090:INFO:Assigning column types.
2024-11-06 15:58:13,419:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-06 15:58:13,457:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 15:58:13,458:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 15:58:13,480:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:58:13,483:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:58:13,518:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 15:58:13,519:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 15:58:13,542:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:58:13,544:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:58:13,545:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-06 15:58:13,580:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 15:58:13,601:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:58:13,603:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:58:13,638:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 15:58:13,660:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:58:13,662:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:58:13,662:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-06 15:58:13,718:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:58:13,720:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:58:13,781:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:58:13,783:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:58:13,783:INFO:Preparing preprocessing pipeline...
2024-11-06 15:58:13,806:INFO:Set up simple imputation.
2024-11-06 15:58:13,826:INFO:Set up column name cleaning.
2024-11-06 15:58:14,729:INFO:Finished creating preprocessing pipeline.
2024-11-06 15:58:14,740:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['diagnostics_Image-original_Mean',
                                             'diagnostics_Image-original_Maximum',
                                             'diagnostics_Mask-original_VoxelNum',
                                             'original_shape_Elongation',
                                             'original_shape_Flatness',
                                             'original_shape_LeastAxisLength',
                                             'original_shape_MajorAxisLength',
                                             'ori...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-06 15:58:14,740:INFO:Creating final display dataframe.
2024-11-06 15:58:16,307:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pipeline.py:289: UserWarning: Persisting input arguments took 0.83s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-11-06 15:58:17,612:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pipeline.py:289: UserWarning: Persisting input arguments took 0.59s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-11-06 15:58:18,552:INFO:Setup _display_container:                     Description             Value
0                    Session id              3148
1                        Target        Transition
2                   Target type        Multiclass
3           Original data shape       (295, 2014)
4        Transformed data shape       (295, 2014)
5   Transformed train set shape       (206, 2014)
6    Transformed test set shape        (89, 2014)
7              Numeric features              2013
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              54af
2024-11-06 15:58:18,613:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:58:18,616:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:58:18,674:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 15:58:18,677:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 15:58:18,678:INFO:setup() successfully completed in 5.95s...............
2024-11-06 16:21:37,029:INFO:PyCaret ClassificationExperiment
2024-11-06 16:21:37,029:INFO:Logging name: clf-default-name
2024-11-06 16:21:37,029:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-06 16:21:37,029:INFO:version 3.3.2
2024-11-06 16:21:37,029:INFO:Initializing setup()
2024-11-06 16:21:37,029:INFO:self.USI: 3223
2024-11-06 16:21:37,029:INFO:self._variable_keys: {'logging_param', 'idx', 'pipeline', 'seed', 'y_train', 'y_test', 'target_param', 'USI', 'fold_groups_param', 'is_multiclass', 'memory', 'exp_id', 'fold_shuffle_param', '_available_plots', 'y', 'X', 'gpu_n_jobs_param', 'X_test', 'log_plots_param', 'n_jobs_param', 'X_train', 'fold_generator', 'gpu_param', 'data', 'html_param', 'fix_imbalance', 'exp_name_log', '_ml_usecase'}
2024-11-06 16:21:37,029:INFO:Checking environment
2024-11-06 16:21:37,029:INFO:python_version: 3.11.10
2024-11-06 16:21:37,029:INFO:python_build: ('main', 'Oct  3 2024 07:29:13')
2024-11-06 16:21:37,029:INFO:machine: x86_64
2024-11-06 16:21:37,029:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 16:21:37,029:INFO:Memory: svmem(total=33037213696, available=23434842112, percent=29.1, used=8841588736, free=10778890240, active=9799024640, inactive=8932757504, buffers=862081024, cached=12554653696, shared=287399936, slab=2017767424)
2024-11-06 16:21:37,031:INFO:Physical Core: 8
2024-11-06 16:21:37,031:INFO:Logical Core: 16
2024-11-06 16:21:37,031:INFO:Checking libraries
2024-11-06 16:21:37,031:INFO:System:
2024-11-06 16:21:37,031:INFO:    python: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0]
2024-11-06 16:21:37,031:INFO:executable: /home/cid34senhas/miniconda3/envs/myenv/bin/python
2024-11-06 16:21:37,031:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 16:21:37,031:INFO:PyCaret required dependencies:
2024-11-06 16:21:37,031:INFO:                 pip: 24.2
2024-11-06 16:21:37,031:INFO:          setuptools: 75.1.0
2024-11-06 16:21:37,031:INFO:             pycaret: 3.3.2
2024-11-06 16:21:37,031:INFO:             IPython: 8.29.0
2024-11-06 16:21:37,031:INFO:          ipywidgets: 8.1.5
2024-11-06 16:21:37,031:INFO:                tqdm: 4.66.6
2024-11-06 16:21:37,031:INFO:               numpy: 1.26.4
2024-11-06 16:21:37,031:INFO:              pandas: 2.1.4
2024-11-06 16:21:37,031:INFO:              jinja2: 3.1.4
2024-11-06 16:21:37,031:INFO:               scipy: 1.11.4
2024-11-06 16:21:37,031:INFO:              joblib: 1.3.2
2024-11-06 16:21:37,032:INFO:             sklearn: 1.4.2
2024-11-06 16:21:37,032:INFO:                pyod: 2.0.2
2024-11-06 16:21:37,032:INFO:            imblearn: 0.12.4
2024-11-06 16:21:37,032:INFO:   category_encoders: 2.6.4
2024-11-06 16:21:37,032:INFO:            lightgbm: 4.5.0
2024-11-06 16:21:37,032:INFO:               numba: 0.60.0
2024-11-06 16:21:37,032:INFO:            requests: 2.32.3
2024-11-06 16:21:37,032:INFO:          matplotlib: 3.7.5
2024-11-06 16:21:37,032:INFO:          scikitplot: 0.3.7
2024-11-06 16:21:37,032:INFO:         yellowbrick: 1.5
2024-11-06 16:21:37,032:INFO:              plotly: 5.24.1
2024-11-06 16:21:37,032:INFO:    plotly-resampler: Not installed
2024-11-06 16:21:37,032:INFO:             kaleido: 0.2.1
2024-11-06 16:21:37,032:INFO:           schemdraw: 0.15
2024-11-06 16:21:37,032:INFO:         statsmodels: 0.14.4
2024-11-06 16:21:37,032:INFO:              sktime: 0.26.0
2024-11-06 16:21:37,032:INFO:               tbats: 1.1.3
2024-11-06 16:21:37,032:INFO:            pmdarima: 2.0.4
2024-11-06 16:21:37,032:INFO:              psutil: 6.1.0
2024-11-06 16:21:37,032:INFO:          markupsafe: 3.0.2
2024-11-06 16:21:37,032:INFO:             pickle5: Not installed
2024-11-06 16:21:37,032:INFO:         cloudpickle: 3.1.0
2024-11-06 16:21:37,032:INFO:         deprecation: 2.1.0
2024-11-06 16:21:37,032:INFO:              xxhash: 3.5.0
2024-11-06 16:21:37,032:INFO:           wurlitzer: 3.1.1
2024-11-06 16:21:37,032:INFO:PyCaret optional dependencies:
2024-11-06 16:21:37,032:INFO:                shap: Not installed
2024-11-06 16:21:37,032:INFO:           interpret: Not installed
2024-11-06 16:21:37,032:INFO:                umap: Not installed
2024-11-06 16:21:37,032:INFO:     ydata_profiling: Not installed
2024-11-06 16:21:37,032:INFO:  explainerdashboard: Not installed
2024-11-06 16:21:37,032:INFO:             autoviz: Not installed
2024-11-06 16:21:37,032:INFO:           fairlearn: Not installed
2024-11-06 16:21:37,032:INFO:          deepchecks: Not installed
2024-11-06 16:21:37,032:INFO:             xgboost: 2.1.2
2024-11-06 16:21:37,032:INFO:            catboost: Not installed
2024-11-06 16:21:37,032:INFO:              kmodes: Not installed
2024-11-06 16:21:37,032:INFO:             mlxtend: Not installed
2024-11-06 16:21:37,032:INFO:       statsforecast: Not installed
2024-11-06 16:21:37,032:INFO:        tune_sklearn: Not installed
2024-11-06 16:21:37,032:INFO:                 ray: Not installed
2024-11-06 16:21:37,032:INFO:            hyperopt: Not installed
2024-11-06 16:21:37,032:INFO:              optuna: Not installed
2024-11-06 16:21:37,032:INFO:               skopt: Not installed
2024-11-06 16:21:37,032:INFO:              mlflow: Not installed
2024-11-06 16:21:37,032:INFO:              gradio: Not installed
2024-11-06 16:21:37,032:INFO:             fastapi: Not installed
2024-11-06 16:21:37,032:INFO:             uvicorn: Not installed
2024-11-06 16:21:37,032:INFO:              m2cgen: Not installed
2024-11-06 16:21:37,032:INFO:           evidently: Not installed
2024-11-06 16:21:37,032:INFO:               fugue: Not installed
2024-11-06 16:21:37,032:INFO:           streamlit: Not installed
2024-11-06 16:21:37,032:INFO:             prophet: Not installed
2024-11-06 16:21:37,032:INFO:None
2024-11-06 16:21:37,032:INFO:Set up data.
2024-11-06 16:21:37,234:INFO:Set up folding strategy.
2024-11-06 16:21:37,234:INFO:Set up train/test split.
2024-11-06 16:21:37,569:INFO:Set up index.
2024-11-06 16:21:37,573:INFO:Assigning column types.
2024-11-06 16:21:37,931:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-06 16:21:37,966:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 16:21:37,966:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:21:37,988:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:21:37,990:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:21:38,025:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 16:21:38,025:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:21:38,047:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:21:38,049:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:21:38,049:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-06 16:21:38,084:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:21:38,106:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:21:38,108:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:21:38,143:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:21:38,165:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:21:38,167:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:21:38,167:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-06 16:21:38,222:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:21:38,224:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:21:38,281:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:21:38,283:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:21:38,283:INFO:Preparing preprocessing pipeline...
2024-11-06 16:21:38,305:INFO:Set up simple imputation.
2024-11-06 16:21:38,324:INFO:Set up column name cleaning.
2024-11-06 16:21:38,952:INFO:Finished creating preprocessing pipeline.
2024-11-06 16:21:38,961:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['diagnostics_Image-original_Mean',
                                             'diagnostics_Image-original_Maximum',
                                             'diagnostics_Mask-original_VoxelNum',
                                             'original_shape_Elongation',
                                             'original_shape_Flatness',
                                             'original_shape_LeastAxisLength',
                                             'original_shape_MajorAxisLength',
                                             'ori...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-06 16:21:38,961:INFO:Creating final display dataframe.
2024-11-06 16:21:40,434:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pipeline.py:289: UserWarning: Persisting input arguments took 0.57s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-11-06 16:21:41,853:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pipeline.py:289: UserWarning: Persisting input arguments took 0.74s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-11-06 16:21:42,734:INFO:Setup _display_container:                     Description             Value
0                    Session id              3436
1                        Target        Transition
2                   Target type        Multiclass
3           Original data shape       (295, 2014)
4        Transformed data shape       (295, 2014)
5   Transformed train set shape       (206, 2014)
6    Transformed test set shape        (89, 2014)
7              Numeric features              2013
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              3223
2024-11-06 16:21:42,795:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:21:42,798:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:21:42,856:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:21:42,859:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:21:42,859:INFO:setup() successfully completed in 5.83s...............
2024-11-06 16:22:03,377:INFO:PyCaret ClassificationExperiment
2024-11-06 16:22:03,378:INFO:Logging name: clf-default-name
2024-11-06 16:22:03,378:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-06 16:22:03,378:INFO:version 3.3.2
2024-11-06 16:22:03,378:INFO:Initializing setup()
2024-11-06 16:22:03,378:INFO:self.USI: 2572
2024-11-06 16:22:03,378:INFO:self._variable_keys: {'logging_param', 'idx', 'pipeline', 'seed', 'y_train', 'y_test', 'target_param', 'USI', 'fold_groups_param', 'is_multiclass', 'memory', 'exp_id', 'fold_shuffle_param', '_available_plots', 'y', 'X', 'gpu_n_jobs_param', 'X_test', 'log_plots_param', 'n_jobs_param', 'X_train', 'fold_generator', 'gpu_param', 'data', 'html_param', 'fix_imbalance', 'exp_name_log', '_ml_usecase'}
2024-11-06 16:22:03,378:INFO:Checking environment
2024-11-06 16:22:03,378:INFO:python_version: 3.11.10
2024-11-06 16:22:03,378:INFO:python_build: ('main', 'Oct  3 2024 07:29:13')
2024-11-06 16:22:03,378:INFO:machine: x86_64
2024-11-06 16:22:03,378:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 16:22:03,379:INFO:Memory: svmem(total=33037213696, available=23460745216, percent=29.0, used=8811479040, free=10798718976, active=9815543808, inactive=8938700800, buffers=862347264, cached=12564668416, shared=291561472, slab=2018021376)
2024-11-06 16:22:03,380:INFO:Physical Core: 8
2024-11-06 16:22:03,381:INFO:Logical Core: 16
2024-11-06 16:22:03,381:INFO:Checking libraries
2024-11-06 16:22:03,381:INFO:System:
2024-11-06 16:22:03,381:INFO:    python: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0]
2024-11-06 16:22:03,381:INFO:executable: /home/cid34senhas/miniconda3/envs/myenv/bin/python
2024-11-06 16:22:03,381:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 16:22:03,381:INFO:PyCaret required dependencies:
2024-11-06 16:22:03,381:INFO:                 pip: 24.2
2024-11-06 16:22:03,381:INFO:          setuptools: 75.1.0
2024-11-06 16:22:03,381:INFO:             pycaret: 3.3.2
2024-11-06 16:22:03,381:INFO:             IPython: 8.29.0
2024-11-06 16:22:03,381:INFO:          ipywidgets: 8.1.5
2024-11-06 16:22:03,381:INFO:                tqdm: 4.66.6
2024-11-06 16:22:03,381:INFO:               numpy: 1.26.4
2024-11-06 16:22:03,381:INFO:              pandas: 2.1.4
2024-11-06 16:22:03,381:INFO:              jinja2: 3.1.4
2024-11-06 16:22:03,381:INFO:               scipy: 1.11.4
2024-11-06 16:22:03,381:INFO:              joblib: 1.3.2
2024-11-06 16:22:03,381:INFO:             sklearn: 1.4.2
2024-11-06 16:22:03,381:INFO:                pyod: 2.0.2
2024-11-06 16:22:03,381:INFO:            imblearn: 0.12.4
2024-11-06 16:22:03,381:INFO:   category_encoders: 2.6.4
2024-11-06 16:22:03,381:INFO:            lightgbm: 4.5.0
2024-11-06 16:22:03,381:INFO:               numba: 0.60.0
2024-11-06 16:22:03,381:INFO:            requests: 2.32.3
2024-11-06 16:22:03,381:INFO:          matplotlib: 3.7.5
2024-11-06 16:22:03,381:INFO:          scikitplot: 0.3.7
2024-11-06 16:22:03,381:INFO:         yellowbrick: 1.5
2024-11-06 16:22:03,381:INFO:              plotly: 5.24.1
2024-11-06 16:22:03,381:INFO:    plotly-resampler: Not installed
2024-11-06 16:22:03,382:INFO:             kaleido: 0.2.1
2024-11-06 16:22:03,382:INFO:           schemdraw: 0.15
2024-11-06 16:22:03,382:INFO:         statsmodels: 0.14.4
2024-11-06 16:22:03,382:INFO:              sktime: 0.26.0
2024-11-06 16:22:03,382:INFO:               tbats: 1.1.3
2024-11-06 16:22:03,382:INFO:            pmdarima: 2.0.4
2024-11-06 16:22:03,382:INFO:              psutil: 6.1.0
2024-11-06 16:22:03,382:INFO:          markupsafe: 3.0.2
2024-11-06 16:22:03,382:INFO:             pickle5: Not installed
2024-11-06 16:22:03,382:INFO:         cloudpickle: 3.1.0
2024-11-06 16:22:03,382:INFO:         deprecation: 2.1.0
2024-11-06 16:22:03,382:INFO:              xxhash: 3.5.0
2024-11-06 16:22:03,382:INFO:           wurlitzer: 3.1.1
2024-11-06 16:22:03,382:INFO:PyCaret optional dependencies:
2024-11-06 16:22:03,382:INFO:                shap: Not installed
2024-11-06 16:22:03,382:INFO:           interpret: Not installed
2024-11-06 16:22:03,382:INFO:                umap: Not installed
2024-11-06 16:22:03,382:INFO:     ydata_profiling: Not installed
2024-11-06 16:22:03,382:INFO:  explainerdashboard: Not installed
2024-11-06 16:22:03,382:INFO:             autoviz: Not installed
2024-11-06 16:22:03,382:INFO:           fairlearn: Not installed
2024-11-06 16:22:03,382:INFO:          deepchecks: Not installed
2024-11-06 16:22:03,382:INFO:             xgboost: 2.1.2
2024-11-06 16:22:03,382:INFO:            catboost: Not installed
2024-11-06 16:22:03,382:INFO:              kmodes: Not installed
2024-11-06 16:22:03,382:INFO:             mlxtend: Not installed
2024-11-06 16:22:03,382:INFO:       statsforecast: Not installed
2024-11-06 16:22:03,382:INFO:        tune_sklearn: Not installed
2024-11-06 16:22:03,382:INFO:                 ray: Not installed
2024-11-06 16:22:03,382:INFO:            hyperopt: Not installed
2024-11-06 16:22:03,382:INFO:              optuna: Not installed
2024-11-06 16:22:03,382:INFO:               skopt: Not installed
2024-11-06 16:22:03,382:INFO:              mlflow: Not installed
2024-11-06 16:22:03,383:INFO:              gradio: Not installed
2024-11-06 16:22:03,383:INFO:             fastapi: Not installed
2024-11-06 16:22:03,383:INFO:             uvicorn: Not installed
2024-11-06 16:22:03,383:INFO:              m2cgen: Not installed
2024-11-06 16:22:03,383:INFO:           evidently: Not installed
2024-11-06 16:22:03,383:INFO:               fugue: Not installed
2024-11-06 16:22:03,383:INFO:           streamlit: Not installed
2024-11-06 16:22:03,383:INFO:             prophet: Not installed
2024-11-06 16:22:03,383:INFO:None
2024-11-06 16:22:03,383:INFO:Set up data.
2024-11-06 16:22:03,583:INFO:Set up folding strategy.
2024-11-06 16:22:03,583:INFO:Set up train/test split.
2024-11-06 16:22:03,711:INFO:Set up index.
2024-11-06 16:22:03,716:INFO:Assigning column types.
2024-11-06 16:22:04,051:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-06 16:22:04,086:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 16:22:04,087:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:22:04,109:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:22:04,111:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:22:04,145:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 16:22:04,146:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:22:04,167:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:22:04,170:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:22:04,170:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-06 16:22:04,205:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:22:04,226:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:22:04,228:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:22:04,263:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:22:04,285:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:22:04,287:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:22:04,287:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-06 16:22:04,342:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:22:04,345:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:22:04,401:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:22:04,403:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:22:04,403:INFO:Preparing preprocessing pipeline...
2024-11-06 16:22:04,426:INFO:Set up simple imputation.
2024-11-06 16:22:04,447:INFO:Set up column name cleaning.
2024-11-06 16:22:05,274:INFO:Finished creating preprocessing pipeline.
2024-11-06 16:22:05,283:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['diagnostics_Image-original_Mean',
                                             'diagnostics_Image-original_Maximum',
                                             'diagnostics_Mask-original_VoxelNum',
                                             'original_shape_Elongation',
                                             'original_shape_Flatness',
                                             'original_shape_LeastAxisLength',
                                             'original_shape_MajorAxisLength',
                                             'ori...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-06 16:22:05,283:INFO:Creating final display dataframe.
2024-11-06 16:22:06,738:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pipeline.py:289: UserWarning: Persisting input arguments took 0.75s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-11-06 16:22:07,968:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pipeline.py:289: UserWarning: Persisting input arguments took 0.56s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-11-06 16:22:08,848:INFO:Setup _display_container:                     Description             Value
0                    Session id              7814
1                        Target        Transition
2                   Target type        Multiclass
3           Original data shape       (295, 2014)
4        Transformed data shape       (295, 2014)
5   Transformed train set shape       (206, 2014)
6    Transformed test set shape        (89, 2014)
7              Numeric features              2013
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              2572
2024-11-06 16:22:08,909:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:22:08,911:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:22:08,967:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:22:08,970:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:22:08,970:INFO:setup() successfully completed in 5.6s...............
2024-11-06 16:22:43,232:INFO:PyCaret ClassificationExperiment
2024-11-06 16:22:43,232:INFO:Logging name: clf-default-name
2024-11-06 16:22:43,232:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-06 16:22:43,232:INFO:version 3.3.2
2024-11-06 16:22:43,232:INFO:Initializing setup()
2024-11-06 16:22:43,232:INFO:self.USI: 1874
2024-11-06 16:22:43,232:INFO:self._variable_keys: {'logging_param', 'idx', 'pipeline', 'seed', 'y_train', 'y_test', 'target_param', 'USI', 'fold_groups_param', 'is_multiclass', 'memory', 'exp_id', 'fold_shuffle_param', '_available_plots', 'y', 'X', 'gpu_n_jobs_param', 'X_test', 'log_plots_param', 'n_jobs_param', 'X_train', 'fold_generator', 'gpu_param', 'data', 'html_param', 'fix_imbalance', 'exp_name_log', '_ml_usecase'}
2024-11-06 16:22:43,232:INFO:Checking environment
2024-11-06 16:22:43,232:INFO:python_version: 3.11.10
2024-11-06 16:22:43,232:INFO:python_build: ('main', 'Oct  3 2024 07:29:13')
2024-11-06 16:22:43,232:INFO:machine: x86_64
2024-11-06 16:22:43,232:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 16:22:43,232:INFO:Memory: svmem(total=33037213696, available=23356895232, percent=29.3, used=8900075520, free=10688737280, active=9868910592, inactive=8944656384, buffers=862752768, cached=12585648128, shared=306855936, slab=2018881536)
2024-11-06 16:22:43,233:INFO:Physical Core: 8
2024-11-06 16:22:43,233:INFO:Logical Core: 16
2024-11-06 16:22:43,233:INFO:Checking libraries
2024-11-06 16:22:43,233:INFO:System:
2024-11-06 16:22:43,233:INFO:    python: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0]
2024-11-06 16:22:43,233:INFO:executable: /home/cid34senhas/miniconda3/envs/myenv/bin/python
2024-11-06 16:22:43,233:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 16:22:43,233:INFO:PyCaret required dependencies:
2024-11-06 16:22:43,233:INFO:                 pip: 24.2
2024-11-06 16:22:43,233:INFO:          setuptools: 75.1.0
2024-11-06 16:22:43,233:INFO:             pycaret: 3.3.2
2024-11-06 16:22:43,233:INFO:             IPython: 8.29.0
2024-11-06 16:22:43,233:INFO:          ipywidgets: 8.1.5
2024-11-06 16:22:43,233:INFO:                tqdm: 4.66.6
2024-11-06 16:22:43,233:INFO:               numpy: 1.26.4
2024-11-06 16:22:43,233:INFO:              pandas: 2.1.4
2024-11-06 16:22:43,233:INFO:              jinja2: 3.1.4
2024-11-06 16:22:43,233:INFO:               scipy: 1.11.4
2024-11-06 16:22:43,233:INFO:              joblib: 1.3.2
2024-11-06 16:22:43,233:INFO:             sklearn: 1.4.2
2024-11-06 16:22:43,233:INFO:                pyod: 2.0.2
2024-11-06 16:22:43,233:INFO:            imblearn: 0.12.4
2024-11-06 16:22:43,233:INFO:   category_encoders: 2.6.4
2024-11-06 16:22:43,233:INFO:            lightgbm: 4.5.0
2024-11-06 16:22:43,233:INFO:               numba: 0.60.0
2024-11-06 16:22:43,233:INFO:            requests: 2.32.3
2024-11-06 16:22:43,233:INFO:          matplotlib: 3.7.5
2024-11-06 16:22:43,233:INFO:          scikitplot: 0.3.7
2024-11-06 16:22:43,233:INFO:         yellowbrick: 1.5
2024-11-06 16:22:43,233:INFO:              plotly: 5.24.1
2024-11-06 16:22:43,233:INFO:    plotly-resampler: Not installed
2024-11-06 16:22:43,233:INFO:             kaleido: 0.2.1
2024-11-06 16:22:43,233:INFO:           schemdraw: 0.15
2024-11-06 16:22:43,233:INFO:         statsmodels: 0.14.4
2024-11-06 16:22:43,233:INFO:              sktime: 0.26.0
2024-11-06 16:22:43,233:INFO:               tbats: 1.1.3
2024-11-06 16:22:43,233:INFO:            pmdarima: 2.0.4
2024-11-06 16:22:43,233:INFO:              psutil: 6.1.0
2024-11-06 16:22:43,233:INFO:          markupsafe: 3.0.2
2024-11-06 16:22:43,233:INFO:             pickle5: Not installed
2024-11-06 16:22:43,233:INFO:         cloudpickle: 3.1.0
2024-11-06 16:22:43,233:INFO:         deprecation: 2.1.0
2024-11-06 16:22:43,233:INFO:              xxhash: 3.5.0
2024-11-06 16:22:43,233:INFO:           wurlitzer: 3.1.1
2024-11-06 16:22:43,234:INFO:PyCaret optional dependencies:
2024-11-06 16:22:43,234:INFO:                shap: Not installed
2024-11-06 16:22:43,234:INFO:           interpret: Not installed
2024-11-06 16:22:43,234:INFO:                umap: Not installed
2024-11-06 16:22:43,234:INFO:     ydata_profiling: Not installed
2024-11-06 16:22:43,234:INFO:  explainerdashboard: Not installed
2024-11-06 16:22:43,234:INFO:             autoviz: Not installed
2024-11-06 16:22:43,234:INFO:           fairlearn: Not installed
2024-11-06 16:22:43,234:INFO:          deepchecks: Not installed
2024-11-06 16:22:43,234:INFO:             xgboost: 2.1.2
2024-11-06 16:22:43,234:INFO:            catboost: Not installed
2024-11-06 16:22:43,234:INFO:              kmodes: Not installed
2024-11-06 16:22:43,234:INFO:             mlxtend: Not installed
2024-11-06 16:22:43,234:INFO:       statsforecast: Not installed
2024-11-06 16:22:43,234:INFO:        tune_sklearn: Not installed
2024-11-06 16:22:43,234:INFO:                 ray: Not installed
2024-11-06 16:22:43,234:INFO:            hyperopt: Not installed
2024-11-06 16:22:43,234:INFO:              optuna: Not installed
2024-11-06 16:22:43,234:INFO:               skopt: Not installed
2024-11-06 16:22:43,234:INFO:              mlflow: Not installed
2024-11-06 16:22:43,234:INFO:              gradio: Not installed
2024-11-06 16:22:43,234:INFO:             fastapi: Not installed
2024-11-06 16:22:43,234:INFO:             uvicorn: Not installed
2024-11-06 16:22:43,234:INFO:              m2cgen: Not installed
2024-11-06 16:22:43,234:INFO:           evidently: Not installed
2024-11-06 16:22:43,234:INFO:               fugue: Not installed
2024-11-06 16:22:43,234:INFO:           streamlit: Not installed
2024-11-06 16:22:43,234:INFO:             prophet: Not installed
2024-11-06 16:22:43,234:INFO:None
2024-11-06 16:22:43,234:INFO:Set up data.
2024-11-06 16:22:43,611:INFO:Set up folding strategy.
2024-11-06 16:22:43,612:INFO:Set up train/test split.
2024-11-06 16:22:43,732:INFO:Set up index.
2024-11-06 16:22:43,736:INFO:Assigning column types.
2024-11-06 16:22:44,067:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-06 16:22:44,102:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 16:22:44,103:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:22:44,124:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:22:44,126:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:22:44,160:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 16:22:44,161:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:22:44,182:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:22:44,184:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:22:44,184:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-06 16:22:44,219:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:22:44,240:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:22:44,242:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:22:44,275:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:22:44,297:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:22:44,299:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:22:44,299:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-06 16:22:44,353:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:22:44,355:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:22:44,411:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:22:44,413:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:22:44,414:INFO:Preparing preprocessing pipeline...
2024-11-06 16:22:44,435:INFO:Set up simple imputation.
2024-11-06 16:22:44,455:INFO:Set up column name cleaning.
2024-11-06 16:22:45,811:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pipeline.py:256: UserWarning: Persisting input arguments took 0.56s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_transform(

2024-11-06 16:22:45,829:INFO:Finished creating preprocessing pipeline.
2024-11-06 16:22:45,839:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['diagnostics_Image-original_Mean',
                                             'diagnostics_Image-original_Maximum',
                                             'diagnostics_Mask-original_VoxelNum',
                                             'original_shape_Elongation',
                                             'original_shape_Flatness',
                                             'original_shape_LeastAxisLength',
                                             'original_shape_MajorAxisLength',
                                             'ori...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-06 16:22:45,839:INFO:Creating final display dataframe.
2024-11-06 16:22:47,109:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pipeline.py:289: UserWarning: Persisting input arguments took 0.57s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-11-06 16:22:48,577:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pipeline.py:289: UserWarning: Persisting input arguments took 0.59s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-11-06 16:22:49,500:INFO:Setup _display_container:                     Description             Value
0                    Session id              6881
1                        Target        Transition
2                   Target type        Multiclass
3           Original data shape       (295, 2014)
4        Transformed data shape       (295, 2014)
5   Transformed train set shape       (206, 2014)
6    Transformed test set shape        (89, 2014)
7              Numeric features              2013
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              1874
2024-11-06 16:22:49,566:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:22:49,568:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:22:49,629:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:22:49,631:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:22:49,632:INFO:setup() successfully completed in 6.4s...............
2024-11-06 16:31:00,822:INFO:PyCaret ClassificationExperiment
2024-11-06 16:31:00,823:INFO:Logging name: clf-default-name
2024-11-06 16:31:00,823:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-06 16:31:00,823:INFO:version 3.3.2
2024-11-06 16:31:00,823:INFO:Initializing setup()
2024-11-06 16:31:00,823:INFO:self.USI: aad9
2024-11-06 16:31:00,823:INFO:self._variable_keys: {'logging_param', 'idx', 'pipeline', 'seed', 'y_train', 'y_test', 'target_param', 'USI', 'fold_groups_param', 'is_multiclass', 'memory', 'exp_id', 'fold_shuffle_param', '_available_plots', 'y', 'X', 'gpu_n_jobs_param', 'X_test', 'log_plots_param', 'n_jobs_param', 'X_train', 'fold_generator', 'gpu_param', 'data', 'html_param', 'fix_imbalance', 'exp_name_log', '_ml_usecase'}
2024-11-06 16:31:00,823:INFO:Checking environment
2024-11-06 16:31:00,823:INFO:python_version: 3.11.10
2024-11-06 16:31:00,823:INFO:python_build: ('main', 'Oct  3 2024 07:29:13')
2024-11-06 16:31:00,823:INFO:machine: x86_64
2024-11-06 16:31:00,823:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 16:31:00,823:INFO:Memory: svmem(total=33037213696, available=23246426112, percent=29.6, used=9031421952, free=10529763328, active=9992507392, inactive=8991264768, buffers=866828288, cached=12609200128, shared=285982720, slab=2023612416)
2024-11-06 16:31:00,825:INFO:Physical Core: 8
2024-11-06 16:31:00,825:INFO:Logical Core: 16
2024-11-06 16:31:00,825:INFO:Checking libraries
2024-11-06 16:31:00,825:INFO:System:
2024-11-06 16:31:00,825:INFO:    python: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0]
2024-11-06 16:31:00,825:INFO:executable: /home/cid34senhas/miniconda3/envs/myenv/bin/python
2024-11-06 16:31:00,825:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 16:31:00,825:INFO:PyCaret required dependencies:
2024-11-06 16:31:00,825:INFO:                 pip: 24.2
2024-11-06 16:31:00,825:INFO:          setuptools: 75.1.0
2024-11-06 16:31:00,825:INFO:             pycaret: 3.3.2
2024-11-06 16:31:00,825:INFO:             IPython: 8.29.0
2024-11-06 16:31:00,825:INFO:          ipywidgets: 8.1.5
2024-11-06 16:31:00,825:INFO:                tqdm: 4.66.6
2024-11-06 16:31:00,825:INFO:               numpy: 1.26.4
2024-11-06 16:31:00,825:INFO:              pandas: 2.1.4
2024-11-06 16:31:00,825:INFO:              jinja2: 3.1.4
2024-11-06 16:31:00,826:INFO:               scipy: 1.11.4
2024-11-06 16:31:00,826:INFO:              joblib: 1.3.2
2024-11-06 16:31:00,826:INFO:             sklearn: 1.4.2
2024-11-06 16:31:00,826:INFO:                pyod: 2.0.2
2024-11-06 16:31:00,826:INFO:            imblearn: 0.12.4
2024-11-06 16:31:00,826:INFO:   category_encoders: 2.6.4
2024-11-06 16:31:00,826:INFO:            lightgbm: 4.5.0
2024-11-06 16:31:00,826:INFO:               numba: 0.60.0
2024-11-06 16:31:00,826:INFO:            requests: 2.32.3
2024-11-06 16:31:00,826:INFO:          matplotlib: 3.7.5
2024-11-06 16:31:00,826:INFO:          scikitplot: 0.3.7
2024-11-06 16:31:00,826:INFO:         yellowbrick: 1.5
2024-11-06 16:31:00,826:INFO:              plotly: 5.24.1
2024-11-06 16:31:00,826:INFO:    plotly-resampler: Not installed
2024-11-06 16:31:00,826:INFO:             kaleido: 0.2.1
2024-11-06 16:31:00,826:INFO:           schemdraw: 0.15
2024-11-06 16:31:00,826:INFO:         statsmodels: 0.14.4
2024-11-06 16:31:00,826:INFO:              sktime: 0.26.0
2024-11-06 16:31:00,826:INFO:               tbats: 1.1.3
2024-11-06 16:31:00,826:INFO:            pmdarima: 2.0.4
2024-11-06 16:31:00,826:INFO:              psutil: 6.1.0
2024-11-06 16:31:00,826:INFO:          markupsafe: 3.0.2
2024-11-06 16:31:00,826:INFO:             pickle5: Not installed
2024-11-06 16:31:00,826:INFO:         cloudpickle: 3.1.0
2024-11-06 16:31:00,826:INFO:         deprecation: 2.1.0
2024-11-06 16:31:00,826:INFO:              xxhash: 3.5.0
2024-11-06 16:31:00,826:INFO:           wurlitzer: 3.1.1
2024-11-06 16:31:00,826:INFO:PyCaret optional dependencies:
2024-11-06 16:31:00,827:INFO:                shap: Not installed
2024-11-06 16:31:00,827:INFO:           interpret: Not installed
2024-11-06 16:31:00,827:INFO:                umap: Not installed
2024-11-06 16:31:00,827:INFO:     ydata_profiling: Not installed
2024-11-06 16:31:00,827:INFO:  explainerdashboard: Not installed
2024-11-06 16:31:00,827:INFO:             autoviz: Not installed
2024-11-06 16:31:00,827:INFO:           fairlearn: Not installed
2024-11-06 16:31:00,827:INFO:          deepchecks: Not installed
2024-11-06 16:31:00,827:INFO:             xgboost: 2.1.2
2024-11-06 16:31:00,827:INFO:            catboost: Not installed
2024-11-06 16:31:00,827:INFO:              kmodes: Not installed
2024-11-06 16:31:00,827:INFO:             mlxtend: Not installed
2024-11-06 16:31:00,827:INFO:       statsforecast: Not installed
2024-11-06 16:31:00,827:INFO:        tune_sklearn: Not installed
2024-11-06 16:31:00,827:INFO:                 ray: Not installed
2024-11-06 16:31:00,827:INFO:            hyperopt: Not installed
2024-11-06 16:31:00,827:INFO:              optuna: Not installed
2024-11-06 16:31:00,827:INFO:               skopt: Not installed
2024-11-06 16:31:00,827:INFO:              mlflow: Not installed
2024-11-06 16:31:00,827:INFO:              gradio: Not installed
2024-11-06 16:31:00,827:INFO:             fastapi: Not installed
2024-11-06 16:31:00,827:INFO:             uvicorn: Not installed
2024-11-06 16:31:00,827:INFO:              m2cgen: Not installed
2024-11-06 16:31:00,827:INFO:           evidently: Not installed
2024-11-06 16:31:00,827:INFO:               fugue: Not installed
2024-11-06 16:31:00,827:INFO:           streamlit: Not installed
2024-11-06 16:31:00,827:INFO:             prophet: Not installed
2024-11-06 16:31:00,828:INFO:None
2024-11-06 16:31:00,828:INFO:Set up data.
2024-11-06 16:31:01,377:INFO:Set up folding strategy.
2024-11-06 16:31:01,378:INFO:Set up train/test split.
2024-11-06 16:31:01,519:INFO:Set up index.
2024-11-06 16:31:01,527:INFO:Assigning column types.
2024-11-06 16:31:01,923:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-06 16:31:01,959:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 16:31:01,959:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:31:01,981:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:31:01,983:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:31:02,018:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 16:31:02,019:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:31:02,041:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:31:02,043:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:31:02,044:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-06 16:31:02,078:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:31:02,101:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:31:02,103:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:31:02,140:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:31:02,163:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:31:02,165:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:31:02,165:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-06 16:31:02,221:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:31:02,223:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:31:02,279:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:31:02,281:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:31:02,282:INFO:Preparing preprocessing pipeline...
2024-11-06 16:31:02,304:INFO:Set up simple imputation.
2024-11-06 16:31:02,324:INFO:Set up column name cleaning.
2024-11-06 16:31:02,685:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 16:31:02,685:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 16:31:02,685:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 16:31:02,685:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 16:31:03,477:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pipeline.py:249: UserWarning: Persisting input arguments took 0.58s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  fitted_transformer = self._memory_fit(

2024-11-06 16:31:03,746:INFO:Finished creating preprocessing pipeline.
2024-11-06 16:31:03,757:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['diagnostics_Image-original_Mean',
                                             'diagnostics_Image-original_Maximum',
                                             'diagnostics_Mask-original_VoxelNum',
                                             'original_shape_Elongation',
                                             'original_shape_Flatness',
                                             'original_shape_LeastAxisLength',
                                             'original_shape_MajorAxisLength',
                                             'ori...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-06 16:31:03,757:INFO:Creating final display dataframe.
2024-11-06 16:31:05,220:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pipeline.py:289: UserWarning: Persisting input arguments took 0.75s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-11-06 16:31:06,486:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pipeline.py:289: UserWarning: Persisting input arguments took 0.57s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-11-06 16:31:07,399:INFO:Setup _display_container:                     Description             Value
0                    Session id              5881
1                        Target        Transition
2                   Target type        Multiclass
3           Original data shape       (295, 2014)
4        Transformed data shape       (295, 2014)
5   Transformed train set shape       (206, 2014)
6    Transformed test set shape        (89, 2014)
7              Numeric features              2013
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              aad9
2024-11-06 16:31:07,462:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:31:07,464:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:31:07,522:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:31:07,524:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:31:07,525:INFO:setup() successfully completed in 6.71s...............
2024-11-06 16:31:07,545:INFO:Initializing compare_models()
2024-11-06 16:31:07,545:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc1fdc3610>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x79cc1fdc3610>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-06 16:31:07,546:INFO:Checking exceptions
2024-11-06 16:31:07,836:INFO:Preparing display monitor
2024-11-06 16:31:07,854:INFO:Initializing Logistic Regression
2024-11-06 16:31:07,854:INFO:Total runtime is 4.804134368896484e-06 minutes
2024-11-06 16:31:07,857:INFO:SubProcess create_model() called ==================================
2024-11-06 16:31:07,857:INFO:Initializing create_model()
2024-11-06 16:31:07,857:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc1fdc3610>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc91132f90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:31:07,857:INFO:Checking exceptions
2024-11-06 16:31:07,857:INFO:Importing libraries
2024-11-06 16:31:07,857:INFO:Copying training dataset
2024-11-06 16:31:08,028:INFO:Defining folds
2024-11-06 16:31:08,028:INFO:Declaring metric variables
2024-11-06 16:31:08,030:INFO:Importing untrained model
2024-11-06 16:31:08,033:INFO:Logistic Regression Imported successfully
2024-11-06 16:31:08,038:INFO:Starting cross validation
2024-11-06 16:31:08,043:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:31:17,708:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:31:17,757:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:31:17,809:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:17,856:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:18,030:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:31:18,041:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:31:18,055:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:31:18,096:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:18,115:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:18,133:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:18,202:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:31:18,254:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:18,305:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:31:18,336:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:31:18,348:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:31:18,359:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:18,395:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:18,404:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:18,412:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:31:18,464:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:18,479:INFO:Calculating mean and std
2024-11-06 16:31:18,482:INFO:Creating metrics dataframe
2024-11-06 16:31:18,485:INFO:Uploading results into container
2024-11-06 16:31:18,486:INFO:Uploading model into container now
2024-11-06 16:31:18,487:INFO:_master_model_container: 1
2024-11-06 16:31:18,488:INFO:_display_container: 2
2024-11-06 16:31:18,488:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=5881, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-06 16:31:18,488:INFO:create_model() successfully completed......................................
2024-11-06 16:31:18,694:INFO:SubProcess create_model() end ==================================
2024-11-06 16:31:18,694:INFO:Creating metrics dataframe
2024-11-06 16:31:18,699:INFO:Initializing K Neighbors Classifier
2024-11-06 16:31:18,699:INFO:Total runtime is 0.18076170682907106 minutes
2024-11-06 16:31:18,702:INFO:SubProcess create_model() called ==================================
2024-11-06 16:31:18,703:INFO:Initializing create_model()
2024-11-06 16:31:18,703:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc1fdc3610>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc91132f90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:31:18,703:INFO:Checking exceptions
2024-11-06 16:31:18,703:INFO:Importing libraries
2024-11-06 16:31:18,703:INFO:Copying training dataset
2024-11-06 16:31:18,871:INFO:Defining folds
2024-11-06 16:31:18,872:INFO:Declaring metric variables
2024-11-06 16:31:18,874:INFO:Importing untrained model
2024-11-06 16:31:18,877:INFO:K Neighbors Classifier Imported successfully
2024-11-06 16:31:18,883:INFO:Starting cross validation
2024-11-06 16:31:18,887:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:31:19,590:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:19,593:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:20,752:INFO:Calculating mean and std
2024-11-06 16:31:20,754:INFO:Creating metrics dataframe
2024-11-06 16:31:20,756:INFO:Uploading results into container
2024-11-06 16:31:20,756:INFO:Uploading model into container now
2024-11-06 16:31:20,757:INFO:_master_model_container: 2
2024-11-06 16:31:20,757:INFO:_display_container: 2
2024-11-06 16:31:20,757:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-06 16:31:20,757:INFO:create_model() successfully completed......................................
2024-11-06 16:31:20,937:INFO:SubProcess create_model() end ==================================
2024-11-06 16:31:20,937:INFO:Creating metrics dataframe
2024-11-06 16:31:20,944:INFO:Initializing Naive Bayes
2024-11-06 16:31:20,944:INFO:Total runtime is 0.21817438602447511 minutes
2024-11-06 16:31:20,947:INFO:SubProcess create_model() called ==================================
2024-11-06 16:31:20,947:INFO:Initializing create_model()
2024-11-06 16:31:20,947:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc1fdc3610>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc91132f90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:31:20,947:INFO:Checking exceptions
2024-11-06 16:31:20,947:INFO:Importing libraries
2024-11-06 16:31:20,947:INFO:Copying training dataset
2024-11-06 16:31:21,125:INFO:Defining folds
2024-11-06 16:31:21,125:INFO:Declaring metric variables
2024-11-06 16:31:21,128:INFO:Importing untrained model
2024-11-06 16:31:21,132:INFO:Naive Bayes Imported successfully
2024-11-06 16:31:21,139:INFO:Starting cross validation
2024-11-06 16:31:21,142:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:31:21,872:INFO:Calculating mean and std
2024-11-06 16:31:21,873:INFO:Creating metrics dataframe
2024-11-06 16:31:21,874:INFO:Uploading results into container
2024-11-06 16:31:21,874:INFO:Uploading model into container now
2024-11-06 16:31:21,875:INFO:_master_model_container: 3
2024-11-06 16:31:21,875:INFO:_display_container: 2
2024-11-06 16:31:21,875:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-06 16:31:21,875:INFO:create_model() successfully completed......................................
2024-11-06 16:31:22,067:INFO:SubProcess create_model() end ==================================
2024-11-06 16:31:22,067:INFO:Creating metrics dataframe
2024-11-06 16:31:22,073:INFO:Initializing Decision Tree Classifier
2024-11-06 16:31:22,073:INFO:Total runtime is 0.2369938890139262 minutes
2024-11-06 16:31:22,076:INFO:SubProcess create_model() called ==================================
2024-11-06 16:31:22,076:INFO:Initializing create_model()
2024-11-06 16:31:22,076:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc1fdc3610>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc91132f90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:31:22,076:INFO:Checking exceptions
2024-11-06 16:31:22,076:INFO:Importing libraries
2024-11-06 16:31:22,076:INFO:Copying training dataset
2024-11-06 16:31:22,246:INFO:Defining folds
2024-11-06 16:31:22,246:INFO:Declaring metric variables
2024-11-06 16:31:22,248:INFO:Importing untrained model
2024-11-06 16:31:22,250:INFO:Decision Tree Classifier Imported successfully
2024-11-06 16:31:22,255:INFO:Starting cross validation
2024-11-06 16:31:22,258:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:31:23,156:INFO:Calculating mean and std
2024-11-06 16:31:23,156:INFO:Creating metrics dataframe
2024-11-06 16:31:23,158:INFO:Uploading results into container
2024-11-06 16:31:23,158:INFO:Uploading model into container now
2024-11-06 16:31:23,159:INFO:_master_model_container: 4
2024-11-06 16:31:23,159:INFO:_display_container: 2
2024-11-06 16:31:23,159:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=5881, splitter='best')
2024-11-06 16:31:23,159:INFO:create_model() successfully completed......................................
2024-11-06 16:31:23,339:INFO:SubProcess create_model() end ==================================
2024-11-06 16:31:23,339:INFO:Creating metrics dataframe
2024-11-06 16:31:23,344:INFO:Initializing SVM - Linear Kernel
2024-11-06 16:31:23,344:INFO:Total runtime is 0.2581784764925639 minutes
2024-11-06 16:31:23,346:INFO:SubProcess create_model() called ==================================
2024-11-06 16:31:23,347:INFO:Initializing create_model()
2024-11-06 16:31:23,347:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc1fdc3610>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc91132f90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:31:23,347:INFO:Checking exceptions
2024-11-06 16:31:23,347:INFO:Importing libraries
2024-11-06 16:31:23,347:INFO:Copying training dataset
2024-11-06 16:31:23,518:INFO:Defining folds
2024-11-06 16:31:23,518:INFO:Declaring metric variables
2024-11-06 16:31:23,521:INFO:Importing untrained model
2024-11-06 16:31:23,523:INFO:SVM - Linear Kernel Imported successfully
2024-11-06 16:31:23,528:INFO:Starting cross validation
2024-11-06 16:31:23,532:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:31:23,972:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:23,977:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:24,062:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:24,066:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:24,066:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:24,070:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:24,175:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:24,180:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:24,182:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:24,187:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:24,271:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:24,275:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:24,295:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:24,298:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:24,307:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:24,310:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:24,318:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:24,322:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:24,373:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:24,376:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:24,391:INFO:Calculating mean and std
2024-11-06 16:31:24,392:INFO:Creating metrics dataframe
2024-11-06 16:31:24,394:INFO:Uploading results into container
2024-11-06 16:31:24,394:INFO:Uploading model into container now
2024-11-06 16:31:24,394:INFO:_master_model_container: 5
2024-11-06 16:31:24,394:INFO:_display_container: 2
2024-11-06 16:31:24,395:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=5881, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-06 16:31:24,395:INFO:create_model() successfully completed......................................
2024-11-06 16:31:24,575:INFO:SubProcess create_model() end ==================================
2024-11-06 16:31:24,575:INFO:Creating metrics dataframe
2024-11-06 16:31:24,580:INFO:Initializing Ridge Classifier
2024-11-06 16:31:24,581:INFO:Total runtime is 0.27878247896830244 minutes
2024-11-06 16:31:24,583:INFO:SubProcess create_model() called ==================================
2024-11-06 16:31:24,583:INFO:Initializing create_model()
2024-11-06 16:31:24,583:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc1fdc3610>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc91132f90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:31:24,583:INFO:Checking exceptions
2024-11-06 16:31:24,583:INFO:Importing libraries
2024-11-06 16:31:24,583:INFO:Copying training dataset
2024-11-06 16:31:24,747:INFO:Defining folds
2024-11-06 16:31:24,748:INFO:Declaring metric variables
2024-11-06 16:31:24,750:INFO:Importing untrained model
2024-11-06 16:31:24,753:INFO:Ridge Classifier Imported successfully
2024-11-06 16:31:24,757:INFO:Starting cross validation
2024-11-06 16:31:24,761:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:31:24,979:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:31:25,008:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:31:25,081:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:31:25,112:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:31:25,124:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:25,125:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:25,226:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:31:25,234:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:25,247:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:25,267:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:31:25,293:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:31:25,345:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:25,347:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:31:25,360:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:31:25,405:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:31:25,408:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:25,411:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:25,457:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:25,472:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:25,503:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:25,516:INFO:Calculating mean and std
2024-11-06 16:31:25,517:INFO:Creating metrics dataframe
2024-11-06 16:31:25,518:INFO:Uploading results into container
2024-11-06 16:31:25,519:INFO:Uploading model into container now
2024-11-06 16:31:25,519:INFO:_master_model_container: 6
2024-11-06 16:31:25,519:INFO:_display_container: 2
2024-11-06 16:31:25,519:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=5881, solver='auto',
                tol=0.0001)
2024-11-06 16:31:25,519:INFO:create_model() successfully completed......................................
2024-11-06 16:31:25,700:INFO:SubProcess create_model() end ==================================
2024-11-06 16:31:25,700:INFO:Creating metrics dataframe
2024-11-06 16:31:25,706:INFO:Initializing Random Forest Classifier
2024-11-06 16:31:25,706:INFO:Total runtime is 0.29753990173339845 minutes
2024-11-06 16:31:25,709:INFO:SubProcess create_model() called ==================================
2024-11-06 16:31:25,710:INFO:Initializing create_model()
2024-11-06 16:31:25,710:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc1fdc3610>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc91132f90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:31:25,710:INFO:Checking exceptions
2024-11-06 16:31:25,710:INFO:Importing libraries
2024-11-06 16:31:25,710:INFO:Copying training dataset
2024-11-06 16:31:25,882:INFO:Defining folds
2024-11-06 16:31:25,882:INFO:Declaring metric variables
2024-11-06 16:31:25,885:INFO:Importing untrained model
2024-11-06 16:31:25,888:INFO:Random Forest Classifier Imported successfully
2024-11-06 16:31:25,894:INFO:Starting cross validation
2024-11-06 16:31:25,898:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:31:26,911:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:26,968:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:27,005:INFO:Calculating mean and std
2024-11-06 16:31:27,006:INFO:Creating metrics dataframe
2024-11-06 16:31:27,007:INFO:Uploading results into container
2024-11-06 16:31:27,007:INFO:Uploading model into container now
2024-11-06 16:31:27,008:INFO:_master_model_container: 7
2024-11-06 16:31:27,008:INFO:_display_container: 2
2024-11-06 16:31:27,008:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=5881, verbose=0,
                       warm_start=False)
2024-11-06 16:31:27,008:INFO:create_model() successfully completed......................................
2024-11-06 16:31:27,190:INFO:SubProcess create_model() end ==================================
2024-11-06 16:31:27,190:INFO:Creating metrics dataframe
2024-11-06 16:31:27,197:INFO:Initializing Quadratic Discriminant Analysis
2024-11-06 16:31:27,197:INFO:Total runtime is 0.3223969618479411 minutes
2024-11-06 16:31:27,200:INFO:SubProcess create_model() called ==================================
2024-11-06 16:31:27,200:INFO:Initializing create_model()
2024-11-06 16:31:27,200:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc1fdc3610>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc91132f90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:31:27,200:INFO:Checking exceptions
2024-11-06 16:31:27,200:INFO:Importing libraries
2024-11-06 16:31:27,200:INFO:Copying training dataset
2024-11-06 16:31:27,361:INFO:Defining folds
2024-11-06 16:31:27,361:INFO:Declaring metric variables
2024-11-06 16:31:27,363:INFO:Importing untrained model
2024-11-06 16:31:27,366:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-06 16:31:27,371:INFO:Starting cross validation
2024-11-06 16:31:27,374:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:31:27,656:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:31:27,683:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:31:27,718:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:31:27,736:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:31:27,770:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:31:27,818:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:31:27,836:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:27,840:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:27,851:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:27,880:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:27,895:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:27,898:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:27,912:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:31:27,924:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:27,927:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:27,956:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:27,958:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:31:27,959:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:27,968:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:31:28,011:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:31:28,060:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:28,062:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:28,083:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:28,086:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:28,086:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:28,120:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:28,132:INFO:Calculating mean and std
2024-11-06 16:31:28,134:INFO:Creating metrics dataframe
2024-11-06 16:31:28,137:INFO:Uploading results into container
2024-11-06 16:31:28,137:INFO:Uploading model into container now
2024-11-06 16:31:28,138:INFO:_master_model_container: 8
2024-11-06 16:31:28,139:INFO:_display_container: 2
2024-11-06 16:31:28,139:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-06 16:31:28,139:INFO:create_model() successfully completed......................................
2024-11-06 16:31:28,322:INFO:SubProcess create_model() end ==================================
2024-11-06 16:31:28,322:INFO:Creating metrics dataframe
2024-11-06 16:31:28,328:INFO:Initializing Ada Boost Classifier
2024-11-06 16:31:28,328:INFO:Total runtime is 0.3412410298983256 minutes
2024-11-06 16:31:28,331:INFO:SubProcess create_model() called ==================================
2024-11-06 16:31:28,331:INFO:Initializing create_model()
2024-11-06 16:31:28,331:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc1fdc3610>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc91132f90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:31:28,331:INFO:Checking exceptions
2024-11-06 16:31:28,331:INFO:Importing libraries
2024-11-06 16:31:28,331:INFO:Copying training dataset
2024-11-06 16:31:28,506:INFO:Defining folds
2024-11-06 16:31:28,507:INFO:Declaring metric variables
2024-11-06 16:31:28,509:INFO:Importing untrained model
2024-11-06 16:31:28,512:INFO:Ada Boost Classifier Imported successfully
2024-11-06 16:31:28,516:INFO:Starting cross validation
2024-11-06 16:31:28,520:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:31:28,736:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:31:28,779:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:31:28,857:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:31:28,902:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:31:28,960:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:31:29,006:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:31:29,008:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:31:29,146:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:31:29,151:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:31:29,181:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:31:30,828:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:30,833:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:31:30,919:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:31,004:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:31,039:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:31,114:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:31,164:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:31,175:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:31,178:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:31,245:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:31,273:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:31:31,286:INFO:Calculating mean and std
2024-11-06 16:31:31,286:INFO:Creating metrics dataframe
2024-11-06 16:31:31,288:INFO:Uploading results into container
2024-11-06 16:31:31,289:INFO:Uploading model into container now
2024-11-06 16:31:31,289:INFO:_master_model_container: 9
2024-11-06 16:31:31,289:INFO:_display_container: 2
2024-11-06 16:31:31,289:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=5881)
2024-11-06 16:31:31,289:INFO:create_model() successfully completed......................................
2024-11-06 16:31:31,503:INFO:SubProcess create_model() end ==================================
2024-11-06 16:31:31,503:INFO:Creating metrics dataframe
2024-11-06 16:31:31,508:INFO:Initializing Gradient Boosting Classifier
2024-11-06 16:31:31,508:INFO:Total runtime is 0.3942458430926005 minutes
2024-11-06 16:31:31,511:INFO:SubProcess create_model() called ==================================
2024-11-06 16:31:31,511:INFO:Initializing create_model()
2024-11-06 16:31:31,511:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc1fdc3610>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc91132f90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:31:31,511:INFO:Checking exceptions
2024-11-06 16:31:31,511:INFO:Importing libraries
2024-11-06 16:31:31,511:INFO:Copying training dataset
2024-11-06 16:31:31,685:INFO:Defining folds
2024-11-06 16:31:31,686:INFO:Declaring metric variables
2024-11-06 16:31:31,688:INFO:Importing untrained model
2024-11-06 16:31:31,691:INFO:Gradient Boosting Classifier Imported successfully
2024-11-06 16:31:31,698:INFO:Starting cross validation
2024-11-06 16:31:31,702:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:32:09,096:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:09,606:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:09,955:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:10,629:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:10,742:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:11,233:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:11,337:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:11,421:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:12,214:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:12,933:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:12,958:INFO:Calculating mean and std
2024-11-06 16:32:12,959:INFO:Creating metrics dataframe
2024-11-06 16:32:12,961:INFO:Uploading results into container
2024-11-06 16:32:12,961:INFO:Uploading model into container now
2024-11-06 16:32:12,962:INFO:_master_model_container: 10
2024-11-06 16:32:12,962:INFO:_display_container: 2
2024-11-06 16:32:12,963:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=5881, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-06 16:32:12,963:INFO:create_model() successfully completed......................................
2024-11-06 16:32:13,166:INFO:SubProcess create_model() end ==================================
2024-11-06 16:32:13,166:INFO:Creating metrics dataframe
2024-11-06 16:32:13,171:INFO:Initializing Linear Discriminant Analysis
2024-11-06 16:32:13,172:INFO:Total runtime is 1.0886332352956136 minutes
2024-11-06 16:32:13,174:INFO:SubProcess create_model() called ==================================
2024-11-06 16:32:13,174:INFO:Initializing create_model()
2024-11-06 16:32:13,174:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc1fdc3610>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc91132f90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:32:13,174:INFO:Checking exceptions
2024-11-06 16:32:13,174:INFO:Importing libraries
2024-11-06 16:32:13,174:INFO:Copying training dataset
2024-11-06 16:32:13,347:INFO:Defining folds
2024-11-06 16:32:13,347:INFO:Declaring metric variables
2024-11-06 16:32:13,350:INFO:Importing untrained model
2024-11-06 16:32:13,353:INFO:Linear Discriminant Analysis Imported successfully
2024-11-06 16:32:13,358:INFO:Starting cross validation
2024-11-06 16:32:13,362:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:32:13,883:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:14,110:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:14,141:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:14,143:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:14,251:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:14,256:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:14,275:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:14,282:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:14,354:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:14,359:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:32:14,425:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:32:14,446:INFO:Calculating mean and std
2024-11-06 16:32:14,447:INFO:Creating metrics dataframe
2024-11-06 16:32:14,449:INFO:Uploading results into container
2024-11-06 16:32:14,450:INFO:Uploading model into container now
2024-11-06 16:32:14,450:INFO:_master_model_container: 11
2024-11-06 16:32:14,450:INFO:_display_container: 2
2024-11-06 16:32:14,451:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-06 16:32:14,451:INFO:create_model() successfully completed......................................
2024-11-06 16:32:14,633:INFO:SubProcess create_model() end ==================================
2024-11-06 16:32:14,634:INFO:Creating metrics dataframe
2024-11-06 16:32:14,640:INFO:Initializing Extra Trees Classifier
2024-11-06 16:32:14,640:INFO:Total runtime is 1.1131050984064739 minutes
2024-11-06 16:32:14,642:INFO:SubProcess create_model() called ==================================
2024-11-06 16:32:14,642:INFO:Initializing create_model()
2024-11-06 16:32:14,642:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc1fdc3610>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc91132f90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:32:14,642:INFO:Checking exceptions
2024-11-06 16:32:14,642:INFO:Importing libraries
2024-11-06 16:32:14,642:INFO:Copying training dataset
2024-11-06 16:32:14,813:INFO:Defining folds
2024-11-06 16:32:14,814:INFO:Declaring metric variables
2024-11-06 16:32:14,816:INFO:Importing untrained model
2024-11-06 16:32:14,820:INFO:Extra Trees Classifier Imported successfully
2024-11-06 16:32:14,825:INFO:Starting cross validation
2024-11-06 16:32:14,829:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:32:15,865:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:32:15,897:INFO:Calculating mean and std
2024-11-06 16:32:15,898:INFO:Creating metrics dataframe
2024-11-06 16:32:15,900:INFO:Uploading results into container
2024-11-06 16:32:15,900:INFO:Uploading model into container now
2024-11-06 16:32:15,901:INFO:_master_model_container: 12
2024-11-06 16:32:15,901:INFO:_display_container: 2
2024-11-06 16:32:15,902:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=5881, verbose=0,
                     warm_start=False)
2024-11-06 16:32:15,902:INFO:create_model() successfully completed......................................
2024-11-06 16:32:16,116:INFO:SubProcess create_model() end ==================================
2024-11-06 16:32:16,116:INFO:Creating metrics dataframe
2024-11-06 16:32:16,123:INFO:Initializing Extreme Gradient Boosting
2024-11-06 16:32:16,124:INFO:Total runtime is 1.1378355820973716 minutes
2024-11-06 16:32:16,126:INFO:SubProcess create_model() called ==================================
2024-11-06 16:32:16,126:INFO:Initializing create_model()
2024-11-06 16:32:16,127:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc1fdc3610>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc91132f90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:32:16,127:INFO:Checking exceptions
2024-11-06 16:32:16,127:INFO:Importing libraries
2024-11-06 16:32:16,127:INFO:Copying training dataset
2024-11-06 16:32:16,324:INFO:Defining folds
2024-11-06 16:32:16,324:INFO:Declaring metric variables
2024-11-06 16:32:16,327:INFO:Importing untrained model
2024-11-06 16:32:16,331:INFO:Extreme Gradient Boosting Imported successfully
2024-11-06 16:32:16,336:INFO:Starting cross validation
2024-11-06 16:32:16,340:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:32:42,375:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:32:43,105:INFO:Calculating mean and std
2024-11-06 16:32:43,107:INFO:Creating metrics dataframe
2024-11-06 16:32:43,110:INFO:Uploading results into container
2024-11-06 16:32:43,111:INFO:Uploading model into container now
2024-11-06 16:32:43,112:INFO:_master_model_container: 13
2024-11-06 16:32:43,112:INFO:_display_container: 2
2024-11-06 16:32:43,113:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-11-06 16:32:43,114:INFO:create_model() successfully completed......................................
2024-11-06 16:32:43,299:INFO:SubProcess create_model() end ==================================
2024-11-06 16:32:43,299:INFO:Creating metrics dataframe
2024-11-06 16:32:43,306:INFO:Initializing Light Gradient Boosting Machine
2024-11-06 16:32:43,306:INFO:Total runtime is 1.5908672690391543 minutes
2024-11-06 16:32:43,308:INFO:SubProcess create_model() called ==================================
2024-11-06 16:32:43,308:INFO:Initializing create_model()
2024-11-06 16:32:43,308:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc1fdc3610>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc91132f90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:32:43,308:INFO:Checking exceptions
2024-11-06 16:32:43,308:INFO:Importing libraries
2024-11-06 16:32:43,308:INFO:Copying training dataset
2024-11-06 16:32:43,469:INFO:Defining folds
2024-11-06 16:32:43,469:INFO:Declaring metric variables
2024-11-06 16:32:43,472:INFO:Importing untrained model
2024-11-06 16:32:43,474:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-06 16:32:43,479:INFO:Starting cross validation
2024-11-06 16:32:43,483:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:33:10,701:INFO:Calculating mean and std
2024-11-06 16:33:10,702:INFO:Creating metrics dataframe
2024-11-06 16:33:10,705:INFO:Uploading results into container
2024-11-06 16:33:10,706:INFO:Uploading model into container now
2024-11-06 16:33:10,706:INFO:_master_model_container: 14
2024-11-06 16:33:10,706:INFO:_display_container: 2
2024-11-06 16:33:10,706:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5881, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-06 16:33:10,706:INFO:create_model() successfully completed......................................
2024-11-06 16:33:10,912:INFO:SubProcess create_model() end ==================================
2024-11-06 16:33:10,912:INFO:Creating metrics dataframe
2024-11-06 16:33:10,919:INFO:Initializing Dummy Classifier
2024-11-06 16:33:10,919:INFO:Total runtime is 2.0510937531789146 minutes
2024-11-06 16:33:10,921:INFO:SubProcess create_model() called ==================================
2024-11-06 16:33:10,922:INFO:Initializing create_model()
2024-11-06 16:33:10,922:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc1fdc3610>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x79cc91132f90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:33:10,922:INFO:Checking exceptions
2024-11-06 16:33:10,922:INFO:Importing libraries
2024-11-06 16:33:10,922:INFO:Copying training dataset
2024-11-06 16:33:11,093:INFO:Defining folds
2024-11-06 16:33:11,093:INFO:Declaring metric variables
2024-11-06 16:33:11,096:INFO:Importing untrained model
2024-11-06 16:33:11,099:INFO:Dummy Classifier Imported successfully
2024-11-06 16:33:11,105:INFO:Starting cross validation
2024-11-06 16:33:11,110:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:33:11,389:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:33:11,507:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:33:11,521:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:33:11,576:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:33:11,632:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:33:11,678:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:33:11,695:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:33:11,726:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:33:11,782:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:33:11,813:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:33:11,822:INFO:Calculating mean and std
2024-11-06 16:33:11,823:INFO:Creating metrics dataframe
2024-11-06 16:33:11,826:INFO:Uploading results into container
2024-11-06 16:33:11,827:INFO:Uploading model into container now
2024-11-06 16:33:11,828:INFO:_master_model_container: 15
2024-11-06 16:33:11,828:INFO:_display_container: 2
2024-11-06 16:33:11,828:INFO:DummyClassifier(constant=None, random_state=5881, strategy='prior')
2024-11-06 16:33:11,829:INFO:create_model() successfully completed......................................
2024-11-06 16:33:12,013:INFO:SubProcess create_model() end ==================================
2024-11-06 16:33:12,013:INFO:Creating metrics dataframe
2024-11-06 16:33:12,020:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-06 16:33:12,028:INFO:Initializing create_model()
2024-11-06 16:33:12,028:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x79cc1fdc3610>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=5881, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:33:12,028:INFO:Checking exceptions
2024-11-06 16:33:12,030:INFO:Importing libraries
2024-11-06 16:33:12,030:INFO:Copying training dataset
2024-11-06 16:33:12,193:INFO:Defining folds
2024-11-06 16:33:12,193:INFO:Declaring metric variables
2024-11-06 16:33:12,193:INFO:Importing untrained model
2024-11-06 16:33:12,193:INFO:Declaring custom model
2024-11-06 16:33:12,194:INFO:Random Forest Classifier Imported successfully
2024-11-06 16:33:12,197:INFO:Cross validation set to False
2024-11-06 16:33:12,197:INFO:Fitting Model
2024-11-06 16:33:12,485:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=5881, verbose=0,
                       warm_start=False)
2024-11-06 16:33:12,485:INFO:create_model() successfully completed......................................
2024-11-06 16:33:12,692:INFO:_master_model_container: 15
2024-11-06 16:33:12,693:INFO:_display_container: 2
2024-11-06 16:33:12,693:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=5881, verbose=0,
                       warm_start=False)
2024-11-06 16:33:12,693:INFO:compare_models() successfully completed......................................
2024-11-06 16:38:11,231:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 16:38:11,231:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 16:38:11,231:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 16:38:11,231:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-06 16:38:12,070:INFO:PyCaret ClassificationExperiment
2024-11-06 16:38:12,071:INFO:Logging name: clf-default-name
2024-11-06 16:38:12,071:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-06 16:38:12,071:INFO:version 3.3.2
2024-11-06 16:38:12,071:INFO:Initializing setup()
2024-11-06 16:38:12,071:INFO:self.USI: 0844
2024-11-06 16:38:12,071:INFO:self._variable_keys: {'X', 'gpu_param', 'idx', 'fold_generator', 'pipeline', 'html_param', 'data', 'memory', 'USI', 'logging_param', 'X_test', 'n_jobs_param', 'fold_groups_param', 'seed', 'gpu_n_jobs_param', 'X_train', 'y_train', 'y', '_available_plots', '_ml_usecase', 'exp_name_log', 'target_param', 'is_multiclass', 'log_plots_param', 'exp_id', 'fold_shuffle_param', 'y_test', 'fix_imbalance'}
2024-11-06 16:38:12,071:INFO:Checking environment
2024-11-06 16:38:12,071:INFO:python_version: 3.11.10
2024-11-06 16:38:12,071:INFO:python_build: ('main', 'Oct  3 2024 07:29:13')
2024-11-06 16:38:12,071:INFO:machine: x86_64
2024-11-06 16:38:12,071:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 16:38:12,071:INFO:Memory: svmem(total=33037213696, available=23394082816, percent=29.2, used=8905527296, free=10643595264, active=9852420096, inactive=9023516672, buffers=869457920, cached=12618633216, shared=264220672, slab=2031407104)
2024-11-06 16:38:12,072:INFO:Physical Core: 8
2024-11-06 16:38:12,072:INFO:Logical Core: 16
2024-11-06 16:38:12,072:INFO:Checking libraries
2024-11-06 16:38:12,072:INFO:System:
2024-11-06 16:38:12,072:INFO:    python: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0]
2024-11-06 16:38:12,072:INFO:executable: /home/cid34senhas/miniconda3/envs/myenv/bin/python
2024-11-06 16:38:12,072:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 16:38:12,072:INFO:PyCaret required dependencies:
2024-11-06 16:38:12,100:INFO:                 pip: 24.2
2024-11-06 16:38:12,100:INFO:          setuptools: 75.1.0
2024-11-06 16:38:12,100:INFO:             pycaret: 3.3.2
2024-11-06 16:38:12,100:INFO:             IPython: 8.29.0
2024-11-06 16:38:12,100:INFO:          ipywidgets: 8.1.5
2024-11-06 16:38:12,100:INFO:                tqdm: 4.66.6
2024-11-06 16:38:12,100:INFO:               numpy: 1.26.4
2024-11-06 16:38:12,101:INFO:              pandas: 2.1.4
2024-11-06 16:38:12,101:INFO:              jinja2: 3.1.4
2024-11-06 16:38:12,101:INFO:               scipy: 1.11.4
2024-11-06 16:38:12,101:INFO:              joblib: 1.3.2
2024-11-06 16:38:12,101:INFO:             sklearn: 1.4.2
2024-11-06 16:38:12,101:INFO:                pyod: 2.0.2
2024-11-06 16:38:12,101:INFO:            imblearn: 0.12.4
2024-11-06 16:38:12,101:INFO:   category_encoders: 2.6.4
2024-11-06 16:38:12,101:INFO:            lightgbm: 4.5.0
2024-11-06 16:38:12,101:INFO:               numba: 0.60.0
2024-11-06 16:38:12,101:INFO:            requests: 2.32.3
2024-11-06 16:38:12,101:INFO:          matplotlib: 3.7.5
2024-11-06 16:38:12,101:INFO:          scikitplot: 0.3.7
2024-11-06 16:38:12,101:INFO:         yellowbrick: 1.5
2024-11-06 16:38:12,101:INFO:              plotly: 5.24.1
2024-11-06 16:38:12,101:INFO:    plotly-resampler: Not installed
2024-11-06 16:38:12,101:INFO:             kaleido: 0.2.1
2024-11-06 16:38:12,101:INFO:           schemdraw: 0.15
2024-11-06 16:38:12,101:INFO:         statsmodels: 0.14.4
2024-11-06 16:38:12,101:INFO:              sktime: 0.26.0
2024-11-06 16:38:12,101:INFO:               tbats: 1.1.3
2024-11-06 16:38:12,101:INFO:            pmdarima: 2.0.4
2024-11-06 16:38:12,101:INFO:              psutil: 6.1.0
2024-11-06 16:38:12,101:INFO:          markupsafe: 3.0.2
2024-11-06 16:38:12,101:INFO:             pickle5: Not installed
2024-11-06 16:38:12,101:INFO:         cloudpickle: 3.1.0
2024-11-06 16:38:12,101:INFO:         deprecation: 2.1.0
2024-11-06 16:38:12,101:INFO:              xxhash: 3.5.0
2024-11-06 16:38:12,101:INFO:           wurlitzer: 3.1.1
2024-11-06 16:38:12,101:INFO:PyCaret optional dependencies:
2024-11-06 16:38:12,121:INFO:                shap: Not installed
2024-11-06 16:38:12,121:INFO:           interpret: Not installed
2024-11-06 16:38:12,121:INFO:                umap: Not installed
2024-11-06 16:38:12,121:INFO:     ydata_profiling: Not installed
2024-11-06 16:38:12,121:INFO:  explainerdashboard: Not installed
2024-11-06 16:38:12,121:INFO:             autoviz: Not installed
2024-11-06 16:38:12,121:INFO:           fairlearn: Not installed
2024-11-06 16:38:12,121:INFO:          deepchecks: Not installed
2024-11-06 16:38:12,121:INFO:             xgboost: 2.1.2
2024-11-06 16:38:12,121:INFO:            catboost: Not installed
2024-11-06 16:38:12,121:INFO:              kmodes: Not installed
2024-11-06 16:38:12,121:INFO:             mlxtend: Not installed
2024-11-06 16:38:12,121:INFO:       statsforecast: Not installed
2024-11-06 16:38:12,121:INFO:        tune_sklearn: Not installed
2024-11-06 16:38:12,121:INFO:                 ray: Not installed
2024-11-06 16:38:12,121:INFO:            hyperopt: Not installed
2024-11-06 16:38:12,121:INFO:              optuna: Not installed
2024-11-06 16:38:12,121:INFO:               skopt: Not installed
2024-11-06 16:38:12,121:INFO:              mlflow: Not installed
2024-11-06 16:38:12,121:INFO:              gradio: Not installed
2024-11-06 16:38:12,121:INFO:             fastapi: Not installed
2024-11-06 16:38:12,121:INFO:             uvicorn: Not installed
2024-11-06 16:38:12,121:INFO:              m2cgen: Not installed
2024-11-06 16:38:12,121:INFO:           evidently: Not installed
2024-11-06 16:38:12,121:INFO:               fugue: Not installed
2024-11-06 16:38:12,121:INFO:           streamlit: Not installed
2024-11-06 16:38:12,121:INFO:             prophet: Not installed
2024-11-06 16:38:12,121:INFO:None
2024-11-06 16:38:12,121:INFO:Set up data.
2024-11-06 16:38:12,310:INFO:Set up folding strategy.
2024-11-06 16:38:12,310:INFO:Set up train/test split.
2024-11-06 16:38:12,566:INFO:Set up index.
2024-11-06 16:38:12,570:INFO:Assigning column types.
2024-11-06 16:38:12,784:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-06 16:38:12,819:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 16:38:12,821:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:38:12,845:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:38:12,847:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:38:12,882:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 16:38:12,883:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:38:12,904:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:38:12,906:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:38:12,907:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-06 16:38:12,942:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:38:12,963:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:38:12,966:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:38:13,001:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:38:13,022:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:38:13,024:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:38:13,024:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-06 16:38:13,082:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:38:13,084:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:38:13,140:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:38:13,142:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:38:13,143:INFO:Preparing preprocessing pipeline...
2024-11-06 16:38:13,165:INFO:Set up simple imputation.
2024-11-06 16:38:13,186:INFO:Set up column name cleaning.
2024-11-06 16:38:14,393:INFO:Finished creating preprocessing pipeline.
2024-11-06 16:38:14,402:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['diagnostics_Image-original_Mean',
                                             'diagnostics_Image-original_Maximum',
                                             'diagnostics_Mask-original_VoxelNum',
                                             'original_shape_Elongation',
                                             'original_shape_Flatness',
                                             'original_shape_LeastAxisLength',
                                             'original_shape_MajorAxisLength',
                                             'ori...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-06 16:38:14,403:INFO:Creating final display dataframe.
2024-11-06 16:38:16,940:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pipeline.py:289: UserWarning: Persisting input arguments took 0.53s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-11-06 16:38:17,748:INFO:Setup _display_container:                     Description             Value
0                    Session id               605
1                        Target        Transition
2                   Target type        Multiclass
3           Original data shape       (295, 2014)
4        Transformed data shape       (295, 2014)
5   Transformed train set shape       (206, 2014)
6    Transformed test set shape        (89, 2014)
7              Numeric features              2013
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              0844
2024-11-06 16:38:17,810:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:38:17,812:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:38:17,868:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:38:17,871:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:38:17,872:INFO:setup() successfully completed in 5.81s...............
2024-11-06 16:38:17,898:INFO:Initializing compare_models()
2024-11-06 16:38:17,899:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea83bf510>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea83bf510>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-06 16:38:17,899:INFO:Checking exceptions
2024-11-06 16:38:18,076:INFO:Preparing display monitor
2024-11-06 16:38:18,095:INFO:Initializing Logistic Regression
2024-11-06 16:38:18,096:INFO:Total runtime is 4.220008850097656e-06 minutes
2024-11-06 16:38:18,099:INFO:SubProcess create_model() called ==================================
2024-11-06 16:38:18,099:INFO:Initializing create_model()
2024-11-06 16:38:18,099:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea83bf510>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0ea3382b50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:38:18,099:INFO:Checking exceptions
2024-11-06 16:38:18,099:INFO:Importing libraries
2024-11-06 16:38:18,099:INFO:Copying training dataset
2024-11-06 16:38:18,339:INFO:Defining folds
2024-11-06 16:38:18,339:INFO:Declaring metric variables
2024-11-06 16:38:18,341:INFO:Importing untrained model
2024-11-06 16:38:18,344:INFO:Logistic Regression Imported successfully
2024-11-06 16:38:18,348:INFO:Starting cross validation
2024-11-06 16:38:18,352:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:38:26,027:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:38:26,102:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:26,216:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:38:26,287:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:26,292:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:26,303:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:38:26,368:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:26,452:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:38:26,508:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:26,571:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:38:26,617:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:38:26,624:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:26,667:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:26,820:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:38:26,867:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:26,982:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:38:27,025:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:27,148:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:38:27,159:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:38:27,187:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:27,197:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:27,215:INFO:Calculating mean and std
2024-11-06 16:38:27,218:INFO:Creating metrics dataframe
2024-11-06 16:38:27,222:INFO:Uploading results into container
2024-11-06 16:38:27,223:INFO:Uploading model into container now
2024-11-06 16:38:27,224:INFO:_master_model_container: 1
2024-11-06 16:38:27,225:INFO:_display_container: 2
2024-11-06 16:38:27,226:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=605, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-06 16:38:27,226:INFO:create_model() successfully completed......................................
2024-11-06 16:38:27,331:INFO:SubProcess create_model() end ==================================
2024-11-06 16:38:27,331:INFO:Creating metrics dataframe
2024-11-06 16:38:27,336:INFO:Initializing K Neighbors Classifier
2024-11-06 16:38:27,336:INFO:Total runtime is 0.15400840044021605 minutes
2024-11-06 16:38:27,339:INFO:SubProcess create_model() called ==================================
2024-11-06 16:38:27,339:INFO:Initializing create_model()
2024-11-06 16:38:27,339:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea83bf510>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0ea3382b50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:38:27,339:INFO:Checking exceptions
2024-11-06 16:38:27,339:INFO:Importing libraries
2024-11-06 16:38:27,339:INFO:Copying training dataset
2024-11-06 16:38:27,576:INFO:Defining folds
2024-11-06 16:38:27,576:INFO:Declaring metric variables
2024-11-06 16:38:27,578:INFO:Importing untrained model
2024-11-06 16:38:27,582:INFO:K Neighbors Classifier Imported successfully
2024-11-06 16:38:27,588:INFO:Starting cross validation
2024-11-06 16:38:27,593:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:38:28,285:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:29,287:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:29,312:INFO:Calculating mean and std
2024-11-06 16:38:29,314:INFO:Creating metrics dataframe
2024-11-06 16:38:29,316:INFO:Uploading results into container
2024-11-06 16:38:29,318:INFO:Uploading model into container now
2024-11-06 16:38:29,319:INFO:_master_model_container: 2
2024-11-06 16:38:29,319:INFO:_display_container: 2
2024-11-06 16:38:29,320:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-06 16:38:29,320:INFO:create_model() successfully completed......................................
2024-11-06 16:38:29,410:INFO:SubProcess create_model() end ==================================
2024-11-06 16:38:29,410:INFO:Creating metrics dataframe
2024-11-06 16:38:29,414:INFO:Initializing Naive Bayes
2024-11-06 16:38:29,414:INFO:Total runtime is 0.18865112463633218 minutes
2024-11-06 16:38:29,417:INFO:SubProcess create_model() called ==================================
2024-11-06 16:38:29,417:INFO:Initializing create_model()
2024-11-06 16:38:29,417:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea83bf510>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0ea3382b50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:38:29,417:INFO:Checking exceptions
2024-11-06 16:38:29,417:INFO:Importing libraries
2024-11-06 16:38:29,417:INFO:Copying training dataset
2024-11-06 16:38:29,645:INFO:Defining folds
2024-11-06 16:38:29,646:INFO:Declaring metric variables
2024-11-06 16:38:29,648:INFO:Importing untrained model
2024-11-06 16:38:29,651:INFO:Naive Bayes Imported successfully
2024-11-06 16:38:29,657:INFO:Starting cross validation
2024-11-06 16:38:29,660:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:38:30,361:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:30,368:INFO:Calculating mean and std
2024-11-06 16:38:30,369:INFO:Creating metrics dataframe
2024-11-06 16:38:30,373:INFO:Uploading results into container
2024-11-06 16:38:30,373:INFO:Uploading model into container now
2024-11-06 16:38:30,374:INFO:_master_model_container: 3
2024-11-06 16:38:30,374:INFO:_display_container: 2
2024-11-06 16:38:30,374:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-06 16:38:30,374:INFO:create_model() successfully completed......................................
2024-11-06 16:38:30,464:INFO:SubProcess create_model() end ==================================
2024-11-06 16:38:30,464:INFO:Creating metrics dataframe
2024-11-06 16:38:30,470:INFO:Initializing Decision Tree Classifier
2024-11-06 16:38:30,470:INFO:Total runtime is 0.20624874432881674 minutes
2024-11-06 16:38:30,473:INFO:SubProcess create_model() called ==================================
2024-11-06 16:38:30,473:INFO:Initializing create_model()
2024-11-06 16:38:30,473:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea83bf510>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0ea3382b50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:38:30,473:INFO:Checking exceptions
2024-11-06 16:38:30,473:INFO:Importing libraries
2024-11-06 16:38:30,473:INFO:Copying training dataset
2024-11-06 16:38:30,692:INFO:Defining folds
2024-11-06 16:38:30,692:INFO:Declaring metric variables
2024-11-06 16:38:30,695:INFO:Importing untrained model
2024-11-06 16:38:30,699:INFO:Decision Tree Classifier Imported successfully
2024-11-06 16:38:30,704:INFO:Starting cross validation
2024-11-06 16:38:30,707:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:38:31,324:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:31,568:INFO:Calculating mean and std
2024-11-06 16:38:31,569:INFO:Creating metrics dataframe
2024-11-06 16:38:31,572:INFO:Uploading results into container
2024-11-06 16:38:31,573:INFO:Uploading model into container now
2024-11-06 16:38:31,573:INFO:_master_model_container: 4
2024-11-06 16:38:31,574:INFO:_display_container: 2
2024-11-06 16:38:31,574:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=605, splitter='best')
2024-11-06 16:38:31,575:INFO:create_model() successfully completed......................................
2024-11-06 16:38:31,650:INFO:SubProcess create_model() end ==================================
2024-11-06 16:38:31,650:INFO:Creating metrics dataframe
2024-11-06 16:38:31,655:INFO:Initializing SVM - Linear Kernel
2024-11-06 16:38:31,656:INFO:Total runtime is 0.22600951194763183 minutes
2024-11-06 16:38:31,659:INFO:SubProcess create_model() called ==================================
2024-11-06 16:38:31,660:INFO:Initializing create_model()
2024-11-06 16:38:31,660:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea83bf510>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0ea3382b50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:38:31,660:INFO:Checking exceptions
2024-11-06 16:38:31,660:INFO:Importing libraries
2024-11-06 16:38:31,660:INFO:Copying training dataset
2024-11-06 16:38:31,886:INFO:Defining folds
2024-11-06 16:38:31,886:INFO:Declaring metric variables
2024-11-06 16:38:31,888:INFO:Importing untrained model
2024-11-06 16:38:31,890:INFO:SVM - Linear Kernel Imported successfully
2024-11-06 16:38:31,894:INFO:Starting cross validation
2024-11-06 16:38:31,898:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:38:32,373:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:32,379:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:32,382:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:32,387:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:32,488:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:32,492:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:32,500:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:32,502:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:32,503:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:32,506:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:32,571:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:32,573:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:32,584:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:32,588:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:32,602:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:32,606:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:32,674:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:32,677:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:32,711:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:32,714:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:32,724:INFO:Calculating mean and std
2024-11-06 16:38:32,726:INFO:Creating metrics dataframe
2024-11-06 16:38:32,727:INFO:Uploading results into container
2024-11-06 16:38:32,728:INFO:Uploading model into container now
2024-11-06 16:38:32,728:INFO:_master_model_container: 5
2024-11-06 16:38:32,728:INFO:_display_container: 2
2024-11-06 16:38:32,728:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=605, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-06 16:38:32,728:INFO:create_model() successfully completed......................................
2024-11-06 16:38:32,807:INFO:SubProcess create_model() end ==================================
2024-11-06 16:38:32,807:INFO:Creating metrics dataframe
2024-11-06 16:38:32,813:INFO:Initializing Ridge Classifier
2024-11-06 16:38:32,813:INFO:Total runtime is 0.24530392090479533 minutes
2024-11-06 16:38:32,816:INFO:SubProcess create_model() called ==================================
2024-11-06 16:38:32,816:INFO:Initializing create_model()
2024-11-06 16:38:32,817:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea83bf510>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0ea3382b50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:38:32,817:INFO:Checking exceptions
2024-11-06 16:38:32,817:INFO:Importing libraries
2024-11-06 16:38:32,817:INFO:Copying training dataset
2024-11-06 16:38:33,041:INFO:Defining folds
2024-11-06 16:38:33,042:INFO:Declaring metric variables
2024-11-06 16:38:33,044:INFO:Importing untrained model
2024-11-06 16:38:33,047:INFO:Ridge Classifier Imported successfully
2024-11-06 16:38:33,053:INFO:Starting cross validation
2024-11-06 16:38:33,057:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:38:33,257:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:38:33,326:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:38:33,341:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:38:33,365:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:33,438:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:38:33,461:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:38:33,478:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:33,482:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:38:33,503:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:33,533:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:38:33,587:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:38:33,588:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:33,608:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:33,615:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:38:33,628:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:33,648:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:33,678:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:38:33,690:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:33,719:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:33,768:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:33,778:INFO:Calculating mean and std
2024-11-06 16:38:33,779:INFO:Creating metrics dataframe
2024-11-06 16:38:33,781:INFO:Uploading results into container
2024-11-06 16:38:33,781:INFO:Uploading model into container now
2024-11-06 16:38:33,782:INFO:_master_model_container: 6
2024-11-06 16:38:33,782:INFO:_display_container: 2
2024-11-06 16:38:33,782:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=605, solver='auto',
                tol=0.0001)
2024-11-06 16:38:33,782:INFO:create_model() successfully completed......................................
2024-11-06 16:38:33,875:INFO:SubProcess create_model() end ==================================
2024-11-06 16:38:33,875:INFO:Creating metrics dataframe
2024-11-06 16:38:33,880:INFO:Initializing Random Forest Classifier
2024-11-06 16:38:33,880:INFO:Total runtime is 0.2630751132965088 minutes
2024-11-06 16:38:33,882:INFO:SubProcess create_model() called ==================================
2024-11-06 16:38:33,882:INFO:Initializing create_model()
2024-11-06 16:38:33,882:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea83bf510>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0ea3382b50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:38:33,882:INFO:Checking exceptions
2024-11-06 16:38:33,882:INFO:Importing libraries
2024-11-06 16:38:33,882:INFO:Copying training dataset
2024-11-06 16:38:34,106:INFO:Defining folds
2024-11-06 16:38:34,106:INFO:Declaring metric variables
2024-11-06 16:38:34,109:INFO:Importing untrained model
2024-11-06 16:38:34,111:INFO:Random Forest Classifier Imported successfully
2024-11-06 16:38:34,116:INFO:Starting cross validation
2024-11-06 16:38:34,119:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:38:35,171:INFO:Calculating mean and std
2024-11-06 16:38:35,172:INFO:Creating metrics dataframe
2024-11-06 16:38:35,174:INFO:Uploading results into container
2024-11-06 16:38:35,175:INFO:Uploading model into container now
2024-11-06 16:38:35,175:INFO:_master_model_container: 7
2024-11-06 16:38:35,175:INFO:_display_container: 2
2024-11-06 16:38:35,175:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=605, verbose=0,
                       warm_start=False)
2024-11-06 16:38:35,175:INFO:create_model() successfully completed......................................
2024-11-06 16:38:35,249:INFO:SubProcess create_model() end ==================================
2024-11-06 16:38:35,249:INFO:Creating metrics dataframe
2024-11-06 16:38:35,254:INFO:Initializing Quadratic Discriminant Analysis
2024-11-06 16:38:35,254:INFO:Total runtime is 0.28597815831502277 minutes
2024-11-06 16:38:35,256:INFO:SubProcess create_model() called ==================================
2024-11-06 16:38:35,256:INFO:Initializing create_model()
2024-11-06 16:38:35,256:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea83bf510>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0ea3382b50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:38:35,257:INFO:Checking exceptions
2024-11-06 16:38:35,257:INFO:Importing libraries
2024-11-06 16:38:35,257:INFO:Copying training dataset
2024-11-06 16:38:35,494:INFO:Defining folds
2024-11-06 16:38:35,495:INFO:Declaring metric variables
2024-11-06 16:38:35,497:INFO:Importing untrained model
2024-11-06 16:38:35,501:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-06 16:38:35,507:INFO:Starting cross validation
2024-11-06 16:38:35,512:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:38:35,748:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:38:35,754:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:38:35,790:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:38:35,864:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:38:35,897:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:35,901:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:35,904:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:38:35,906:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:35,910:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:35,942:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:35,981:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:38:36,001:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:36,004:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:36,015:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:38:36,028:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:38:36,036:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:38:36,049:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:38:36,098:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:36,140:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:36,142:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:36,145:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:36,165:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:36,166:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:36,168:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:36,188:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:36,205:INFO:Calculating mean and std
2024-11-06 16:38:36,206:INFO:Creating metrics dataframe
2024-11-06 16:38:36,209:INFO:Uploading results into container
2024-11-06 16:38:36,210:INFO:Uploading model into container now
2024-11-06 16:38:36,211:INFO:_master_model_container: 8
2024-11-06 16:38:36,211:INFO:_display_container: 2
2024-11-06 16:38:36,211:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-06 16:38:36,212:INFO:create_model() successfully completed......................................
2024-11-06 16:38:36,320:INFO:SubProcess create_model() end ==================================
2024-11-06 16:38:36,320:INFO:Creating metrics dataframe
2024-11-06 16:38:36,326:INFO:Initializing Ada Boost Classifier
2024-11-06 16:38:36,326:INFO:Total runtime is 0.3038417776425679 minutes
2024-11-06 16:38:36,329:INFO:SubProcess create_model() called ==================================
2024-11-06 16:38:36,329:INFO:Initializing create_model()
2024-11-06 16:38:36,329:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea83bf510>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0ea3382b50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:38:36,329:INFO:Checking exceptions
2024-11-06 16:38:36,329:INFO:Importing libraries
2024-11-06 16:38:36,329:INFO:Copying training dataset
2024-11-06 16:38:36,563:INFO:Defining folds
2024-11-06 16:38:36,563:INFO:Declaring metric variables
2024-11-06 16:38:36,566:INFO:Importing untrained model
2024-11-06 16:38:36,570:INFO:Ada Boost Classifier Imported successfully
2024-11-06 16:38:36,575:INFO:Starting cross validation
2024-11-06 16:38:36,579:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:38:36,791:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:38:36,822:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:38:36,894:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:38:36,930:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:38:36,958:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:38:36,961:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:38:37,023:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:38:37,035:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:38:37,120:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:38:37,163:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:38:38,802:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:38,820:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:38,863:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:38,877:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:38,891:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:39,015:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:39,016:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:39,020:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:38:39,032:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:39,150:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:39,300:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:38:39,318:INFO:Calculating mean and std
2024-11-06 16:38:39,320:INFO:Creating metrics dataframe
2024-11-06 16:38:39,323:INFO:Uploading results into container
2024-11-06 16:38:39,324:INFO:Uploading model into container now
2024-11-06 16:38:39,324:INFO:_master_model_container: 9
2024-11-06 16:38:39,324:INFO:_display_container: 2
2024-11-06 16:38:39,325:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=605)
2024-11-06 16:38:39,325:INFO:create_model() successfully completed......................................
2024-11-06 16:38:39,411:INFO:SubProcess create_model() end ==================================
2024-11-06 16:38:39,412:INFO:Creating metrics dataframe
2024-11-06 16:38:39,417:INFO:Initializing Gradient Boosting Classifier
2024-11-06 16:38:39,417:INFO:Total runtime is 0.3553681890169779 minutes
2024-11-06 16:38:39,419:INFO:SubProcess create_model() called ==================================
2024-11-06 16:38:39,420:INFO:Initializing create_model()
2024-11-06 16:38:39,420:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea83bf510>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0ea3382b50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:38:39,420:INFO:Checking exceptions
2024-11-06 16:38:39,420:INFO:Importing libraries
2024-11-06 16:38:39,420:INFO:Copying training dataset
2024-11-06 16:38:39,643:INFO:Defining folds
2024-11-06 16:38:39,643:INFO:Declaring metric variables
2024-11-06 16:38:39,646:INFO:Importing untrained model
2024-11-06 16:38:39,649:INFO:Gradient Boosting Classifier Imported successfully
2024-11-06 16:38:39,654:INFO:Starting cross validation
2024-11-06 16:38:39,657:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:39:15,705:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:15,839:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:15,957:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:16,161:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:16,220:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:16,406:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:19,580:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:19,982:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:20,306:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:20,308:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:20,317:INFO:Calculating mean and std
2024-11-06 16:39:20,318:INFO:Creating metrics dataframe
2024-11-06 16:39:20,321:INFO:Uploading results into container
2024-11-06 16:39:20,321:INFO:Uploading model into container now
2024-11-06 16:39:20,322:INFO:_master_model_container: 10
2024-11-06 16:39:20,322:INFO:_display_container: 2
2024-11-06 16:39:20,323:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=605, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-06 16:39:20,323:INFO:create_model() successfully completed......................................
2024-11-06 16:39:20,409:INFO:SubProcess create_model() end ==================================
2024-11-06 16:39:20,409:INFO:Creating metrics dataframe
2024-11-06 16:39:20,416:INFO:Initializing Linear Discriminant Analysis
2024-11-06 16:39:20,416:INFO:Total runtime is 1.0386827945709227 minutes
2024-11-06 16:39:20,419:INFO:SubProcess create_model() called ==================================
2024-11-06 16:39:20,420:INFO:Initializing create_model()
2024-11-06 16:39:20,420:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea83bf510>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0ea3382b50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:39:20,420:INFO:Checking exceptions
2024-11-06 16:39:20,420:INFO:Importing libraries
2024-11-06 16:39:20,420:INFO:Copying training dataset
2024-11-06 16:39:20,658:INFO:Defining folds
2024-11-06 16:39:20,658:INFO:Declaring metric variables
2024-11-06 16:39:20,661:INFO:Importing untrained model
2024-11-06 16:39:20,666:INFO:Linear Discriminant Analysis Imported successfully
2024-11-06 16:39:20,672:INFO:Starting cross validation
2024-11-06 16:39:20,676:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:39:21,289:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:21,490:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:21,499:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:21,518:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:21,518:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:21,536:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:21,543:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:21,552:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:21,604:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:21,622:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:39:21,638:INFO:Calculating mean and std
2024-11-06 16:39:21,639:INFO:Creating metrics dataframe
2024-11-06 16:39:21,641:INFO:Uploading results into container
2024-11-06 16:39:21,641:INFO:Uploading model into container now
2024-11-06 16:39:21,641:INFO:_master_model_container: 11
2024-11-06 16:39:21,641:INFO:_display_container: 2
2024-11-06 16:39:21,641:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-06 16:39:21,641:INFO:create_model() successfully completed......................................
2024-11-06 16:39:21,716:INFO:SubProcess create_model() end ==================================
2024-11-06 16:39:21,716:INFO:Creating metrics dataframe
2024-11-06 16:39:21,722:INFO:Initializing Extra Trees Classifier
2024-11-06 16:39:21,722:INFO:Total runtime is 1.0604477842648823 minutes
2024-11-06 16:39:21,725:INFO:SubProcess create_model() called ==================================
2024-11-06 16:39:21,725:INFO:Initializing create_model()
2024-11-06 16:39:21,725:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea83bf510>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0ea3382b50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:39:21,725:INFO:Checking exceptions
2024-11-06 16:39:21,726:INFO:Importing libraries
2024-11-06 16:39:21,726:INFO:Copying training dataset
2024-11-06 16:39:21,953:INFO:Defining folds
2024-11-06 16:39:21,953:INFO:Declaring metric variables
2024-11-06 16:39:21,955:INFO:Importing untrained model
2024-11-06 16:39:21,957:INFO:Extra Trees Classifier Imported successfully
2024-11-06 16:39:21,961:INFO:Starting cross validation
2024-11-06 16:39:21,964:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:39:22,937:INFO:Calculating mean and std
2024-11-06 16:39:22,939:INFO:Creating metrics dataframe
2024-11-06 16:39:22,940:INFO:Uploading results into container
2024-11-06 16:39:22,941:INFO:Uploading model into container now
2024-11-06 16:39:22,941:INFO:_master_model_container: 12
2024-11-06 16:39:22,941:INFO:_display_container: 2
2024-11-06 16:39:22,941:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=605, verbose=0,
                     warm_start=False)
2024-11-06 16:39:22,941:INFO:create_model() successfully completed......................................
2024-11-06 16:39:23,017:INFO:SubProcess create_model() end ==================================
2024-11-06 16:39:23,018:INFO:Creating metrics dataframe
2024-11-06 16:39:23,023:INFO:Initializing Extreme Gradient Boosting
2024-11-06 16:39:23,024:INFO:Total runtime is 1.0821376919746397 minutes
2024-11-06 16:39:23,025:INFO:SubProcess create_model() called ==================================
2024-11-06 16:39:23,026:INFO:Initializing create_model()
2024-11-06 16:39:23,026:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea83bf510>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0ea3382b50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:39:23,026:INFO:Checking exceptions
2024-11-06 16:39:23,026:INFO:Importing libraries
2024-11-06 16:39:23,026:INFO:Copying training dataset
2024-11-06 16:39:23,249:INFO:Defining folds
2024-11-06 16:39:23,249:INFO:Declaring metric variables
2024-11-06 16:39:23,251:INFO:Importing untrained model
2024-11-06 16:39:23,254:INFO:Extreme Gradient Boosting Imported successfully
2024-11-06 16:39:23,258:INFO:Starting cross validation
2024-11-06 16:39:23,262:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:39:47,919:INFO:Calculating mean and std
2024-11-06 16:39:47,920:INFO:Creating metrics dataframe
2024-11-06 16:39:47,923:INFO:Uploading results into container
2024-11-06 16:39:47,924:INFO:Uploading model into container now
2024-11-06 16:39:47,924:INFO:_master_model_container: 13
2024-11-06 16:39:47,924:INFO:_display_container: 2
2024-11-06 16:39:47,925:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-11-06 16:39:47,925:INFO:create_model() successfully completed......................................
2024-11-06 16:39:48,013:INFO:SubProcess create_model() end ==================================
2024-11-06 16:39:48,013:INFO:Creating metrics dataframe
2024-11-06 16:39:48,019:INFO:Initializing Light Gradient Boosting Machine
2024-11-06 16:39:48,019:INFO:Total runtime is 1.4987303614616392 minutes
2024-11-06 16:39:48,021:INFO:SubProcess create_model() called ==================================
2024-11-06 16:39:48,022:INFO:Initializing create_model()
2024-11-06 16:39:48,022:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea83bf510>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0ea3382b50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:39:48,022:INFO:Checking exceptions
2024-11-06 16:39:48,022:INFO:Importing libraries
2024-11-06 16:39:48,022:INFO:Copying training dataset
2024-11-06 16:39:48,252:INFO:Defining folds
2024-11-06 16:39:48,252:INFO:Declaring metric variables
2024-11-06 16:39:48,254:INFO:Importing untrained model
2024-11-06 16:39:48,258:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-06 16:39:48,263:INFO:Starting cross validation
2024-11-06 16:39:48,267:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:40:12,164:INFO:Calculating mean and std
2024-11-06 16:40:12,165:INFO:Creating metrics dataframe
2024-11-06 16:40:12,166:INFO:Uploading results into container
2024-11-06 16:40:12,166:INFO:Uploading model into container now
2024-11-06 16:40:12,167:INFO:_master_model_container: 14
2024-11-06 16:40:12,167:INFO:_display_container: 2
2024-11-06 16:40:12,170:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=605, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-06 16:40:12,170:INFO:create_model() successfully completed......................................
2024-11-06 16:40:12,257:INFO:SubProcess create_model() end ==================================
2024-11-06 16:40:12,257:INFO:Creating metrics dataframe
2024-11-06 16:40:12,267:INFO:Initializing Dummy Classifier
2024-11-06 16:40:12,267:INFO:Total runtime is 1.902866760889689 minutes
2024-11-06 16:40:12,270:INFO:SubProcess create_model() called ==================================
2024-11-06 16:40:12,271:INFO:Initializing create_model()
2024-11-06 16:40:12,271:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea83bf510>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0ea3382b50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:40:12,271:INFO:Checking exceptions
2024-11-06 16:40:12,271:INFO:Importing libraries
2024-11-06 16:40:12,271:INFO:Copying training dataset
2024-11-06 16:40:12,514:INFO:Defining folds
2024-11-06 16:40:12,515:INFO:Declaring metric variables
2024-11-06 16:40:12,517:INFO:Importing untrained model
2024-11-06 16:40:12,521:INFO:Dummy Classifier Imported successfully
2024-11-06 16:40:12,526:INFO:Starting cross validation
2024-11-06 16:40:12,530:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:40:12,820:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:40:12,909:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:40:12,972:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:40:13,052:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:40:13,093:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:40:13,110:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:40:13,134:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:40:13,174:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:40:13,205:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:40:13,257:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:40:13,271:INFO:Calculating mean and std
2024-11-06 16:40:13,273:INFO:Creating metrics dataframe
2024-11-06 16:40:13,278:INFO:Uploading results into container
2024-11-06 16:40:13,279:INFO:Uploading model into container now
2024-11-06 16:40:13,280:INFO:_master_model_container: 15
2024-11-06 16:40:13,280:INFO:_display_container: 2
2024-11-06 16:40:13,281:INFO:DummyClassifier(constant=None, random_state=605, strategy='prior')
2024-11-06 16:40:13,281:INFO:create_model() successfully completed......................................
2024-11-06 16:40:13,371:INFO:SubProcess create_model() end ==================================
2024-11-06 16:40:13,372:INFO:Creating metrics dataframe
2024-11-06 16:40:13,380:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-06 16:40:13,389:INFO:Initializing create_model()
2024-11-06 16:40:13,389:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea83bf510>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=605, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:40:13,389:INFO:Checking exceptions
2024-11-06 16:40:13,391:INFO:Importing libraries
2024-11-06 16:40:13,391:INFO:Copying training dataset
2024-11-06 16:40:13,626:INFO:Defining folds
2024-11-06 16:40:13,626:INFO:Declaring metric variables
2024-11-06 16:40:13,626:INFO:Importing untrained model
2024-11-06 16:40:13,626:INFO:Declaring custom model
2024-11-06 16:40:13,626:INFO:Random Forest Classifier Imported successfully
2024-11-06 16:40:13,629:INFO:Cross validation set to False
2024-11-06 16:40:13,629:INFO:Fitting Model
2024-11-06 16:40:13,921:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=605, verbose=0,
                       warm_start=False)
2024-11-06 16:40:13,921:INFO:create_model() successfully completed......................................
2024-11-06 16:40:14,036:INFO:_master_model_container: 15
2024-11-06 16:40:14,037:INFO:_display_container: 2
2024-11-06 16:40:14,037:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=605, verbose=0,
                       warm_start=False)
2024-11-06 16:40:14,037:INFO:compare_models() successfully completed......................................
2024-11-06 16:43:57,565:INFO:PyCaret ClassificationExperiment
2024-11-06 16:43:57,565:INFO:Logging name: clf-default-name
2024-11-06 16:43:57,565:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-06 16:43:57,565:INFO:version 3.3.2
2024-11-06 16:43:57,565:INFO:Initializing setup()
2024-11-06 16:43:57,565:INFO:self.USI: 8796
2024-11-06 16:43:57,565:INFO:self._variable_keys: {'X', 'gpu_param', 'idx', 'fold_generator', 'pipeline', 'html_param', 'data', 'memory', 'USI', 'logging_param', 'X_test', 'n_jobs_param', 'fold_groups_param', 'seed', 'gpu_n_jobs_param', 'X_train', 'y_train', 'y', '_available_plots', '_ml_usecase', 'exp_name_log', 'target_param', 'is_multiclass', 'log_plots_param', 'exp_id', 'fold_shuffle_param', 'y_test', 'fix_imbalance'}
2024-11-06 16:43:57,565:INFO:Checking environment
2024-11-06 16:43:57,565:INFO:python_version: 3.11.10
2024-11-06 16:43:57,565:INFO:python_build: ('main', 'Oct  3 2024 07:29:13')
2024-11-06 16:43:57,565:INFO:machine: x86_64
2024-11-06 16:43:57,565:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 16:43:57,565:INFO:Memory: svmem(total=33037213696, available=19715653632, percent=40.3, used=12544172032, free=6957420544, active=13498122240, inactive=9030725632, buffers=870834176, cached=12664786944, shared=303988736, slab=2044043264)
2024-11-06 16:43:57,566:INFO:Physical Core: 8
2024-11-06 16:43:57,566:INFO:Logical Core: 16
2024-11-06 16:43:57,566:INFO:Checking libraries
2024-11-06 16:43:57,566:INFO:System:
2024-11-06 16:43:57,566:INFO:    python: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0]
2024-11-06 16:43:57,566:INFO:executable: /home/cid34senhas/miniconda3/envs/myenv/bin/python
2024-11-06 16:43:57,566:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 16:43:57,566:INFO:PyCaret required dependencies:
2024-11-06 16:43:57,566:INFO:                 pip: 24.2
2024-11-06 16:43:57,566:INFO:          setuptools: 75.1.0
2024-11-06 16:43:57,566:INFO:             pycaret: 3.3.2
2024-11-06 16:43:57,566:INFO:             IPython: 8.29.0
2024-11-06 16:43:57,566:INFO:          ipywidgets: 8.1.5
2024-11-06 16:43:57,566:INFO:                tqdm: 4.66.6
2024-11-06 16:43:57,566:INFO:               numpy: 1.26.4
2024-11-06 16:43:57,566:INFO:              pandas: 2.1.4
2024-11-06 16:43:57,566:INFO:              jinja2: 3.1.4
2024-11-06 16:43:57,566:INFO:               scipy: 1.11.4
2024-11-06 16:43:57,566:INFO:              joblib: 1.3.2
2024-11-06 16:43:57,566:INFO:             sklearn: 1.4.2
2024-11-06 16:43:57,566:INFO:                pyod: 2.0.2
2024-11-06 16:43:57,566:INFO:            imblearn: 0.12.4
2024-11-06 16:43:57,566:INFO:   category_encoders: 2.6.4
2024-11-06 16:43:57,566:INFO:            lightgbm: 4.5.0
2024-11-06 16:43:57,566:INFO:               numba: 0.60.0
2024-11-06 16:43:57,566:INFO:            requests: 2.32.3
2024-11-06 16:43:57,566:INFO:          matplotlib: 3.7.5
2024-11-06 16:43:57,566:INFO:          scikitplot: 0.3.7
2024-11-06 16:43:57,566:INFO:         yellowbrick: 1.5
2024-11-06 16:43:57,566:INFO:              plotly: 5.24.1
2024-11-06 16:43:57,566:INFO:    plotly-resampler: Not installed
2024-11-06 16:43:57,566:INFO:             kaleido: 0.2.1
2024-11-06 16:43:57,566:INFO:           schemdraw: 0.15
2024-11-06 16:43:57,566:INFO:         statsmodels: 0.14.4
2024-11-06 16:43:57,566:INFO:              sktime: 0.26.0
2024-11-06 16:43:57,566:INFO:               tbats: 1.1.3
2024-11-06 16:43:57,566:INFO:            pmdarima: 2.0.4
2024-11-06 16:43:57,566:INFO:              psutil: 6.1.0
2024-11-06 16:43:57,567:INFO:          markupsafe: 3.0.2
2024-11-06 16:43:57,567:INFO:             pickle5: Not installed
2024-11-06 16:43:57,567:INFO:         cloudpickle: 3.1.0
2024-11-06 16:43:57,567:INFO:         deprecation: 2.1.0
2024-11-06 16:43:57,567:INFO:              xxhash: 3.5.0
2024-11-06 16:43:57,567:INFO:           wurlitzer: 3.1.1
2024-11-06 16:43:57,567:INFO:PyCaret optional dependencies:
2024-11-06 16:43:57,567:INFO:                shap: Not installed
2024-11-06 16:43:57,567:INFO:           interpret: Not installed
2024-11-06 16:43:57,567:INFO:                umap: Not installed
2024-11-06 16:43:57,567:INFO:     ydata_profiling: Not installed
2024-11-06 16:43:57,567:INFO:  explainerdashboard: Not installed
2024-11-06 16:43:57,567:INFO:             autoviz: Not installed
2024-11-06 16:43:57,567:INFO:           fairlearn: Not installed
2024-11-06 16:43:57,567:INFO:          deepchecks: Not installed
2024-11-06 16:43:57,567:INFO:             xgboost: 2.1.2
2024-11-06 16:43:57,567:INFO:            catboost: Not installed
2024-11-06 16:43:57,567:INFO:              kmodes: Not installed
2024-11-06 16:43:57,567:INFO:             mlxtend: Not installed
2024-11-06 16:43:57,567:INFO:       statsforecast: Not installed
2024-11-06 16:43:57,567:INFO:        tune_sklearn: Not installed
2024-11-06 16:43:57,567:INFO:                 ray: Not installed
2024-11-06 16:43:57,567:INFO:            hyperopt: Not installed
2024-11-06 16:43:57,567:INFO:              optuna: Not installed
2024-11-06 16:43:57,567:INFO:               skopt: Not installed
2024-11-06 16:43:57,567:INFO:              mlflow: Not installed
2024-11-06 16:43:57,567:INFO:              gradio: Not installed
2024-11-06 16:43:57,567:INFO:             fastapi: Not installed
2024-11-06 16:43:57,567:INFO:             uvicorn: Not installed
2024-11-06 16:43:57,567:INFO:              m2cgen: Not installed
2024-11-06 16:43:57,567:INFO:           evidently: Not installed
2024-11-06 16:43:57,567:INFO:               fugue: Not installed
2024-11-06 16:43:57,567:INFO:           streamlit: Not installed
2024-11-06 16:43:57,567:INFO:             prophet: Not installed
2024-11-06 16:43:57,567:INFO:None
2024-11-06 16:43:57,567:INFO:Set up data.
2024-11-06 16:43:57,762:INFO:Set up folding strategy.
2024-11-06 16:43:57,762:INFO:Set up train/test split.
2024-11-06 16:43:57,984:INFO:Set up index.
2024-11-06 16:43:57,988:INFO:Assigning column types.
2024-11-06 16:43:58,305:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-06 16:43:58,340:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 16:43:58,340:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:43:58,362:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:43:58,364:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:43:58,398:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 16:43:58,399:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:43:58,420:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:43:58,422:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:43:58,423:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-06 16:43:58,458:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:43:58,479:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:43:58,481:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:43:58,516:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:43:58,538:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:43:58,541:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:43:58,541:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-06 16:43:58,598:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:43:58,600:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:43:58,658:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:43:58,660:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:43:58,661:INFO:Preparing preprocessing pipeline...
2024-11-06 16:43:58,683:INFO:Set up simple imputation.
2024-11-06 16:43:58,705:INFO:Set up column name cleaning.
2024-11-06 16:43:59,418:INFO:Finished creating preprocessing pipeline.
2024-11-06 16:43:59,428:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['diagnostics_Image-original_Mean',
                                             'diagnostics_Image-original_Maximum',
                                             'diagnostics_Mask-original_VoxelNum',
                                             'original_shape_Elongation',
                                             'original_shape_Flatness',
                                             'original_shape_LeastAxisLength',
                                             'original_shape_MajorAxisLength',
                                             'ori...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-06 16:43:59,428:INFO:Creating final display dataframe.
2024-11-06 16:44:02,867:INFO:Setup _display_container:                     Description             Value
0                    Session id              6512
1                        Target        Transition
2                   Target type        Multiclass
3           Original data shape       (295, 2014)
4        Transformed data shape       (295, 2014)
5   Transformed train set shape       (206, 2014)
6    Transformed test set shape        (89, 2014)
7              Numeric features              2013
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              8796
2024-11-06 16:44:02,929:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:44:02,931:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:44:02,987:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:44:02,989:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:44:02,990:INFO:setup() successfully completed in 5.43s...............
2024-11-06 16:45:05,189:INFO:PyCaret ClassificationExperiment
2024-11-06 16:45:05,189:INFO:Logging name: clf-default-name
2024-11-06 16:45:05,189:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-06 16:45:05,189:INFO:version 3.3.2
2024-11-06 16:45:05,189:INFO:Initializing setup()
2024-11-06 16:45:05,189:INFO:self.USI: 1d1f
2024-11-06 16:45:05,189:INFO:self._variable_keys: {'X', 'gpu_param', 'idx', 'fold_generator', 'pipeline', 'html_param', 'data', 'memory', 'USI', 'logging_param', 'X_test', 'n_jobs_param', 'fold_groups_param', 'seed', 'gpu_n_jobs_param', 'X_train', 'y_train', 'y', '_available_plots', '_ml_usecase', 'exp_name_log', 'target_param', 'is_multiclass', 'log_plots_param', 'exp_id', 'fold_shuffle_param', 'y_test', 'fix_imbalance'}
2024-11-06 16:45:05,189:INFO:Checking environment
2024-11-06 16:45:05,189:INFO:python_version: 3.11.10
2024-11-06 16:45:05,190:INFO:python_build: ('main', 'Oct  3 2024 07:29:13')
2024-11-06 16:45:05,190:INFO:machine: x86_64
2024-11-06 16:45:05,190:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 16:45:05,190:INFO:Memory: svmem(total=33037213696, available=19706896384, percent=40.3, used=12568645632, free=6943154176, active=13488238592, inactive=9036083200, buffers=871157760, cached=12654256128, shared=288292864, slab=2044559360)
2024-11-06 16:45:05,192:INFO:Physical Core: 8
2024-11-06 16:45:05,192:INFO:Logical Core: 16
2024-11-06 16:45:05,192:INFO:Checking libraries
2024-11-06 16:45:05,192:INFO:System:
2024-11-06 16:45:05,192:INFO:    python: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0]
2024-11-06 16:45:05,192:INFO:executable: /home/cid34senhas/miniconda3/envs/myenv/bin/python
2024-11-06 16:45:05,192:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 16:45:05,192:INFO:PyCaret required dependencies:
2024-11-06 16:45:05,192:INFO:                 pip: 24.2
2024-11-06 16:45:05,192:INFO:          setuptools: 75.1.0
2024-11-06 16:45:05,193:INFO:             pycaret: 3.3.2
2024-11-06 16:45:05,193:INFO:             IPython: 8.29.0
2024-11-06 16:45:05,193:INFO:          ipywidgets: 8.1.5
2024-11-06 16:45:05,193:INFO:                tqdm: 4.66.6
2024-11-06 16:45:05,193:INFO:               numpy: 1.26.4
2024-11-06 16:45:05,193:INFO:              pandas: 2.1.4
2024-11-06 16:45:05,193:INFO:              jinja2: 3.1.4
2024-11-06 16:45:05,193:INFO:               scipy: 1.11.4
2024-11-06 16:45:05,193:INFO:              joblib: 1.3.2
2024-11-06 16:45:05,193:INFO:             sklearn: 1.4.2
2024-11-06 16:45:05,193:INFO:                pyod: 2.0.2
2024-11-06 16:45:05,193:INFO:            imblearn: 0.12.4
2024-11-06 16:45:05,193:INFO:   category_encoders: 2.6.4
2024-11-06 16:45:05,193:INFO:            lightgbm: 4.5.0
2024-11-06 16:45:05,193:INFO:               numba: 0.60.0
2024-11-06 16:45:05,193:INFO:            requests: 2.32.3
2024-11-06 16:45:05,193:INFO:          matplotlib: 3.7.5
2024-11-06 16:45:05,193:INFO:          scikitplot: 0.3.7
2024-11-06 16:45:05,193:INFO:         yellowbrick: 1.5
2024-11-06 16:45:05,194:INFO:              plotly: 5.24.1
2024-11-06 16:45:05,194:INFO:    plotly-resampler: Not installed
2024-11-06 16:45:05,194:INFO:             kaleido: 0.2.1
2024-11-06 16:45:05,194:INFO:           schemdraw: 0.15
2024-11-06 16:45:05,194:INFO:         statsmodels: 0.14.4
2024-11-06 16:45:05,194:INFO:              sktime: 0.26.0
2024-11-06 16:45:05,194:INFO:               tbats: 1.1.3
2024-11-06 16:45:05,194:INFO:            pmdarima: 2.0.4
2024-11-06 16:45:05,194:INFO:              psutil: 6.1.0
2024-11-06 16:45:05,194:INFO:          markupsafe: 3.0.2
2024-11-06 16:45:05,194:INFO:             pickle5: Not installed
2024-11-06 16:45:05,194:INFO:         cloudpickle: 3.1.0
2024-11-06 16:45:05,194:INFO:         deprecation: 2.1.0
2024-11-06 16:45:05,194:INFO:              xxhash: 3.5.0
2024-11-06 16:45:05,194:INFO:           wurlitzer: 3.1.1
2024-11-06 16:45:05,194:INFO:PyCaret optional dependencies:
2024-11-06 16:45:05,194:INFO:                shap: Not installed
2024-11-06 16:45:05,195:INFO:           interpret: Not installed
2024-11-06 16:45:05,195:INFO:                umap: Not installed
2024-11-06 16:45:05,195:INFO:     ydata_profiling: Not installed
2024-11-06 16:45:05,195:INFO:  explainerdashboard: Not installed
2024-11-06 16:45:05,195:INFO:             autoviz: Not installed
2024-11-06 16:45:05,195:INFO:           fairlearn: Not installed
2024-11-06 16:45:05,195:INFO:          deepchecks: Not installed
2024-11-06 16:45:05,195:INFO:             xgboost: 2.1.2
2024-11-06 16:45:05,195:INFO:            catboost: Not installed
2024-11-06 16:45:05,195:INFO:              kmodes: Not installed
2024-11-06 16:45:05,195:INFO:             mlxtend: Not installed
2024-11-06 16:45:05,195:INFO:       statsforecast: Not installed
2024-11-06 16:45:05,195:INFO:        tune_sklearn: Not installed
2024-11-06 16:45:05,195:INFO:                 ray: Not installed
2024-11-06 16:45:05,196:INFO:            hyperopt: Not installed
2024-11-06 16:45:05,196:INFO:              optuna: Not installed
2024-11-06 16:45:05,196:INFO:               skopt: Not installed
2024-11-06 16:45:05,196:INFO:              mlflow: Not installed
2024-11-06 16:45:05,196:INFO:              gradio: Not installed
2024-11-06 16:45:05,196:INFO:             fastapi: Not installed
2024-11-06 16:45:05,196:INFO:             uvicorn: Not installed
2024-11-06 16:45:05,196:INFO:              m2cgen: Not installed
2024-11-06 16:45:05,196:INFO:           evidently: Not installed
2024-11-06 16:45:05,196:INFO:               fugue: Not installed
2024-11-06 16:45:05,196:INFO:           streamlit: Not installed
2024-11-06 16:45:05,196:INFO:             prophet: Not installed
2024-11-06 16:45:05,196:INFO:None
2024-11-06 16:45:05,196:INFO:Set up data.
2024-11-06 16:45:05,486:INFO:Set up folding strategy.
2024-11-06 16:45:05,486:INFO:Set up train/test split.
2024-11-06 16:45:05,691:INFO:Set up index.
2024-11-06 16:45:05,696:INFO:Assigning column types.
2024-11-06 16:45:05,936:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-06 16:45:05,971:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 16:45:05,971:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:45:05,993:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:45:05,995:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:45:06,029:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 16:45:06,030:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:45:06,051:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:45:06,053:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:45:06,054:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-06 16:45:06,088:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:45:06,110:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:45:06,112:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:45:06,146:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:45:06,168:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:45:06,170:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:45:06,170:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-06 16:45:06,226:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:45:06,228:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:45:06,286:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:45:06,288:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:45:06,288:INFO:Preparing preprocessing pipeline...
2024-11-06 16:45:06,309:INFO:Set up simple imputation.
2024-11-06 16:45:06,329:INFO:Set up column name cleaning.
2024-11-06 16:45:07,113:INFO:Finished creating preprocessing pipeline.
2024-11-06 16:45:07,122:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['diagnostics_Image-original_Mean',
                                             'diagnostics_Image-original_Maximum',
                                             'diagnostics_Mask-original_VoxelNum',
                                             'original_shape_Elongation',
                                             'original_shape_Flatness',
                                             'original_shape_LeastAxisLength',
                                             'original_shape_MajorAxisLength',
                                             'ori...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-06 16:45:07,122:INFO:Creating final display dataframe.
2024-11-06 16:45:10,460:INFO:Setup _display_container:                     Description             Value
0                    Session id              1663
1                        Target        Transition
2                   Target type        Multiclass
3           Original data shape       (295, 2014)
4        Transformed data shape       (295, 2014)
5   Transformed train set shape       (206, 2014)
6    Transformed test set shape        (89, 2014)
7              Numeric features              2013
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              1d1f
2024-11-06 16:45:10,520:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:45:10,522:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:45:10,577:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:45:10,579:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:45:10,580:INFO:setup() successfully completed in 5.4s...............
2024-11-06 16:51:25,826:INFO:PyCaret ClassificationExperiment
2024-11-06 16:51:25,826:INFO:Logging name: clf-default-name
2024-11-06 16:51:25,826:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-06 16:51:25,826:INFO:version 3.3.2
2024-11-06 16:51:25,826:INFO:Initializing setup()
2024-11-06 16:51:25,826:INFO:self.USI: 5afb
2024-11-06 16:51:25,826:INFO:self._variable_keys: {'X', 'gpu_param', 'idx', 'fold_generator', 'pipeline', 'html_param', 'data', 'memory', 'USI', 'logging_param', 'X_test', 'n_jobs_param', 'fold_groups_param', 'seed', 'gpu_n_jobs_param', 'X_train', 'y_train', 'y', '_available_plots', '_ml_usecase', 'exp_name_log', 'target_param', 'is_multiclass', 'log_plots_param', 'exp_id', 'fold_shuffle_param', 'y_test', 'fix_imbalance'}
2024-11-06 16:51:25,826:INFO:Checking environment
2024-11-06 16:51:25,826:INFO:python_version: 3.11.10
2024-11-06 16:51:25,826:INFO:python_build: ('main', 'Oct  3 2024 07:29:13')
2024-11-06 16:51:25,827:INFO:machine: x86_64
2024-11-06 16:51:25,827:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 16:51:25,827:INFO:Memory: svmem(total=33037213696, available=23152730112, percent=29.9, used=9130577920, free=10381963264, active=10078679040, inactive=9042907136, buffers=872595456, cached=12652077056, shared=280518656, slab=2036191232)
2024-11-06 16:51:25,828:INFO:Physical Core: 8
2024-11-06 16:51:25,828:INFO:Logical Core: 16
2024-11-06 16:51:25,828:INFO:Checking libraries
2024-11-06 16:51:25,828:INFO:System:
2024-11-06 16:51:25,828:INFO:    python: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0]
2024-11-06 16:51:25,828:INFO:executable: /home/cid34senhas/miniconda3/envs/myenv/bin/python
2024-11-06 16:51:25,828:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.39
2024-11-06 16:51:25,828:INFO:PyCaret required dependencies:
2024-11-06 16:51:25,829:INFO:                 pip: 24.2
2024-11-06 16:51:25,829:INFO:          setuptools: 75.1.0
2024-11-06 16:51:25,829:INFO:             pycaret: 3.3.2
2024-11-06 16:51:25,829:INFO:             IPython: 8.29.0
2024-11-06 16:51:25,829:INFO:          ipywidgets: 8.1.5
2024-11-06 16:51:25,829:INFO:                tqdm: 4.66.6
2024-11-06 16:51:25,829:INFO:               numpy: 1.26.4
2024-11-06 16:51:25,829:INFO:              pandas: 2.1.4
2024-11-06 16:51:25,829:INFO:              jinja2: 3.1.4
2024-11-06 16:51:25,829:INFO:               scipy: 1.11.4
2024-11-06 16:51:25,829:INFO:              joblib: 1.3.2
2024-11-06 16:51:25,829:INFO:             sklearn: 1.4.2
2024-11-06 16:51:25,829:INFO:                pyod: 2.0.2
2024-11-06 16:51:25,829:INFO:            imblearn: 0.12.4
2024-11-06 16:51:25,829:INFO:   category_encoders: 2.6.4
2024-11-06 16:51:25,829:INFO:            lightgbm: 4.5.0
2024-11-06 16:51:25,829:INFO:               numba: 0.60.0
2024-11-06 16:51:25,829:INFO:            requests: 2.32.3
2024-11-06 16:51:25,829:INFO:          matplotlib: 3.7.5
2024-11-06 16:51:25,829:INFO:          scikitplot: 0.3.7
2024-11-06 16:51:25,829:INFO:         yellowbrick: 1.5
2024-11-06 16:51:25,829:INFO:              plotly: 5.24.1
2024-11-06 16:51:25,829:INFO:    plotly-resampler: Not installed
2024-11-06 16:51:25,829:INFO:             kaleido: 0.2.1
2024-11-06 16:51:25,829:INFO:           schemdraw: 0.15
2024-11-06 16:51:25,829:INFO:         statsmodels: 0.14.4
2024-11-06 16:51:25,829:INFO:              sktime: 0.26.0
2024-11-06 16:51:25,829:INFO:               tbats: 1.1.3
2024-11-06 16:51:25,829:INFO:            pmdarima: 2.0.4
2024-11-06 16:51:25,829:INFO:              psutil: 6.1.0
2024-11-06 16:51:25,829:INFO:          markupsafe: 3.0.2
2024-11-06 16:51:25,829:INFO:             pickle5: Not installed
2024-11-06 16:51:25,830:INFO:         cloudpickle: 3.1.0
2024-11-06 16:51:25,830:INFO:         deprecation: 2.1.0
2024-11-06 16:51:25,830:INFO:              xxhash: 3.5.0
2024-11-06 16:51:25,830:INFO:           wurlitzer: 3.1.1
2024-11-06 16:51:25,830:INFO:PyCaret optional dependencies:
2024-11-06 16:51:25,830:INFO:                shap: Not installed
2024-11-06 16:51:25,830:INFO:           interpret: Not installed
2024-11-06 16:51:25,830:INFO:                umap: Not installed
2024-11-06 16:51:25,830:INFO:     ydata_profiling: Not installed
2024-11-06 16:51:25,830:INFO:  explainerdashboard: Not installed
2024-11-06 16:51:25,830:INFO:             autoviz: Not installed
2024-11-06 16:51:25,830:INFO:           fairlearn: Not installed
2024-11-06 16:51:25,830:INFO:          deepchecks: Not installed
2024-11-06 16:51:25,830:INFO:             xgboost: 2.1.2
2024-11-06 16:51:25,830:INFO:            catboost: Not installed
2024-11-06 16:51:25,830:INFO:              kmodes: Not installed
2024-11-06 16:51:25,830:INFO:             mlxtend: Not installed
2024-11-06 16:51:25,830:INFO:       statsforecast: Not installed
2024-11-06 16:51:25,830:INFO:        tune_sklearn: Not installed
2024-11-06 16:51:25,830:INFO:                 ray: Not installed
2024-11-06 16:51:25,830:INFO:            hyperopt: Not installed
2024-11-06 16:51:25,830:INFO:              optuna: Not installed
2024-11-06 16:51:25,830:INFO:               skopt: Not installed
2024-11-06 16:51:25,830:INFO:              mlflow: Not installed
2024-11-06 16:51:25,830:INFO:              gradio: Not installed
2024-11-06 16:51:25,830:INFO:             fastapi: Not installed
2024-11-06 16:51:25,830:INFO:             uvicorn: Not installed
2024-11-06 16:51:25,830:INFO:              m2cgen: Not installed
2024-11-06 16:51:25,830:INFO:           evidently: Not installed
2024-11-06 16:51:25,830:INFO:               fugue: Not installed
2024-11-06 16:51:25,830:INFO:           streamlit: Not installed
2024-11-06 16:51:25,831:INFO:             prophet: Not installed
2024-11-06 16:51:25,831:INFO:None
2024-11-06 16:51:25,831:INFO:Set up data.
2024-11-06 16:51:26,113:INFO:Set up folding strategy.
2024-11-06 16:51:26,113:INFO:Set up train/test split.
2024-11-06 16:51:26,316:INFO:Set up index.
2024-11-06 16:51:26,321:INFO:Assigning column types.
2024-11-06 16:51:26,550:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-06 16:51:26,585:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 16:51:26,586:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:51:26,607:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:51:26,609:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:51:26,643:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-06 16:51:26,644:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:51:26,665:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:51:26,667:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:51:26,667:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-06 16:51:26,702:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:51:26,723:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:51:26,725:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:51:26,759:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-06 16:51:26,781:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:51:26,783:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:51:26,783:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-06 16:51:26,839:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:51:26,841:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:51:26,897:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:51:26,899:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:51:26,899:INFO:Preparing preprocessing pipeline...
2024-11-06 16:51:26,921:INFO:Set up simple imputation.
2024-11-06 16:51:27,024:INFO:Set up column name cleaning.
2024-11-06 16:51:28,207:INFO:Finished creating preprocessing pipeline.
2024-11-06 16:51:28,216:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['diagnostics_Image-original_Mean',
                                             'diagnostics_Image-original_Maximum',
                                             'diagnostics_Mask-original_VoxelNum',
                                             'original_shape_Elongation',
                                             'original_shape_Flatness',
                                             'original_shape_LeastAxisLength',
                                             'original_shape_MajorAxisLength',
                                             'ori...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-06 16:51:28,217:INFO:Creating final display dataframe.
2024-11-06 16:51:30,872:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pipeline.py:289: UserWarning: Persisting input arguments took 0.50s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-11-06 16:51:31,746:INFO:Setup _display_container:                     Description             Value
0                    Session id              6337
1                        Target        Transition
2                   Target type        Multiclass
3           Original data shape       (295, 2014)
4        Transformed data shape       (295, 2014)
5   Transformed train set shape       (206, 2014)
6    Transformed test set shape        (89, 2014)
7              Numeric features              2013
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              5afb
2024-11-06 16:51:31,807:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:51:31,809:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:51:31,867:INFO:Soft dependency imported: xgboost: 2.1.2
2024-11-06 16:51:31,869:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-06 16:51:31,870:INFO:setup() successfully completed in 6.05s...............
2024-11-06 16:51:31,889:INFO:Initializing compare_models()
2024-11-06 16:51:31,890:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea80e89d0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea80e89d0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-06 16:51:31,890:INFO:Checking exceptions
2024-11-06 16:51:31,991:INFO:Preparing display monitor
2024-11-06 16:51:32,008:INFO:Initializing Logistic Regression
2024-11-06 16:51:32,009:INFO:Total runtime is 1.625219980875651e-06 minutes
2024-11-06 16:51:32,011:INFO:SubProcess create_model() called ==================================
2024-11-06 16:51:32,011:INFO:Initializing create_model()
2024-11-06 16:51:32,012:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea80e89d0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0f14d4f410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:51:32,012:INFO:Checking exceptions
2024-11-06 16:51:32,012:INFO:Importing libraries
2024-11-06 16:51:32,012:INFO:Copying training dataset
2024-11-06 16:51:32,361:INFO:Defining folds
2024-11-06 16:51:32,361:INFO:Declaring metric variables
2024-11-06 16:51:32,364:INFO:Importing untrained model
2024-11-06 16:51:32,366:INFO:Logistic Regression Imported successfully
2024-11-06 16:51:32,371:INFO:Starting cross validation
2024-11-06 16:51:32,374:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:51:40,129:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:51:40,200:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:40,316:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:51:40,354:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:51:40,376:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:40,418:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:40,484:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:51:40,539:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:40,550:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:51:40,607:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:40,775:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:51:40,824:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:41,005:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:51:41,034:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:51:41,051:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:41,074:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:41,082:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:51:41,124:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:41,206:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-06 16:51:41,244:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:41,262:INFO:Calculating mean and std
2024-11-06 16:51:41,265:INFO:Creating metrics dataframe
2024-11-06 16:51:41,270:INFO:Uploading results into container
2024-11-06 16:51:41,272:INFO:Uploading model into container now
2024-11-06 16:51:41,273:INFO:_master_model_container: 1
2024-11-06 16:51:41,273:INFO:_display_container: 2
2024-11-06 16:51:41,274:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=6337, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-06 16:51:41,274:INFO:create_model() successfully completed......................................
2024-11-06 16:51:41,398:INFO:SubProcess create_model() end ==================================
2024-11-06 16:51:41,398:INFO:Creating metrics dataframe
2024-11-06 16:51:41,403:INFO:Initializing K Neighbors Classifier
2024-11-06 16:51:41,403:INFO:Total runtime is 0.15656901995340983 minutes
2024-11-06 16:51:41,405:INFO:SubProcess create_model() called ==================================
2024-11-06 16:51:41,405:INFO:Initializing create_model()
2024-11-06 16:51:41,406:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea80e89d0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0f14d4f410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:51:41,406:INFO:Checking exceptions
2024-11-06 16:51:41,406:INFO:Importing libraries
2024-11-06 16:51:41,406:INFO:Copying training dataset
2024-11-06 16:51:41,682:INFO:Defining folds
2024-11-06 16:51:41,682:INFO:Declaring metric variables
2024-11-06 16:51:41,685:INFO:Importing untrained model
2024-11-06 16:51:41,688:INFO:K Neighbors Classifier Imported successfully
2024-11-06 16:51:41,693:INFO:Starting cross validation
2024-11-06 16:51:41,698:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:51:43,451:INFO:Calculating mean and std
2024-11-06 16:51:43,453:INFO:Creating metrics dataframe
2024-11-06 16:51:43,456:INFO:Uploading results into container
2024-11-06 16:51:43,456:INFO:Uploading model into container now
2024-11-06 16:51:43,457:INFO:_master_model_container: 2
2024-11-06 16:51:43,457:INFO:_display_container: 2
2024-11-06 16:51:43,458:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-06 16:51:43,458:INFO:create_model() successfully completed......................................
2024-11-06 16:51:43,558:INFO:SubProcess create_model() end ==================================
2024-11-06 16:51:43,558:INFO:Creating metrics dataframe
2024-11-06 16:51:43,563:INFO:Initializing Naive Bayes
2024-11-06 16:51:43,563:INFO:Total runtime is 0.19258348941802977 minutes
2024-11-06 16:51:43,566:INFO:SubProcess create_model() called ==================================
2024-11-06 16:51:43,566:INFO:Initializing create_model()
2024-11-06 16:51:43,566:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea80e89d0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0f14d4f410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:51:43,566:INFO:Checking exceptions
2024-11-06 16:51:43,566:INFO:Importing libraries
2024-11-06 16:51:43,566:INFO:Copying training dataset
2024-11-06 16:51:43,803:INFO:Defining folds
2024-11-06 16:51:43,803:INFO:Declaring metric variables
2024-11-06 16:51:43,805:INFO:Importing untrained model
2024-11-06 16:51:43,808:INFO:Naive Bayes Imported successfully
2024-11-06 16:51:43,814:INFO:Starting cross validation
2024-11-06 16:51:43,818:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:51:44,395:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:51:44,560:INFO:Calculating mean and std
2024-11-06 16:51:44,562:INFO:Creating metrics dataframe
2024-11-06 16:51:44,564:INFO:Uploading results into container
2024-11-06 16:51:44,565:INFO:Uploading model into container now
2024-11-06 16:51:44,566:INFO:_master_model_container: 3
2024-11-06 16:51:44,566:INFO:_display_container: 2
2024-11-06 16:51:44,566:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-06 16:51:44,566:INFO:create_model() successfully completed......................................
2024-11-06 16:51:44,667:INFO:SubProcess create_model() end ==================================
2024-11-06 16:51:44,667:INFO:Creating metrics dataframe
2024-11-06 16:51:44,671:INFO:Initializing Decision Tree Classifier
2024-11-06 16:51:44,672:INFO:Total runtime is 0.2110522230466207 minutes
2024-11-06 16:51:44,674:INFO:SubProcess create_model() called ==================================
2024-11-06 16:51:44,674:INFO:Initializing create_model()
2024-11-06 16:51:44,674:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea80e89d0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0f14d4f410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:51:44,674:INFO:Checking exceptions
2024-11-06 16:51:44,674:INFO:Importing libraries
2024-11-06 16:51:44,674:INFO:Copying training dataset
2024-11-06 16:51:44,916:INFO:Defining folds
2024-11-06 16:51:44,916:INFO:Declaring metric variables
2024-11-06 16:51:44,918:INFO:Importing untrained model
2024-11-06 16:51:44,921:INFO:Decision Tree Classifier Imported successfully
2024-11-06 16:51:44,924:INFO:Starting cross validation
2024-11-06 16:51:44,928:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:51:45,763:INFO:Calculating mean and std
2024-11-06 16:51:45,764:INFO:Creating metrics dataframe
2024-11-06 16:51:45,765:INFO:Uploading results into container
2024-11-06 16:51:45,765:INFO:Uploading model into container now
2024-11-06 16:51:45,766:INFO:_master_model_container: 4
2024-11-06 16:51:45,766:INFO:_display_container: 2
2024-11-06 16:51:45,766:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=6337, splitter='best')
2024-11-06 16:51:45,766:INFO:create_model() successfully completed......................................
2024-11-06 16:51:45,851:INFO:SubProcess create_model() end ==================================
2024-11-06 16:51:45,851:INFO:Creating metrics dataframe
2024-11-06 16:51:45,856:INFO:Initializing SVM - Linear Kernel
2024-11-06 16:51:45,856:INFO:Total runtime is 0.23079465627670287 minutes
2024-11-06 16:51:45,859:INFO:SubProcess create_model() called ==================================
2024-11-06 16:51:45,859:INFO:Initializing create_model()
2024-11-06 16:51:45,859:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea80e89d0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0f14d4f410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:51:45,859:INFO:Checking exceptions
2024-11-06 16:51:45,859:INFO:Importing libraries
2024-11-06 16:51:45,859:INFO:Copying training dataset
2024-11-06 16:51:46,119:INFO:Defining folds
2024-11-06 16:51:46,119:INFO:Declaring metric variables
2024-11-06 16:51:46,122:INFO:Importing untrained model
2024-11-06 16:51:46,124:INFO:SVM - Linear Kernel Imported successfully
2024-11-06 16:51:46,130:INFO:Starting cross validation
2024-11-06 16:51:46,134:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:51:46,582:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:46,586:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:51:46,601:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:46,607:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:51:46,693:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:46,698:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:51:46,730:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:46,735:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:51:46,752:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:46,756:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:51:46,784:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:46,787:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:51:46,799:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:46,802:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:51:46,899:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:46,902:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:51:46,905:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:46,908:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:51:46,948:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:46,951:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:51:46,958:INFO:Calculating mean and std
2024-11-06 16:51:46,959:INFO:Creating metrics dataframe
2024-11-06 16:51:46,962:INFO:Uploading results into container
2024-11-06 16:51:46,963:INFO:Uploading model into container now
2024-11-06 16:51:46,963:INFO:_master_model_container: 5
2024-11-06 16:51:46,963:INFO:_display_container: 2
2024-11-06 16:51:46,964:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=6337, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-06 16:51:46,964:INFO:create_model() successfully completed......................................
2024-11-06 16:51:47,061:INFO:SubProcess create_model() end ==================================
2024-11-06 16:51:47,061:INFO:Creating metrics dataframe
2024-11-06 16:51:47,067:INFO:Initializing Ridge Classifier
2024-11-06 16:51:47,067:INFO:Total runtime is 0.2509730617205302 minutes
2024-11-06 16:51:47,069:INFO:SubProcess create_model() called ==================================
2024-11-06 16:51:47,070:INFO:Initializing create_model()
2024-11-06 16:51:47,070:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea80e89d0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0f14d4f410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:51:47,070:INFO:Checking exceptions
2024-11-06 16:51:47,070:INFO:Importing libraries
2024-11-06 16:51:47,070:INFO:Copying training dataset
2024-11-06 16:51:47,307:INFO:Defining folds
2024-11-06 16:51:47,307:INFO:Declaring metric variables
2024-11-06 16:51:47,310:INFO:Importing untrained model
2024-11-06 16:51:47,313:INFO:Ridge Classifier Imported successfully
2024-11-06 16:51:47,318:INFO:Starting cross validation
2024-11-06 16:51:47,322:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:51:47,521:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:51:47,560:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:51:47,611:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:51:47,625:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:51:47,647:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:47,681:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:51:47,689:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:47,736:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:47,740:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:51:47,747:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:47,779:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:51:47,793:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:47,807:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:51:47,848:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:51:47,874:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:243: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(

2024-11-06 16:51:47,893:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:47,893:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:47,918:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:47,951:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:47,976:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:47,984:INFO:Calculating mean and std
2024-11-06 16:51:47,985:INFO:Creating metrics dataframe
2024-11-06 16:51:47,986:INFO:Uploading results into container
2024-11-06 16:51:47,986:INFO:Uploading model into container now
2024-11-06 16:51:47,987:INFO:_master_model_container: 6
2024-11-06 16:51:47,987:INFO:_display_container: 2
2024-11-06 16:51:47,987:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=6337, solver='auto',
                tol=0.0001)
2024-11-06 16:51:47,987:INFO:create_model() successfully completed......................................
2024-11-06 16:51:48,070:INFO:SubProcess create_model() end ==================================
2024-11-06 16:51:48,070:INFO:Creating metrics dataframe
2024-11-06 16:51:48,077:INFO:Initializing Random Forest Classifier
2024-11-06 16:51:48,077:INFO:Total runtime is 0.26780893007914225 minutes
2024-11-06 16:51:48,079:INFO:SubProcess create_model() called ==================================
2024-11-06 16:51:48,080:INFO:Initializing create_model()
2024-11-06 16:51:48,080:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea80e89d0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0f14d4f410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:51:48,080:INFO:Checking exceptions
2024-11-06 16:51:48,080:INFO:Importing libraries
2024-11-06 16:51:48,080:INFO:Copying training dataset
2024-11-06 16:51:48,317:INFO:Defining folds
2024-11-06 16:51:48,317:INFO:Declaring metric variables
2024-11-06 16:51:48,319:INFO:Importing untrained model
2024-11-06 16:51:48,322:INFO:Random Forest Classifier Imported successfully
2024-11-06 16:51:48,327:INFO:Starting cross validation
2024-11-06 16:51:48,330:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:51:49,314:INFO:Calculating mean and std
2024-11-06 16:51:49,315:INFO:Creating metrics dataframe
2024-11-06 16:51:49,320:INFO:Uploading results into container
2024-11-06 16:51:49,320:INFO:Uploading model into container now
2024-11-06 16:51:49,321:INFO:_master_model_container: 7
2024-11-06 16:51:49,321:INFO:_display_container: 2
2024-11-06 16:51:49,321:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6337, verbose=0,
                       warm_start=False)
2024-11-06 16:51:49,321:INFO:create_model() successfully completed......................................
2024-11-06 16:51:49,409:INFO:SubProcess create_model() end ==================================
2024-11-06 16:51:49,409:INFO:Creating metrics dataframe
2024-11-06 16:51:49,414:INFO:Initializing Quadratic Discriminant Analysis
2024-11-06 16:51:49,414:INFO:Total runtime is 0.29009531339009603 minutes
2024-11-06 16:51:49,417:INFO:SubProcess create_model() called ==================================
2024-11-06 16:51:49,417:INFO:Initializing create_model()
2024-11-06 16:51:49,417:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea80e89d0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0f14d4f410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:51:49,417:INFO:Checking exceptions
2024-11-06 16:51:49,417:INFO:Importing libraries
2024-11-06 16:51:49,417:INFO:Copying training dataset
2024-11-06 16:51:49,659:INFO:Defining folds
2024-11-06 16:51:49,660:INFO:Declaring metric variables
2024-11-06 16:51:49,662:INFO:Importing untrained model
2024-11-06 16:51:49,665:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-06 16:51:49,670:INFO:Starting cross validation
2024-11-06 16:51:49,674:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:51:49,879:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:51:49,931:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:51:49,960:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:51:50,030:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:51:50,038:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:51:50,046:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:50,051:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:51:50,071:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:50,106:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:50,112:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:51:50,143:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:51:50,157:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:50,168:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:50,171:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:51:50,205:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:51:50,228:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:51:50,266:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:50,269:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:51:50,292:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:51:50,293:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-06 16:51:50,327:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:50,351:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:50,400:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:50,400:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:50,403:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:51:50,413:INFO:Calculating mean and std
2024-11-06 16:51:50,415:INFO:Creating metrics dataframe
2024-11-06 16:51:50,416:INFO:Uploading results into container
2024-11-06 16:51:50,418:INFO:Uploading model into container now
2024-11-06 16:51:50,419:INFO:_master_model_container: 8
2024-11-06 16:51:50,419:INFO:_display_container: 2
2024-11-06 16:51:50,419:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-06 16:51:50,420:INFO:create_model() successfully completed......................................
2024-11-06 16:51:50,520:INFO:SubProcess create_model() end ==================================
2024-11-06 16:51:50,520:INFO:Creating metrics dataframe
2024-11-06 16:51:50,526:INFO:Initializing Ada Boost Classifier
2024-11-06 16:51:50,526:INFO:Total runtime is 0.3086229165395101 minutes
2024-11-06 16:51:50,528:INFO:SubProcess create_model() called ==================================
2024-11-06 16:51:50,528:INFO:Initializing create_model()
2024-11-06 16:51:50,528:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea80e89d0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0f14d4f410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:51:50,528:INFO:Checking exceptions
2024-11-06 16:51:50,529:INFO:Importing libraries
2024-11-06 16:51:50,529:INFO:Copying training dataset
2024-11-06 16:51:50,771:INFO:Defining folds
2024-11-06 16:51:50,771:INFO:Declaring metric variables
2024-11-06 16:51:50,774:INFO:Importing untrained model
2024-11-06 16:51:50,777:INFO:Ada Boost Classifier Imported successfully
2024-11-06 16:51:50,783:INFO:Starting cross validation
2024-11-06 16:51:50,787:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:51:51,017:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:51:51,032:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:51:51,084:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:51:51,111:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:51:51,149:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:51:51,236:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:51:51,261:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:51:51,286:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:51:51,332:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:51:51,397:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-06 16:51:52,976:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:53,073:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:53,105:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:53,170:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:53,255:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:53,256:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:53,260:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:51:53,284:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:53,330:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:53,508:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:53,610:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:51:53,620:INFO:Calculating mean and std
2024-11-06 16:51:53,622:INFO:Creating metrics dataframe
2024-11-06 16:51:53,625:INFO:Uploading results into container
2024-11-06 16:51:53,626:INFO:Uploading model into container now
2024-11-06 16:51:53,626:INFO:_master_model_container: 9
2024-11-06 16:51:53,627:INFO:_display_container: 2
2024-11-06 16:51:53,627:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=6337)
2024-11-06 16:51:53,627:INFO:create_model() successfully completed......................................
2024-11-06 16:51:53,731:INFO:SubProcess create_model() end ==================================
2024-11-06 16:51:53,731:INFO:Creating metrics dataframe
2024-11-06 16:51:53,738:INFO:Initializing Gradient Boosting Classifier
2024-11-06 16:51:53,738:INFO:Total runtime is 0.36215860446294146 minutes
2024-11-06 16:51:53,740:INFO:SubProcess create_model() called ==================================
2024-11-06 16:51:53,741:INFO:Initializing create_model()
2024-11-06 16:51:53,741:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea80e89d0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0f14d4f410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:51:53,741:INFO:Checking exceptions
2024-11-06 16:51:53,741:INFO:Importing libraries
2024-11-06 16:51:53,741:INFO:Copying training dataset
2024-11-06 16:51:53,981:INFO:Defining folds
2024-11-06 16:51:53,981:INFO:Declaring metric variables
2024-11-06 16:51:53,984:INFO:Importing untrained model
2024-11-06 16:51:53,987:INFO:Gradient Boosting Classifier Imported successfully
2024-11-06 16:51:53,992:INFO:Starting cross validation
2024-11-06 16:51:53,997:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:52:30,160:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:30,161:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:30,453:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:30,616:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:30,744:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:30,746:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:34,124:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:34,193:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:34,333:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:34,783:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:34,796:INFO:Calculating mean and std
2024-11-06 16:52:34,798:INFO:Creating metrics dataframe
2024-11-06 16:52:34,800:INFO:Uploading results into container
2024-11-06 16:52:34,801:INFO:Uploading model into container now
2024-11-06 16:52:34,801:INFO:_master_model_container: 10
2024-11-06 16:52:34,802:INFO:_display_container: 2
2024-11-06 16:52:34,802:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6337, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-06 16:52:34,802:INFO:create_model() successfully completed......................................
2024-11-06 16:52:34,899:INFO:SubProcess create_model() end ==================================
2024-11-06 16:52:34,899:INFO:Creating metrics dataframe
2024-11-06 16:52:34,905:INFO:Initializing Linear Discriminant Analysis
2024-11-06 16:52:34,905:INFO:Total runtime is 1.048269975185394 minutes
2024-11-06 16:52:34,907:INFO:SubProcess create_model() called ==================================
2024-11-06 16:52:34,907:INFO:Initializing create_model()
2024-11-06 16:52:34,907:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea80e89d0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0f14d4f410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:52:34,907:INFO:Checking exceptions
2024-11-06 16:52:34,907:INFO:Importing libraries
2024-11-06 16:52:34,907:INFO:Copying training dataset
2024-11-06 16:52:35,145:INFO:Defining folds
2024-11-06 16:52:35,145:INFO:Declaring metric variables
2024-11-06 16:52:35,148:INFO:Importing untrained model
2024-11-06 16:52:35,150:INFO:Linear Discriminant Analysis Imported successfully
2024-11-06 16:52:35,155:INFO:Starting cross validation
2024-11-06 16:52:35,159:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:52:35,731:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:35,769:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:35,778:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:35,844:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:35,847:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:52:35,901:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:35,948:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:35,971:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:35,973:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:35,981:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:35,985:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-06 16:52:35,994:INFO:Calculating mean and std
2024-11-06 16:52:35,996:INFO:Creating metrics dataframe
2024-11-06 16:52:35,998:INFO:Uploading results into container
2024-11-06 16:52:36,000:INFO:Uploading model into container now
2024-11-06 16:52:36,001:INFO:_master_model_container: 11
2024-11-06 16:52:36,001:INFO:_display_container: 2
2024-11-06 16:52:36,001:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-06 16:52:36,001:INFO:create_model() successfully completed......................................
2024-11-06 16:52:36,100:INFO:SubProcess create_model() end ==================================
2024-11-06 16:52:36,100:INFO:Creating metrics dataframe
2024-11-06 16:52:36,106:INFO:Initializing Extra Trees Classifier
2024-11-06 16:52:36,106:INFO:Total runtime is 1.0682975530624388 minutes
2024-11-06 16:52:36,109:INFO:SubProcess create_model() called ==================================
2024-11-06 16:52:36,109:INFO:Initializing create_model()
2024-11-06 16:52:36,109:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea80e89d0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0f14d4f410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:52:36,109:INFO:Checking exceptions
2024-11-06 16:52:36,109:INFO:Importing libraries
2024-11-06 16:52:36,109:INFO:Copying training dataset
2024-11-06 16:52:36,348:INFO:Defining folds
2024-11-06 16:52:36,349:INFO:Declaring metric variables
2024-11-06 16:52:36,352:INFO:Importing untrained model
2024-11-06 16:52:36,355:INFO:Extra Trees Classifier Imported successfully
2024-11-06 16:52:36,361:INFO:Starting cross validation
2024-11-06 16:52:36,365:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:52:36,949:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:52:37,141:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:52:37,165:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:52:37,300:INFO:Calculating mean and std
2024-11-06 16:52:37,301:INFO:Creating metrics dataframe
2024-11-06 16:52:37,302:INFO:Uploading results into container
2024-11-06 16:52:37,303:INFO:Uploading model into container now
2024-11-06 16:52:37,303:INFO:_master_model_container: 12
2024-11-06 16:52:37,303:INFO:_display_container: 2
2024-11-06 16:52:37,303:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=6337, verbose=0,
                     warm_start=False)
2024-11-06 16:52:37,303:INFO:create_model() successfully completed......................................
2024-11-06 16:52:37,392:INFO:SubProcess create_model() end ==================================
2024-11-06 16:52:37,392:INFO:Creating metrics dataframe
2024-11-06 16:52:37,399:INFO:Initializing Extreme Gradient Boosting
2024-11-06 16:52:37,399:INFO:Total runtime is 1.089846098423004 minutes
2024-11-06 16:52:37,402:INFO:SubProcess create_model() called ==================================
2024-11-06 16:52:37,402:INFO:Initializing create_model()
2024-11-06 16:52:37,402:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea80e89d0>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0f14d4f410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:52:37,402:INFO:Checking exceptions
2024-11-06 16:52:37,402:INFO:Importing libraries
2024-11-06 16:52:37,402:INFO:Copying training dataset
2024-11-06 16:52:37,641:INFO:Defining folds
2024-11-06 16:52:37,641:INFO:Declaring metric variables
2024-11-06 16:52:37,643:INFO:Importing untrained model
2024-11-06 16:52:37,646:INFO:Extreme Gradient Boosting Imported successfully
2024-11-06 16:52:37,650:INFO:Starting cross validation
2024-11-06 16:52:37,653:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:53:01,911:INFO:Calculating mean and std
2024-11-06 16:53:01,913:INFO:Creating metrics dataframe
2024-11-06 16:53:01,916:INFO:Uploading results into container
2024-11-06 16:53:01,916:INFO:Uploading model into container now
2024-11-06 16:53:01,917:INFO:_master_model_container: 13
2024-11-06 16:53:01,917:INFO:_display_container: 2
2024-11-06 16:53:01,919:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-11-06 16:53:01,919:INFO:create_model() successfully completed......................................
2024-11-06 16:53:02,015:INFO:SubProcess create_model() end ==================================
2024-11-06 16:53:02,015:INFO:Creating metrics dataframe
2024-11-06 16:53:02,021:INFO:Initializing Light Gradient Boosting Machine
2024-11-06 16:53:02,022:INFO:Total runtime is 1.5002173860867818 minutes
2024-11-06 16:53:02,024:INFO:SubProcess create_model() called ==================================
2024-11-06 16:53:02,024:INFO:Initializing create_model()
2024-11-06 16:53:02,024:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea80e89d0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0f14d4f410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:53:02,024:INFO:Checking exceptions
2024-11-06 16:53:02,024:INFO:Importing libraries
2024-11-06 16:53:02,024:INFO:Copying training dataset
2024-11-06 16:53:02,271:INFO:Defining folds
2024-11-06 16:53:02,271:INFO:Declaring metric variables
2024-11-06 16:53:02,274:INFO:Importing untrained model
2024-11-06 16:53:02,276:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-06 16:53:02,281:INFO:Starting cross validation
2024-11-06 16:53:02,284:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:53:26,110:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:53:26,662:INFO:Calculating mean and std
2024-11-06 16:53:26,663:INFO:Creating metrics dataframe
2024-11-06 16:53:26,664:INFO:Uploading results into container
2024-11-06 16:53:26,664:INFO:Uploading model into container now
2024-11-06 16:53:26,665:INFO:_master_model_container: 14
2024-11-06 16:53:26,665:INFO:_display_container: 2
2024-11-06 16:53:26,665:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6337, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-06 16:53:26,669:INFO:create_model() successfully completed......................................
2024-11-06 16:53:26,773:INFO:SubProcess create_model() end ==================================
2024-11-06 16:53:26,774:INFO:Creating metrics dataframe
2024-11-06 16:53:26,785:INFO:Initializing Dummy Classifier
2024-11-06 16:53:26,785:INFO:Total runtime is 1.912939480940501 minutes
2024-11-06 16:53:26,790:INFO:SubProcess create_model() called ==================================
2024-11-06 16:53:26,790:INFO:Initializing create_model()
2024-11-06 16:53:26,790:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea80e89d0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7e0f14d4f410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:53:26,790:INFO:Checking exceptions
2024-11-06 16:53:26,790:INFO:Importing libraries
2024-11-06 16:53:26,790:INFO:Copying training dataset
2024-11-06 16:53:27,045:INFO:Defining folds
2024-11-06 16:53:27,046:INFO:Declaring metric variables
2024-11-06 16:53:27,048:INFO:Importing untrained model
2024-11-06 16:53:27,050:INFO:Dummy Classifier Imported successfully
2024-11-06 16:53:27,057:INFO:Starting cross validation
2024-11-06 16:53:27,061:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-06 16:53:27,379:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:53:27,413:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:53:27,464:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:53:27,471:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:53:27,512:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:53:27,587:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:53:27,627:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:53:27,630:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:53:27,666:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:53:27,717:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-06 16:53:27,730:INFO:Calculating mean and std
2024-11-06 16:53:27,732:INFO:Creating metrics dataframe
2024-11-06 16:53:27,735:INFO:Uploading results into container
2024-11-06 16:53:27,736:INFO:Uploading model into container now
2024-11-06 16:53:27,737:INFO:_master_model_container: 15
2024-11-06 16:53:27,737:INFO:_display_container: 2
2024-11-06 16:53:27,737:INFO:DummyClassifier(constant=None, random_state=6337, strategy='prior')
2024-11-06 16:53:27,738:INFO:create_model() successfully completed......................................
2024-11-06 16:53:27,839:INFO:SubProcess create_model() end ==================================
2024-11-06 16:53:27,839:INFO:Creating metrics dataframe
2024-11-06 16:53:27,846:WARNING:/home/cid34senhas/miniconda3/envs/myenv/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-06 16:53:27,852:INFO:Initializing create_model()
2024-11-06 16:53:27,852:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7e0ea80e89d0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6337, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-06 16:53:27,852:INFO:Checking exceptions
2024-11-06 16:53:27,854:INFO:Importing libraries
2024-11-06 16:53:27,854:INFO:Copying training dataset
2024-11-06 16:53:28,101:INFO:Defining folds
2024-11-06 16:53:28,101:INFO:Declaring metric variables
2024-11-06 16:53:28,101:INFO:Importing untrained model
2024-11-06 16:53:28,101:INFO:Declaring custom model
2024-11-06 16:53:28,102:INFO:Random Forest Classifier Imported successfully
2024-11-06 16:53:28,105:INFO:Cross validation set to False
2024-11-06 16:53:28,105:INFO:Fitting Model
2024-11-06 16:53:28,381:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6337, verbose=0,
                       warm_start=False)
2024-11-06 16:53:28,381:INFO:create_model() successfully completed......................................
2024-11-06 16:53:28,517:INFO:_master_model_container: 15
2024-11-06 16:53:28,517:INFO:_display_container: 2
2024-11-06 16:53:28,517:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6337, verbose=0,
                       warm_start=False)
2024-11-06 16:53:28,518:INFO:compare_models() successfully completed......................................
