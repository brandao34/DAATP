{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEITURA DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/cid34senhas/Desktop/DAATP/process/Main_Versao/Com CN-MCI/Processamento/DF_CN-MCI_1_After_Pre_Processamento.csv') \n",
    "df_test = pd.read_csv('/home/cid34senhas/Desktop/DAATP/process/Main_Versao/Com CN-MCI/Processamento/DF_Test_1_After_Pre_Processamento.csv')\n",
    "\n",
    "\n",
    "label_mapping = {\n",
    "    'CN-CN': 0,\n",
    "    'AD-AD': 1,\n",
    "    'MCI-AD': 2,\n",
    "    'MCI-MCI': 3,\n",
    "    'CN-MCI' : 4\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcao Axuliar: Matrix de Confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def plot_confusion_matrix_with_labels(confusion_matrix, label_mapping):\n",
    "    # Criar um mapeamento inverso\n",
    "    reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "    \n",
    "    # Aplicar o mapeamento inverso na matriz de confusão\n",
    "    cm_with_labels = np.zeros_like(confusion_matrix, dtype=object)\n",
    "    for i in range(confusion_matrix.shape[0]):\n",
    "        for j in range(confusion_matrix.shape[1]):\n",
    "            cm_with_labels[i, j] = f\"{reverse_label_mapping[i]} (Pred: {reverse_label_mapping[j]})\"\n",
    "\n",
    "    # Criar um DataFrame para facilitar a visualização\n",
    "    df_cm = pd.DataFrame(confusion_matrix, index=reverse_label_mapping.values(), columns=reverse_label_mapping.values())\n",
    "    \n",
    "    # Plotar a matriz de confusão\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix with Labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Devisão Origem e Destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "0      0\n",
      "1      0\n",
      "2      1\n",
      "3      4\n",
      "4      0\n",
      "      ..\n",
      "300    0\n",
      "301    0\n",
      "302    2\n",
      "303    3\n",
      "304    0\n",
      "Name: Transition, Length: 305, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Transition'].dtype)\n",
    "print(df['Transition'])\n",
    "\n",
    "\n",
    "label_mapping = {\n",
    "    'CN-CN': 0,\n",
    "    'AD-AD': 1,\n",
    "    'MCI-AD': 2,\n",
    "    'MCI-MCI': 3,\n",
    "    'CN-MCI' : 4\n",
    "}\n",
    "\n",
    "# Criar o dicionário de reverse mapping\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# Aplicar o reverse mapping à coluna Transition\n",
    "df['Transition'] = df['Transition'].map(reverse_label_mapping)\n",
    "\n",
    "df['Transition'] = df['Transition'].astype(str)\n",
    "\n",
    "df[['Origem', 'Destino']] = df['Transition'].str.split('-', expand=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_transition_previsao(row, stats):\n",
    "    # Combinações válidas\n",
    "    combinacoes_validas = {'CN-CN', 'AD-AD', 'MCI-AD', 'MCI-MCI', 'CN-MCI'}\n",
    "\n",
    "    # Criar a combinação da origem e destino previstos\n",
    "    transition = f\"{row['Origem_Prevista']}-{row['Destino_Prevista']}\"\n",
    "\n",
    "    # Verificar se a combinação é válida\n",
    "    if transition in combinacoes_validas:\n",
    "        return transition\n",
    "    else:\n",
    "        # Regras para combinações inválidas\n",
    "        if row['Destino_Prevista'] == 'MCI' and row['Origem_Prevista'] not in {'MCI', 'CN'}:\n",
    "            stats['alterados'] += 1\n",
    "            return 'MCI-MCI'\n",
    "        elif row['Origem_Prevista'] == 'CN' and row['Destino_Prevista'] not in {'MCI', 'CN'}:\n",
    "            stats['alterados'] += 1\n",
    "            return 'CN-MCI'\n",
    "        elif row['Origem_Prevista'] == 'AD' and row['Destino_Prevista'] != 'AD':\n",
    "            stats['alterados'] += 1\n",
    "            return 'AD-AD'\n",
    "        elif row['Origem_Prevista'] == 'MCI' and row['Destino_Prevista'] not in {'MCI', 'AD'}:\n",
    "            stats['alterados'] += 1\n",
    "            return 'MCI-MCI'\n",
    "        else:\n",
    "            # Contar os casos que não foram corrigidos por falta de regra\n",
    "            stats['nao_corrigidos'] += 1\n",
    "            return transition  # Retorna o original, mesmo que inválido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_origem(df, modelo_origem): \n",
    "    # Separar as features e os targets\n",
    "    X = df.drop(columns=['Origem', 'Destino', 'Transition'])\n",
    "    y_origem = df['Origem']\n",
    "    y_destino = df['Destino']\n",
    "    y_transtion = df['Transition']\n",
    "    # Dividir o conjunto de dados em treino e teste para validação\n",
    "\n",
    "    # Divisão principal para garantir que os índices de teste sejam os mesmos\n",
    "    X_train_transition, X_test_transition, y_train_transition, y_test_transition = train_test_split(\n",
    "        X, y_transtion, test_size=0.40, random_state=34, stratify=y_transtion\n",
    "    )\n",
    "\n",
    "    # Usar os mesmos índices de treino e teste para y_origem e y_destino\n",
    "    y_train_origem, y_test_origem = y_origem[X_train_transition.index], y_origem[X_test_transition.index]\n",
    "\n",
    "    X_train_origem, X_test_origem = X.loc[X_train_transition.index], X.loc[X_test_transition.index]\n",
    "    #\n",
    "\n",
    "\n",
    "    # ? ORIGEM #########################################################\n",
    "    # Criar o modelo Random Forest\n",
    "    rf_origem_model = modelo_origem\n",
    "    # Avaliar desempenho com cross-validation no treino\n",
    "    rf_score_origem = cross_val_score(rf_origem_model, X_train_origem, y_train_origem, cv=5, scoring='f1_macro')\n",
    "    #print(\"F1 Macro (Cross-Validation):\", rf_score_origem.mean())\n",
    "    # Treinar o modelo no conjunto de treino e prever no conjunto de teste\n",
    "    rf_origem_model.fit(X_train_origem, y_train_origem)\n",
    "    rf_origem_pred = rf_origem_model.predict(X_test_origem)\n",
    "    # Avaliar o desempenho no conjunto de teste\n",
    "    print(\"Relatório de Classificação (Teste):\")\n",
    "    print(classification_report(y_test_origem, rf_origem_pred))\n",
    "\n",
    "    \n",
    "    selector = SelectFromModel(estimator=RandomForestClassifier(n_estimators=800, random_state=42), threshold=\"median\")\n",
    "    X_train_reduzido = selector.fit_transform(X_train_origem, y_train_origem)\n",
    "    X_test_reduzido = selector.transform(X_test_origem)\n",
    "    print(X_train_origem.shape)\n",
    "    print(X_train_reduzido.shape)\n",
    "    modelo_reduzido = RandomForestClassifier(random_state=42, n_estimators=800)\n",
    "    modelo_reduzido.fit(X_train_reduzido, y_train_origem)\n",
    "    y_pred_reduzido = modelo_reduzido.predict(X_test_reduzido)\n",
    "\n",
    "    print(\"\\nRelatório com Seleção Automática de Features:\")\n",
    "    print(classification_report(y_test_origem, y_pred_reduzido))\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    report_dict = classification_report(\n",
    "    y_test_origem, \n",
    "    rf_origem_pred,    output_dict=True)\n",
    "\n",
    "\n",
    "    return report_dict, rf_score_origem.mean(), rf_score_origem.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def processar_transicoes_com_label_encoder(df, modelo_origem):\n",
    "    # Separar as features e os targets\n",
    "    X = df.drop(columns=['Origem', 'Destino', 'Transition'])\n",
    "    y_origem = df['Origem']\n",
    "    y_transition = df['Transition']\n",
    "\n",
    "    # Codificar as classes para y_origem, y_destino e y_transition\n",
    "    label_encoder_origem = LabelEncoder()\n",
    "    y_origem_encoded = label_encoder_origem.fit_transform(y_origem)\n",
    "\n",
    "\n",
    "    label_encoder_transition = LabelEncoder()\n",
    "    y_transition_encoded = label_encoder_transition.fit_transform(y_transition)\n",
    "\n",
    "    # Dividir o conjunto de dados em treino e teste para validação\n",
    "    X_train_transition, X_test_transition, y_train_transition, y_test_transition = train_test_split(\n",
    "        X, y_transition_encoded, test_size=0.40, random_state=34, stratify=y_transition_encoded\n",
    "    )\n",
    "\n",
    "    # Usar os mesmos índices de treino e teste para y_origem e y_destino\n",
    "    y_train_origem, y_test_origem = y_origem_encoded[X_train_transition.index], y_origem_encoded[X_test_transition.index]\n",
    "\n",
    "    X_train_origem, X_test_origem = X.loc[X_train_transition.index], X.loc[X_test_transition.index]\n",
    "\n",
    "\n",
    "\n",
    "    modelo_origem.fit(X_train_origem, y_train_origem)\n",
    "    origem_pred = modelo_origem.predict(X_test_origem)\n",
    "    origem_pred_test = modelo_origem.predict(X_test_transition)\n",
    "\n",
    "       # Avaliar o desempenho no conjunto de teste\n",
    "    print(\"Relatório de Classificação (Teste):\")\n",
    "    print(classification_report(y_test_origem, origem_pred_test))\n",
    "\n",
    "        # Treinar e avaliar o modelo de Origem\n",
    "    rf_score_origem = cross_val_score(modelo_origem, X_train_origem, y_train_origem, cv=5, scoring='f1_macro')\n",
    "    print(\"F1 Macro (Cross-Validation - Origem):\", rf_score_origem.mean())\n",
    "    print(\"Desvio Padrão F1 Macro (Origem):\", rf_score_origem.std())\n",
    "\n",
    "    \n",
    "  \n",
    "    # Aplicar SelectFromModel para reduzir as features\n",
    "    selector = SelectFromModel(estimator=modelo_origem)\n",
    "    X_train_reduzido = selector.fit_transform(X_train_origem, y_train_origem)\n",
    "    X_test_reduzido = selector.transform(X_test_origem)\n",
    "    \n",
    "    # Exibir as dimensões antes e depois da redução\n",
    "    print(\"Dimensão original:\", X_train_origem.shape)\n",
    "    print(\"Dimensão reduzida:\", X_train_reduzido.shape)\n",
    "    \n",
    "    # Treinar o modelo com as features reduzidas\n",
    "    modelo_reduzido = modelo_origem\n",
    "    modelo_reduzido.fit(X_train_reduzido, y_train_origem)\n",
    "    y_pred_reduzido = modelo_reduzido.predict(X_test_reduzido)\n",
    "    \n",
    "    # Relatório de classificação\n",
    "    print(\"\\nRelatório com Seleção Automática de Features:\")\n",
    "    print(classification_report(y_test_origem, y_pred_reduzido))\n",
    "    rf_score_reduce = cross_val_score(modelo_reduzido, X_train_reduzido, y_train_origem, cv=5, scoring='f1_macro')\n",
    "    print(\"F1 Macro (Cross-Validation - Origem):\", rf_score_reduce.mean())\n",
    "    print(\"Desvio Padrão F1 Macro (Origem):\", rf_score_reduce.std())\n",
    "\n",
    "\n",
    "    # Obter as colunas selecionadas pelo SelectFromModel\n",
    "    mask = selector.get_support()  # Máscara booleana indicando as colunas selecionadas\n",
    "    colunas_selecionadas = X_train_origem.columns[mask]\n",
    "    colunas_removidas = X_train_origem.columns[~mask]\n",
    "    \n",
    "    # Salvar as colunas removidas em um arquivo CSV\n",
    "    colunas_removidas_df = pd.DataFrame({'Features Removidas': colunas_removidas})\n",
    "    colunas_removidas_df.to_csv('colunas_removidas.csv', index=False)\n",
    "    \n",
    "    print(f\"Lista de {len(colunas_removidas)} colunas removidas salva em 'colunas_removidas.csv'\")\n",
    "    \n",
    "    #print(classification_report(y_test_origem, origem_pred, target_names=label_encoder_origem.classes_ ))\n",
    "    \n",
    "\n",
    "\n",
    "    report_dict = classification_report(\n",
    "    y_test_origem, \n",
    "    origem_pred_test, \n",
    "    output_dict=True, target_names=label_encoder_origem.classes_)\n",
    "    return  report_dict , rf_score_origem.mean(),  rf_score_origem.std(), origem_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import LabelEncoder\n",
    "\n",
    "\n",
    "def gerar_origem_feature_importance(df, modelo, feature_importance_threshold, test_size=0.40, random_state=34):\n",
    "    \"\"\"\n",
    "    Realiza seleção de features e avalia o modelo fornecido.\n",
    "    \n",
    "    Parâmetros:\n",
    "        - df: DataFrame contendo os dados (incluindo 'Origem', 'Destino', 'Transition').\n",
    "        - modelo: Modelo de machine learning compatível com scikit-learn.\n",
    "        - feature_importance_threshold: Limite para seleção de features importantes (default: 0.001).\n",
    "        - test_size: Proporção do conjunto de teste (default: 0.40).\n",
    "        - random_state: Semente para reprodutibilidade (default: 34).\n",
    "        - cv_folds: Número de folds para cross-validation (default: 5).\n",
    "        \n",
    "    Retorna:\n",
    "        - report_dict: Relatório de classificação (teste).\n",
    "        - score_mean: Média das pontuações F1 (cross-validation).\n",
    "        - score_std: Desvio padrão das pontuações F1 (cross-validation).\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score\n",
    "    from sklearn.metrics import classification_report\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    # Separar as features e os targets\n",
    "    X = df.drop(columns=['Origem', 'Destino', 'Transition'])\n",
    "    y_origem = df['Origem']\n",
    "    y_transtion = df['Transition']\n",
    "\n",
    "     # Codificar as classes para y_origem, y_destino e y_transition\n",
    "    label_encoder_origem = LabelEncoder()\n",
    "    y_origem_encoded = label_encoder_origem.fit_transform(y_origem)\n",
    "\n",
    "\n",
    "    label_encoder_transition = LabelEncoder()\n",
    "    y_transition_encoded = label_encoder_transition.fit_transform(y_transtion)\n",
    "\n",
    "    # Dividir o conjunto de dados em treino e teste para validação\n",
    "    X_train_transition, X_test_transition, y_train_transition, y_test_transition = train_test_split(\n",
    "        X, y_transition_encoded, test_size=0.40, random_state=34, stratify=y_transition_encoded\n",
    "    )\n",
    "\n",
    "\n",
    "    # Usar os mesmos índices para y_origem\n",
    "    y_train_origem, y_test_origem = y_origem_encoded[X_train_transition.index], y_origem_encoded[X_test_transition.index]\n",
    "    X_train_origem, X_test_origem = X.loc[X_train_transition.index], X.loc[X_test_transition.index]\n",
    "\n",
    "    # Avaliar desempenho com cross-validation no treino\n",
    "    scores = cross_val_score(modelo, X_train_origem, y_train_origem, cv=5, scoring='f1_macro')\n",
    "    print(f\"F1 Macro (Cross-Validation): Média = {scores.mean():.4f}, Desvio = {scores.std():.4f}\")\n",
    "\n",
    "    # Treinar o modelo no conjunto de treino e prever no conjunto de teste\n",
    "    modelo.fit(X_train_origem, y_train_origem)\n",
    "    y_pred = modelo.predict(X_test_origem)\n",
    "\n",
    "    # Avaliar o desempenho no conjunto de teste\n",
    "    print(\"\\nRelatório de Classificação (Teste):\")\n",
    "    print(classification_report(y_test_origem, y_pred))\n",
    "\n",
    "    # Verificar se o modelo suporta feature_importances_\n",
    "    if hasattr(modelo, \"feature_importances_\"):\n",
    "        importancias = modelo.feature_importances_\n",
    "\n",
    "        features_importance_df = pd.DataFrame({\n",
    "            'Feature': X_train_origem.columns,\n",
    "            'Importance': importancias\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "     \n",
    "\n",
    "    \n",
    "\n",
    "        # Selecionar features com importância maior que o limite especificado\n",
    "        selected_features = features_importance_df[features_importance_df['Importance'] > feature_importance_threshold]['Feature']\n",
    "        X_train_reduzido = X_train_origem[selected_features]\n",
    "        X_test_reduzido = X_test_origem[selected_features]\n",
    "\n",
    "            # Exibir as dimensões antes e depois da redução\n",
    "        print(\"Dimensão original:\", X_train_origem.shape)   \n",
    "        print(\"Dimensão reduzida:\", X_train_reduzido.shape)\n",
    "\n",
    "        print(f\"\\nFeatures Selecionadas: {len(selected_features)}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nO modelo fornecido não suporta feature_importances_. Pulando seleção de features.\")\n",
    "        X_train_reduzido, X_test_reduzido = X_train_origem, X_test_origem\n",
    "\n",
    "    # Treinar novamente o modelo com features selecionadas\n",
    "    modelo.fit(X_train_reduzido, y_train_origem)\n",
    "    y_pred_reduzido = modelo.predict(X_test_reduzido)\n",
    "\n",
    "    print(\"\\nRelatório de Classificação com Features Selecionadas:\")\n",
    "    print(classification_report(y_test_origem, y_pred_reduzido))\n",
    "\n",
    "    # Relatório de classificação como dicionário\n",
    "    report_dict = classification_report(y_test_origem, y_pred, output_dict=True)\n",
    "\n",
    "    return report_dict, scores.mean(), scores.std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectFromModel Resultados :\n",
      "ExtraTrees\n",
      "Relatório de Classificação (Teste):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.25      0.34        24\n",
      "           1       0.65      0.76      0.70        42\n",
      "           2       0.56      0.62      0.59        56\n",
      "\n",
      "    accuracy                           0.60       122\n",
      "   macro avg       0.59      0.55      0.55       122\n",
      "weighted avg       0.59      0.60      0.58       122\n",
      "\n",
      "F1 Macro (Cross-Validation - Origem): 0.5034715323700831\n",
      "Desvio Padrão F1 Macro (Origem): 0.05276919612463657\n",
      "Dimensão original: (183, 2013)\n",
      "Dimensão reduzida: (183, 689)\n",
      "\n",
      "Relatório com Seleção Automática de Features:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.33      0.41        24\n",
      "           1       0.65      0.79      0.71        42\n",
      "           2       0.57      0.57      0.57        56\n",
      "\n",
      "    accuracy                           0.60       122\n",
      "   macro avg       0.58      0.56      0.56       122\n",
      "weighted avg       0.59      0.60      0.59       122\n",
      "\n",
      "F1 Macro (Cross-Validation - Origem): 0.5538010089622132\n",
      "Desvio Padrão F1 Macro (Origem): 0.07251297617879057\n",
      "Lista de 1324 colunas removidas salva em 'colunas_removidas.csv'\n",
      "{'AD': {'precision': 0.5454545454545454, 'recall': 0.25, 'f1-score': 0.34285714285714286, 'support': 24.0}, 'CN': {'precision': 0.6530612244897959, 'recall': 0.7619047619047619, 'f1-score': 0.7032967032967034, 'support': 42.0}, 'MCI': {'precision': 0.5645161290322581, 'recall': 0.625, 'f1-score': 0.5932203389830508, 'support': 56.0}, 'accuracy': 0.5983606557377049, 'macro avg': {'precision': 0.5876772996588664, 'recall': 0.5456349206349206, 'f1-score': 0.546458061712299, 'support': 122.0}, 'weighted avg': {'precision': 0.5912490470925161, 'recall': 0.5983606557377049, 'f1-score': 0.5818637045088837, 'support': 122.0}}\n",
      "0.5034715323700831\n",
      "0.05276919612463657\n",
      "RandomForest\n",
      "Relatório de Classificação (Teste):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.38      0.45        24\n",
      "           1       0.64      0.76      0.70        42\n",
      "           2       0.57      0.57      0.57        56\n",
      "\n",
      "    accuracy                           0.60       122\n",
      "   macro avg       0.59      0.57      0.57       122\n",
      "weighted avg       0.59      0.60      0.59       122\n",
      "\n",
      "F1 Macro (Cross-Validation - Origem): 0.5116289875938999\n",
      "Desvio Padrão F1 Macro (Origem): 0.07228156299807593\n",
      "Dimensão original: (183, 2013)\n",
      "Dimensão reduzida: (183, 737)\n",
      "\n",
      "Relatório com Seleção Automática de Features:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.33      0.41        24\n",
      "           1       0.65      0.74      0.69        42\n",
      "           2       0.56      0.59      0.57        56\n",
      "\n",
      "    accuracy                           0.59       122\n",
      "   macro avg       0.58      0.55      0.56       122\n",
      "weighted avg       0.58      0.59      0.58       122\n",
      "\n",
      "F1 Macro (Cross-Validation - Origem): 0.5061592675903261\n",
      "Desvio Padrão F1 Macro (Origem): 0.06342966651763904\n",
      "Lista de 1276 colunas removidas salva em 'colunas_removidas.csv'\n",
      "{'AD': {'precision': 0.5625, 'recall': 0.375, 'f1-score': 0.45, 'support': 24.0}, 'CN': {'precision': 0.64, 'recall': 0.7619047619047619, 'f1-score': 0.6956521739130435, 'support': 42.0}, 'MCI': {'precision': 0.5714285714285714, 'recall': 0.5714285714285714, 'f1-score': 0.5714285714285714, 'support': 56.0}, 'accuracy': 0.5983606557377049, 'macro avg': {'precision': 0.5913095238095238, 'recall': 0.5694444444444444, 'f1-score': 0.5723602484472049, 'support': 122.0}, 'weighted avg': {'precision': 0.5932786885245901, 'recall': 0.5983606557377049, 'f1-score': 0.5903064861012117, 'support': 122.0}}\n",
      "0.5116289875938999\n",
      "0.07228156299807593\n",
      "GradientBoosting\n",
      "Relatório de Classificação (Teste):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.38      0.44        24\n",
      "           1       0.62      0.79      0.69        42\n",
      "           2       0.60      0.55      0.57        56\n",
      "\n",
      "    accuracy                           0.60       122\n",
      "   macro avg       0.58      0.57      0.57       122\n",
      "weighted avg       0.59      0.60      0.59       122\n",
      "\n",
      "F1 Macro (Cross-Validation - Origem): 0.5489108029939673\n",
      "Desvio Padrão F1 Macro (Origem): 0.04348007314390208\n",
      "Dimensão original: (183, 2013)\n",
      "Dimensão reduzida: (183, 213)\n",
      "\n",
      "Relatório com Seleção Automática de Features:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.38      0.45        24\n",
      "           1       0.62      0.79      0.69        42\n",
      "           2       0.58      0.55      0.57        56\n",
      "\n",
      "    accuracy                           0.60       122\n",
      "   macro avg       0.59      0.57      0.57       122\n",
      "weighted avg       0.59      0.60      0.59       122\n",
      "\n",
      "F1 Macro (Cross-Validation - Origem): 0.6154191695858363\n",
      "Desvio Padrão F1 Macro (Origem): 0.03486940284648427\n",
      "Lista de 1800 colunas removidas salva em 'colunas_removidas.csv'\n",
      "{'AD': {'precision': 0.5294117647058824, 'recall': 0.375, 'f1-score': 0.43902439024390244, 'support': 24.0}, 'CN': {'precision': 0.6226415094339622, 'recall': 0.7857142857142857, 'f1-score': 0.6947368421052632, 'support': 42.0}, 'MCI': {'precision': 0.5961538461538461, 'recall': 0.5535714285714286, 'f1-score': 0.5740740740740741, 'support': 56.0}, 'accuracy': 0.5983606557377049, 'macro avg': {'precision': 0.5827357067645637, 'recall': 0.5714285714285714, 'f1-score': 0.5692784354744133, 'support': 122.0}, 'weighted avg': {'precision': 0.5921429601129753, 'recall': 0.5983606557377049, 'f1-score': 0.5890465646100235, 'support': 122.0}}\n",
      "0.5489108029939673\n",
      "0.04348007314390208\n",
      "XGBOOST\n",
      "Relatório de Classificação (Teste):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.29      0.37        24\n",
      "           1       0.60      0.76      0.67        42\n",
      "           2       0.55      0.54      0.54        56\n",
      "\n",
      "    accuracy                           0.57       122\n",
      "   macro avg       0.55      0.53      0.53       122\n",
      "weighted avg       0.56      0.57      0.55       122\n",
      "\n",
      "F1 Macro (Cross-Validation - Origem): 0.5212664278559861\n",
      "Desvio Padrão F1 Macro (Origem): 0.0694837665933138\n",
      "Dimensão original: (183, 2013)\n",
      "Dimensão reduzida: (183, 629)\n",
      "\n",
      "Relatório com Seleção Automática de Features:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.29      0.37        24\n",
      "           1       0.64      0.81      0.72        42\n",
      "           2       0.58      0.57      0.58        56\n",
      "\n",
      "    accuracy                           0.60       122\n",
      "   macro avg       0.57      0.56      0.55       122\n",
      "weighted avg       0.59      0.60      0.58       122\n",
      "\n",
      "F1 Macro (Cross-Validation - Origem): 0.6105329089426452\n",
      "Desvio Padrão F1 Macro (Origem): 0.04278958616524867\n",
      "Lista de 1384 colunas removidas salva em 'colunas_removidas.csv'\n",
      "{'AD': {'precision': 0.5, 'recall': 0.2916666666666667, 'f1-score': 0.3684210526315789, 'support': 24.0}, 'CN': {'precision': 0.6037735849056604, 'recall': 0.7619047619047619, 'f1-score': 0.6736842105263158, 'support': 42.0}, 'MCI': {'precision': 0.5454545454545454, 'recall': 0.5357142857142857, 'f1-score': 0.5405405405405406, 'support': 56.0}, 'accuracy': 0.5655737704918032, 'macro avg': {'precision': 0.5497427101200686, 'recall': 0.5297619047619048, 'f1-score': 0.5275486012328118, 'support': 122.0}, 'weighted avg': {'precision': 0.5565897140286252, 'recall': 0.5655737704918032, 'f1-score': 0.5525173145535527, 'support': 122.0}}\n",
      "0.5212664278559861\n",
      "0.0694837665933138\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Criar instâncias dos modelos\n",
    "xgbost_origem = XGBClassifier(n_estimators=100, max_depth=4, learning_rate=0.05,colsample_bytree=0.4, subsample=0.8, random_state=34)\n",
    "\n",
    "random_forest_origem = RandomForestClassifier(n_estimators=800,random_state=34)\n",
    "\n",
    "\n",
    "#report_xgboost, f1_macro_score_xgboost, f1_macro_std_xgboost = processar_transicoes_com_label_encoder(df,xgbost_origem)\n",
    "#report_random, f1_macro_score_rf, f1_macro_std_rf = gerar_origem(df,random_forest_origem)\n",
    "\n",
    "modelos = {\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(\n",
    "        n_estimators=447, max_features='sqrt', max_depth=15,\n",
    "        min_samples_split=10, min_samples_leaf=5, bootstrap=False, random_state=34\n",
    "    ),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=151, max_features='log2', max_depth=13,\n",
    "        min_samples_split=10, min_samples_leaf=3, bootstrap=True, random_state=34\n",
    "    ),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(\n",
    "        learning_rate=0.1, n_estimators=100, random_state=34\n",
    "    ),\n",
    "    \"XGBOOST\" : XGBClassifier(n_estimators=100, max_depth=4, learning_rate=0.05,colsample_bytree=0.4, subsample=0.8, random_state=34)\n",
    "}\n",
    "\n",
    "print(\"SelectFromModel Resultados :\")\n",
    "for nome, modelo in modelos.items():\n",
    "    # Chamar a função de processamento\n",
    "    print(nome)\n",
    "    report, f1_macro_score, f1_macro_std, previsoes_orgiem = processar_transicoes_com_label_encoder(df, modelo)\n",
    "    print(report)\n",
    "    print(f1_macro_score)\n",
    "    print(f1_macro_std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAA123",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
